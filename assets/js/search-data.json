{
  
    
        "post0": {
            "title": "Interfaces, Mixins and Building Powerful Custom Data Structures in Python",
            "content": "Imagine a custom set-like data structure that doesn’t perform hashing and trades performance for tighter memory footprint. Or imagine a dict-like data structure that automatically stores data in a PostgreSQL or Redis database the moment you initialize it; also it lets you to get-set-delete key-value pairs using the usual retrieval-assignment-deletion syntax associated with built-in dictionaries. Custom data structures can give you the power of choice and writing them will make you understand how the built-in data structures in Python are constructed. . One way to understand how built-in objects like dictionary, list, set etc work is to build custom data structures based on them. Python provides several mixin classes in the collection.abc module to design custom data structures that look and act like built-in structures with additional functionalities baked in. . Concept Edifice . To understand how all these work, you’ll need a fair bit of knowledge about Interfaces, Abstract Base Classes, Mixin Classes etc. I’ll build the concept edifice layer by layer where you’ll learn about interfaces first and how they can be created and used via the abc.ABC class. Then you’ll learn how abstract base classes differ from interfaces. After that I’ll introduce mixins and explain how all these concepts can be knitted together to architect custom data structures with amazing capabilities. Let’s dive in. . Interfaces . Python interfaces can help you write classes based on common structures. They ensure that classes that provide similar functionalities will also have similar footprints. Interfaces are not as popular in Python as they are in other statically typed language. The dynamic nature and duck-typing capabilities of Python often make them redundant. . However, in larger applications, interfaces can make you avoid writing code that is poorly encapsulated or build classes that look awfully similar but provide completely unrelated functionalities. Moreover, interfaces implicitly spawn other powerful techniques like mixin classes which can help you achieve DRY nirvana. . Overview . At a high level, an interface acts as a blueprint for designing classes. In Python, an interface is basically a specialized abstract class that defines one or more abstract methods. Abstract classes differs from concrete classes in the sense that they aren’t intended to stand on their own and the methods they define shouldn’t have any implementation. . Usually, you inherit from an interface and implement the methods defined in the abstract class in a concrete subclass. Interfaces provide the skeletons and concrete classes provide the implementation of the methods based on those skeletons. Depending on the ways you can architect interfaces, they can be segmented into two primary categories. . Informal Interfaces | Formal Interfaces | . Informal Interfaces . Informal interfaces are classes which define methods that can be overridden, but there’s no strict enforcement. . Let’s write an informal interface for a simple calculator class: . class ICalc: &quot;&quot;&quot;Informal Interface: Abstract calculator class.&quot;&quot;&quot; def add(self, a, b): raise NotImplementedError def sub(self, a, b): raise NotImplementedError def mul(self, a, b): raise NotImplementedError def div(self, a, b): raise NotImplementedError . Notice that the ICalc class has four different methods that don’t give you any implementation. It’s an informal interface because you can still instantiate the class but the methods will raise NotImplementedError if you try to apply them. You’ve to subclass the interface to use it. Let’s do it: . class Calc(ICalc): &quot;&quot;&quot;Concrete Class: Calculator&quot;&quot;&quot; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) . 3 -1 20 0.8333333333333334 . Now, you might be wondering why you even need all of these boilerplate code and inheritance when you can directly define the concrete Calc class and call it a day. . Consider the following scenario where you want to add additional functionalities to each of the method of the Calc class. Here, you’ve two options. Either you can mutate the original class and add those extra functionalities to the methods or you can create another class with similar footprint and implement all the methods with the added functionalities. . The first option isn’t always viable and can cause regression in real life scenario. The second approach ensures modularity and is generally quicker to implement since you won’t have to worry about messing up the original concrete class. However, figuring out which methods you’ll need to implement in the extended class can be hard because the concrete class might have additional methods that you don’t want in the extended class. . In this case, instead of figuring out the methods from the concrete Calc class, it’s easier to do so from an established structure defined in the ICalc interface. Interfaces make the process of extending class functionalities more tractable. Let’s make another class that will add logging to all of the methods of the Calc class: . import logging logging.basicConfig(level=logging.INFO) class CalcLog(ICalc): &quot;&quot;&quot;Concrete Class: Calculator with logging&quot;&quot;&quot; def add(self, a, b): logging.info(f&quot;Operation: Addition, Arguments: {(a, b)}&quot;) return a + b def sub(self, a, b): logging.info(f&quot;Operation: Subtraction, Arguments: {(a, b)}&quot;) return a - b def mul(self, a, b): logging.info(f&quot;Operation: Multiplication, Arguments: {(a, b)}&quot;) return a * b def div(self, a, b): logging.info(f&quot;Operation: Division, Arguments: {(a, b)}&quot;) return a / b # Using the class clog = CalcLog() print(clog.add(1, 2)) print(clog.sub(2, 3)) print(clog.mul(4, 5)) print(clog.div(5, 6)) . INFO:root:Operation: Addition, Arguments: (1, 2) INFO:root:Operation: Subtraction, Arguments: (2, 3) INFO:root:Operation: Multiplication, Arguments: (4, 5) INFO:root:Operation: Division, Arguments: (5, 6) 3 -1 20 0.8333333333333334 . In the above class, I’ve defined another class called CalcLog that basically extends the functionalities of the previously defined Calc class. Here, I’ve inherited from the informal interface ICalc and implemented all the methods with additional info logging capability. . Although writing informal interfaces is trivial, there are multiple issues that plagues them. The user of the interface class can still instantiate it like a normal class and won’t be able to tell the difference between a it and a concrete class until she tries to use any of the methods define inside the interface. Only then the methods will throw exceptions. This can have unintended side effects. . Moreover, informal interfaces won’t compel you to implement all the methods in the subclasses. You can easily get away without implementing a particular method defined in the interface. It won’t complain about the unimplemented methods in the subclasses. However, if you try to use a method that hasn’t been implemented in the subclass, you’ll get an error. This means even if issubclass(ConcreteSubClass, Interface) shows True, you can’t rely on it since it doesn’t give you the guarantee that the ConcreteSubClass has implemented all the methods defined in the Interface. . Let’s create another class FakeCalc an only implement one method defined in the ICalc abstract class: . class FakeCalc(ICalc): &quot;&quot;&quot;Concrete Class: Fake calculator that doesn&#39;t implement all the methods defined in the interface.&quot;&quot;&quot; def add(self, a, b): return a + b # Using the class cfake = FakeCalc() print(cfake.add(1, 2)) print(cfake.sub(2, 3)) . 3 NotImplementedError Traceback (most recent call last) &lt;ipython-input-48-035c519cee55&gt; in &lt;module&gt; 10 cfake = FakeCalc() 11 print(cfake.add(1,2)) &gt; 12 print(cfake.sub(2,3)) &lt;ipython-input-45-255c6a2093b0&gt; in sub(self, a, b) 6 7 def sub(self, a, b): -&gt; 8 raise NotImplementedError 9 10 def mul(self, a, b): NotImplementedError: . Despite not implementing all the methods defined in the ICalc class, I was still able to instantiate the FakeCalc concrete class. However, when I tried to apply a method sub that wasn’t implemented in the concrete class, it gave me an error. Also, issubclass(FakeCalc, ICalc) returns True which can mislead you into thinking that all the methods of the subclass FakeCalc are usable. It can cause subtle bugs can be difficult to detect. Formal interfaces try to overcome these issues. . Formal Interfaces . Formal interfaces do not suffer from the problems that plague informal interfaces. So if you want to implement an interface that the users can’t initiate independently and that forces them to implement all the methods in the concrete sub classes, formal interface is the way to go. In Python, the idiomatic way to define formal interfaces is via the abc module. Let’s transform the previously mentioned ICalc interface into a formal one: . from abc import ABC, abstractmethod class ICalc(ABC): &quot;&quot;&quot;Formal interface: Abstract calculator class.&quot;&quot;&quot; @abstractmethod def add(self, a, b): pass @abstractmethod def sub(self, a, b): pass @abstractmethod def mul(self, a, b): pass @abstractmethod def div(self, a, b): pass . Here, I’ve imported ABC class and abstractmethod decorator from the abc module of Python’s standard library. The name ABC stands for Abstract Base Class. The interface class needs to inherit from this ABCclass and all the abstract methods need to be decorated using the abstractmethod decorator. If your knowledge on decorators are fuzzy, checkout this in-depth article on python decorators. . Although, it seems like ICalc has merely inherited from the ABC class, under the hood, a metaclass ABCMeta gets attached to the interface which essentially makes sure that you can’t instantiate this class independently. Let’s try to do so and see what happens: . i = ICalc() . TypeError Traceback (most recent call last) &lt;ipython-input-118-a3cb2945d943&gt; in &lt;module&gt; -&gt; 1 i = ICalc() TypeError: Can&#39;t instantiate abstract class ICalc with abstract methods add, div, mul, sub . The error message clearly states that you can’t instantiate the class ICalc directly at all. You’ve make a subclass of ICalc and implement all the abstract methods and only then you’ll be able to make an instance of the subclass. The subclassing and implementation part is same as before. . class Calc(ICalc): &quot;&quot;&quot;Concrete calculator class&quot;&quot;&quot; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) . In the case of formal interface, failing to implement even one abstract method in the subclass will raise TypeError. So you can never write something the like the FakeCalc with a formal interface. This approach is more explicit and if there is an issue, it fails early. . Interfaces vs Abstract Base Classes . You’ve probably seen the term Interface and Abstract Base Class being used interchangeably. However, conceptually they’re different. Interfaces can be thought of as a special case of Abstract Base Classes. . It’s imperative that all the methods of an interface are abstract methods and the classes don’t store any state (instance variables). However, in case of abstract base classes, the methods are generally abstract but there can also be methods that provide implementation (concrete methods) and also, these classes can have instance variables. This generic abstract base classes can get very interesting and they can be used as mixins but more on that in the later sections. . Both interfaces and abstract base classes are similar in the sense that they can’t stand on their own, that means these classes aren’t meant to be instantiated independently. Pay attention to the following snippet to understand how interfaces and abstract base classes differ. . Interface . from abc import ABC, abstractmethod class InterfaceExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass . Here, all the methods must have to be abstract. . Abstract Base Class . from abc import ABC, abstractmethod class AbstractBaseClassExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass def method_c(self): # implement something pass . Notice how method_c in the above class is a concrete method and can have implementation. . The two examples above establish the fact that . All interfaces are abstract base classes but not all abstract base classes are interfaces. . A Complete Example . Before moving on to the next section, let’s see another contrived example to get the idea about the cases where interfaces can come handy. I’ll define an interface called AutoMobile and create three concrete classes called Car, Truck and Bus from it. The interface defines three abstract methods start, accelerate and stop that the concrete classes will need to implement later. . . from abc import ABC, abstractmethod class Automobile(ABC): &quot;&quot;&quot;Formal interface: Abstract automobile class.&quot;&quot;&quot; @abstractmethod def start(self): pass @abstractmethod def accelerate(self): pass @abstractmethod def stop(self): pass class Car(Automobile): &quot;&quot;&quot;Concrete Class: Car&quot;&quot;&quot; def start(self): return &quot;The car is starting&quot; def accelerate(self): return &quot;The car is accelerating&quot; def stop(self): return &quot;The car is stopping&quot; class Truck(Automobile): &quot;&quot;&quot;Concrete Class: Truck&quot;&quot;&quot; def start(self): return &quot;The truck is starting&quot; def accelerate(self): return &quot;The truck is accelerating&quot; def stop(self): return &quot;The truck is stopping&quot; class Bus(Automobile): &quot;&quot;&quot;Concrete Class: Bus&quot;&quot;&quot; def start(self): return &quot;The bus is starting&quot; def accelerate(self): return &quot;The bus is accelerating&quot; def stop(self): return &quot;The bus is stopping&quot; car = Car() truck = Truck() bus = Bus() print(car.start()) print(car.accelerate()) print(car.stop()) print(truck.start()) print(truck.accelerate()) print(truck.stop()) print(bus.start()) print(bus.accelerate()) print(bus.stop()) . The car is starting The car is accelerating The car is stopping The truck is starting The truck is accelerating The truck is stopping The bus is starting The bus is accelerating The bus is stopping . The above example delineates the use cases for interfaces. When you need to create multiple similar classes, interfaces can provide a basic foundation for the subclasses to build upon. In the next section, I’ll be using formal interfaces to create Mixin classes. So, before understanding mixin classes and how they can be used to inject additional plugins to your classes, it’s important that you understand interfaces and abstract base classes properly. . Mixins . Imagine you’re baking chocolate brownies. Now, you can have them without any extra fluff which is fine or you can top them with cream cheese, caramel sauce, chocolate chips etc. Usually you don’t make the extra toppings yourself, rather you prepare the brownies and use off the shelf toppings. This also gives you the ability to mix and match different combinations of toppings to spruce up the flavors quickly. However, making the the toppings from scratch would be a lengthy process and doing it over an over again can ruin the fun of baking. . While creating software, there’s sometimes a limit to the depth we should go. When pieces of what we’d like to achieve have already been executed well by others, it makes a lot of sense to reuse them. One way to achieve modularity and reusability in object-oriented programming is through a concept called a mixin. Different languages implement the concept of mixin in different ways. In Python, mixins are supported via multiple inheritance. . Overview . In the context of Python especially, a mixin is a parent class that provides functionality to subclasses but is not intended to be instantiated itself. This should already incite deja vu in you since classes that aren’t intended to be instantiated and can have both concrete and abstract methods are basically abstract base classes. Mixins can be regarded as a specific strain of abstract base classes where they can house both concrete and abstract methods but don’t keep any internal states. . These can help you when - . You want to provide a lot of optional features for a class. | You want to provide a lot of not-optional features for a class, but you want the features in separate classes so that each of them is about one feature (behavior). | You want to use one particular feature in many different classes | . Let’s see a contrived example. Consider an werkzeug’s request and response system. Werkzeug is the low-level framework that Flask micro-framework is built upon. I can make a plain old request object by saying: . from werkzeug import BaseRequest class Request(BaseRequest): pass . If I want to add accept header support, I would make that: . from werkzeug import BaseRequest, AcceptMixin class Request(AcceptMixin, BaseRequest): pass . If I wanted to make a request object that supports accept headers, etags, user agent and authentication support, I could do this: . from werkzeug import ( BaseRequest, AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, ) class Request( AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, BaseRequest ): pass . The above example might cause you to say, “that’s just multiple inheritance, not really a mixin”, which is can be true in a special case. Indeed, the differences between plain old multiple inheritance and mixin based inheritance collapse when the parent class can be instantiated. Understanding the subtlety in the differences between a mixin class, an abstract base class, an interface and the scope of multiple inheritance is important, so I’ll explore them in a dedicated section. . Differences Between Interfaces, Abstract Classes and Mixins . In order to better understand mixins, it’s be useful to compare mixins with abstract classes and interfaces from a code/implementation perspective: . Interfaces . Interfaces can contain abstract methods only, no concrete methods and no internal states (instance variables). . Abstract Classes . Abstract classes can contain abstract methods, concrete methods and internal state. . Mixins . Like interfaces, mixins do not contain any internal state. But like abstract classes, they can contain one or more concrete methods. So mixins are basically abstract classes without any internal states. . In Python, these are just conventions because all of the above are defined as classes. However, one trait that is common among interfaces, abstract classes and mixins is that they shouldn’t exist on their own, i.e. shouldn’t be instantiated independently. . A Complete Example . Before diving into the real-life examples and how mixins can be used to construct custom data structures, let’s have a look at a self-contained example of a mixin class at work: . import inspect from abc import ABC, abstractmethod from pprint import pprint class DisplayFactorMult(ABC): &quot;&quot;&quot;Mixin class that reveals factor calculation details.&quot;&quot;&quot; @abstractmethod def multiply(self, x): pass def multiply_show(self, x): result = self.multiply(x) print(f&quot;Factor: {self.factor}, Argument: {x}, Result: {result}&quot;) return result class FactorMult(DisplayFactorMult): &quot;&quot;&quot;Concrete class that uses the DisplayFactorMult mixin.&quot;&quot;&quot; def __init__(self, factor): self.factor = factor def multiply(self, x): return x * self.factor # Putting the FactorMult class to use f = FactorMult(10) f.multiply_show(20) # Use the inspect.getmembers method to inspect the methods pprint(inspect.getmembers(f, predicate=inspect.ismethod)) . Factor: 10, Argument: 20, Result: 200 [(&#39;__init__&#39;, &lt;bound method FactorMult.__init__ of &lt;__main__.FactorMult object at 0x7f0f0546bf40&gt;&gt;), (&#39;multiply&#39;, &lt;bound method FactorMult.multiply of &lt;__main__.FactorMult object at 0x7f0f0546bf40&gt;&gt;), (&#39;multiply_show&#39;, &lt;bound method DisplayFactorMult.multiply_show of &lt;__main__.FactorMult object at 0x7f0f0546bf40&gt;&gt;)] . The FactorMult class takes in a number as a factor and the multiply method simply multiplies an argument with the factor. The mixin class DisplayFactorMult provides an additional method multiply_show that enhances the multiply method of the concrete class. Method multiply_show prints the value of the factor, arguments an the result before returning the result. Here, DisplayFactoryMult is a mixin since it houses an abstract method multiply, a concrete method multiply_show and doesn’t store any instance variable. . If you really want to dive deeper into mixins and their real-life use cases, checkout the codebase of the famous Requests library. It defines and employs many powerful mixin classes to bestow superpowers upon different concrete classes. . Building Powerful Custom Data Structures with Mixins . You’ve reached the hall of fame where I’ll be building custom data structures using the mixin classes from the collections.abc module. . Verbose Tuple . This is a tuple-like data structure that acts exactly like the built-in tuple but with one exception. It’ll print out the special methods underneath when you perform any operation with it. . from collections.abc import Sequence class VerboseTuple(Sequence): &quot;&quot;&quot;Custom class that is exactly like a tuple but does some extra magic. Sequence: - Inherits From: Reversible, Collection Abstract Methods: __getitem__, __len__ Mixin Methods: __contains__, __iter__, __reversed__, index, and count &quot;&quot;&quot; def __init__(self, *args): self.args = args @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, index): print(f&quot;Method: __getitem__, Index: {index}&quot;) return self.args[index] def __len__(self): print(f&quot;Method: __len__&quot;) return len(self.args) def __repr__(self): return f&quot;{self._classname()}{tuple(self.args)}&quot; vt = VerboseTuple(1, 3, 4) print(vt) print(f&quot;Abstract Methods: {set(Sequence.__abstractmethods__)}&quot;) print(f&quot;Mixin Methods: { {k for k, v in Sequence.__dict__.items() if callable(v)} }&quot;) . VerboseTuple(1, 3, 4) Abstract Methods: {&#39;__len__&#39;, &#39;__getitem__&#39;} Mixin Methods: {&#39;__iter__&#39;, &#39;__contains__&#39;, &#39;index&#39;, &#39;count&#39;, &#39;__getitem__&#39;, &#39;__reversed__&#39;} . To build the VerboseTuple data structure, first, I’ve inherited the Sequence mixin class from the collections.abc module. The docstring mentions all the abstract and mixin methods provided by the Sequence class. To build the new data structure, you’ll have to implement all the abstract methods defined in the Sequence class and you’ll get all the mixin methods implemented automatically. Notice that the print statement above also reveals the abstract and the mixin methods. . In the following snippet I’ve used some of the functionalities offered by tuple and printed them in a way that will reveal the special methods when they perform any action. . # check __getitem__ print(&quot; n ==== Checking __getitem__ ====&quot;) print(vt[2]) # check __len__ print(&quot; n ==== Checking __len__ ====&quot;) print(len(vt)) # check __contains__ print(&quot; n ==== Checking __contains__ ====&quot;) print(3 in vt) # check __len__ print(&quot; n ==== Checking __iter__ ====&quot;) for elem in vt: print(elem) # check reverse print(f&quot; n ==== Checking reverse ====&quot;) print(list(reversed(vt))) # check count print(&quot; n ==== Checking count ====&quot;) print(vt.count(1)) . ==== Checking __getitem__ ==== Method: __getitem__, Index: 2 4 ==== Checking __len__ ==== Method: __len__ 3 ==== Checking __contains__ ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 True ==== Checking __iter__ ==== Method: __getitem__, Index: 0 1 Method: __getitem__, Index: 1 3 Method: __getitem__, Index: 2 4 Method: __getitem__, Index: 3 ==== Checking reverse ==== Method: __len__ Method: __getitem__, Index: 2 Method: __getitem__, Index: 1 Method: __getitem__, Index: 0 [4, 3, 1] ==== Checking count ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 Method: __getitem__, Index: 2 Method: __getitem__, Index: 3 1 . The printed statements reveal the corresponding special methods used internally when a particular tuple operation occurs. . Verbose List . This is a list-like data structure that acts exactly like the built-in list but with one exception. Like VerboseTuple, it’ll also print out the special methods underneath when you perform any operation on or with it.` . from collections.abc import MutableSequence class VerboseList(MutableSequence): &quot;&quot;&quot;Custom class that is exactly like a list but does some extra magic. MutableSequence: -- Inherits From: Sequence Abstract Methods: __getitem__, __setitem__, __delitem__, __len__, insert Mixin Methods: Inherited Sequence methods and append, reverse, extend, pop, remove, and __iadd__ &quot;&quot;&quot; def __init__(self, *args): self.args = list(args) @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, index): print(f&quot;Method: __getitem__, Index: {index}&quot;) return self.args[index] def __setitem__(self, index, value): print(f&quot;Method: __setitem__, Index: {index}, Value: {value}&quot;) self.args[index] = value def __delitem__(self, index): print(f&quot;Method: __delitem__, Index: {index}&quot;) del self.args[index] def __len__(self): print(f&quot;Method: __len__&quot;) return len(self.args) def __repr__(self): return f&quot;{self._classname()}{tuple(self.args)}&quot; def insert(self, index, value): self.args.insert(index, value) vl = VerboseList(4, 5, 6) vl2 = VerboseList(7, 8, 9) print(vl) print(f&quot;Abstract Methods: {set(MutableSequence.__abstractmethods__)}&quot;) print( f&quot;Mixin Methods: { {k for k, v in MutableSequence.__dict__.items() if callable(v)} }&quot; ) . VerboseList(4, 5, 6) Abstract Methods: {&#39;__delitem__&#39;, &#39;__len__&#39;, &#39;__getitem__&#39;, &#39;insert&#39;, &#39;__setitem__&#39;} Mixin Methods: {&#39;__iadd__&#39;, &#39;__setitem__&#39;, &#39;pop&#39;, &#39;append&#39;, &#39;extend&#39;, &#39;__delitem__&#39;, &#39;reverse&#39;, &#39;insert&#39;, &#39;clear&#39;, &#39;remove&#39;} . In the above segment, I’ve inherited the MutableSequence mixin class from the collections.abc module. This ensures that the VerboseList object will be mutable. All the abstract methods mentioned in the docstring have been implemented and the output print statements reveal the structure of the custom data structure as well as all the abstract and mixin methods. . In the following snippet, I’ve used some of the functionalities offered by list and printed them in a way that will reveal the special methods when they perform any action. . # check __setitem__ print(&quot; n ==== Checking __setitem__ ====&quot;) vl[1] = 44 print(vl) # check remove (__delitem__) print(&quot; n ==== Checking remove ====&quot;) vl.remove(6) print(vl) # check extend print(&quot; n ==== Checking extend ====&quot;) vl.extend([0, 0]) print(vl) # check pop print(&quot; n ==== Checking pop ====&quot;) vl.pop(-1) print(vl) # check __iadd__ print(&quot; n ==== Checking __iadd__&quot;) vl += vl2 print(vl) . ==== Checking __setitem__ ==== Method: __setitem__, Index: 1, Value: 44 VerboseList(4, 44, 6) ==== Checking remove ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 Method: __getitem__, Index: 2 Method: __delitem__, Index: 2 VerboseList(4, 44) ==== Checking extend ==== Method: __len__ Method: __len__ VerboseList(4, 44, 0, 0) ==== Checking pop ==== Method: __getitem__, Index: -1 Method: __delitem__, Index: -1 VerboseList(4, 44, 0) ==== Checking __iadd__ Method: __getitem__, Index: 0 Method: __len__ Method: __getitem__, Index: 1 Method: __len__ Method: __getitem__, Index: 2 Method: __len__ Method: __getitem__, Index: 3 VerboseList(4, 44, 0, 7, 8, 9) . Verbose Frozen Dict . Here, VerboseFrozenDict is an immutable data structure that is similar to the built-in dictionaries. Like the previous structures, this also reveals the internal special methods while performing different operations. . from collections.abc import Mapping class VerboseFrozenDict(Mapping): &quot;&quot;&quot;Custom class that is exactly like an immutable dict but does some extra magic. Mapping: -- Inherits From: Collection Abstract Methods: __getitem__, __iter__, __len__ Mixin Methods: __contains__, keys, items, values, get, __eq__, and __ne__ &quot;&quot;&quot; def __init__(self, **kwargs): self.kwargs = kwargs @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, key): print(f&quot;Method: __getitem__, Key: {key}&quot;) return self.kwargs[key] def __iter__(self): print(f&quot;Method: __iter__&quot;) return iter(self.kwargs) def __len__(self): print(f&quot;Method: __len__&quot;) return len(self.kwargs) def __repr__(self): return f&quot;{self._classname()}({self.kwargs})&quot; vf = VerboseFrozenDict(**{&quot;a&quot;: &quot;apple&quot;}) vf2 = VerboseFrozenDict(**{&quot;b&quot;: &quot;orange&quot;, &quot;c&quot;: &quot;mango&quot;}) print(vf) print(f&quot;Abstract Methods: {set(Mapping.__abstractmethods__)}&quot;) print(f&quot;Mixin Methods: { {k for k, v in Mapping.__dict__.items() if callable(v)} }&quot;) . VerboseFrozenDict({&#39;a&#39;: &#39;apple&#39;}) Abstract Methods: {&#39;__len__&#39;, &#39;__getitem__&#39;, &#39;__iter__&#39;} Mixin Methods: {&#39;items&#39;, &#39;__contains__&#39;, &#39;values&#39;, &#39;__eq__&#39;, &#39;keys&#39;, &#39;get&#39;, &#39;__getitem__&#39;} . In the above segment, I’ve inherited the Mapping mixin class from the collections.abc module. This ensures that the output sequence will be immutable. Just like before, all the abstract methods mentioned in the docstring have been implemented and the output print statements reveal the structure of the custom data structure, all the abstract and mixin methods. . Below the printed output will reveal the special methods used internally when the VerboseFrozenDict objects perform any operation. . # check __getitem__ print(&quot; n ==== Checking __getitem__ ====&quot;) print(vf[&quot;a&quot;]) # check __iter__ print(&quot; n ==== Checking __iter__ ====&quot;) for elem in vf: print(elem) # check __len__ print(&quot; n ==== Checking __len__ ====&quot;) print(len(vf)) # check __contains__ print(&quot; n ==== Checking __iter__ ====&quot;) print(&quot;a&quot; in vf) # check keys, values print(f&quot; n ==== Checking items, keys, values ====&quot;) print(vf.items()) print(vf.keys()) print(vf.values()) # check get print(&quot; n ==== Checking get ====&quot;) print(vf.get(&quot;b&quot;, None)) # check eq &amp; nq print(&quot; n ==== Checking __eq__, __nq__ ====&quot;) print(vf == vf2) print(vf != vf2) . ==== Checking __getitem__ ==== Method: __getitem__, Key: a apple ==== Checking __iter__ ==== Method: __iter__ a ==== Checking __len__ ==== Method: __len__ 1 ==== Checking __iter__ ==== Method: __getitem__, Key: a True ==== Checking items, keys, values ==== ItemsView(VerboseFrozenDict({&#39;a&#39;: &#39;apple&#39;})) KeysView(VerboseFrozenDict({&#39;a&#39;: &#39;apple&#39;})) ValuesView(VerboseFrozenDict({&#39;a&#39;: &#39;apple&#39;})) ==== Checking get ==== Method: __getitem__, Key: b None ==== Checking __eq__, __nq__ ==== Method: __iter__ Method: __getitem__, Key: a Method: __iter__ Method: __getitem__, Key: b Method: __getitem__, Key: c False Method: __iter__ Method: __getitem__, Key: a Method: __iter__ Method: __getitem__, Key: b Method: __getitem__, Key: c True . Verbose Dict . The VerboseDict data structure is the mutable version of VerboseFrozedDict. It supports all the operations of VerboseFrozenDict with some additional features like adding and deleting key-value pairs, updating values corresponding to different keys etc. . from collections.abc import MutableMapping class VerboseDict(MutableMapping): &quot;&quot;&quot;Custom class that is exactly like a dict but does some extra magic. MutableMapping: -- Inherits From: Mapping Abstract Methods: __getitem__, __setitem__, __delitem__, __iter__, __len__ Mixin Methods: Inherited Mapping methods and pop, popitem, clear, update, and setdefault &quot;&quot;&quot; def __init__(self, **kwargs): self.kwargs = kwargs @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, key): print(f&quot;Method: __getitem__, Key: {key}&quot;) return self.kwargs[key] def __setitem__(self, key, value): print(f&quot;Method: __setitem__, Key: {key}&quot;) self.kwargs[key] = value def __delitem__(self, key): print(f&quot;Method: __delitem__, Key: {key}&quot;) del self.kwargs[key] def __iter__(self): print(f&quot;Method: __iter__&quot;) return iter(self.kwargs) def __len__(self): print(f&quot;Method: __len__&quot;) return len(self.kwargs) def __repr__(self): return f&quot;{self._classname()}({self.kwargs})&quot; vd = VerboseDict(**{&quot;a&quot;: &quot;apple&quot;, &quot;b&quot;: &quot;ball&quot;, &quot;c&quot;: &quot;cat&quot;}) print(vd) print(f&quot;Abstract Methods: {set(MutableMapping.__abstractmethods__)}&quot;) print( f&quot;Mixin Methods: { {k for k, v in MutableMapping.__dict__.items() if callable(v)} }&quot; ) . VerboseDict({&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;ball&#39;, &#39;c&#39;: &#39;cat&#39;}) Abstract Methods: {&#39;__delitem__&#39;, &#39;__len__&#39;, &#39;__iter__&#39;, &#39;__getitem__&#39;, &#39;__setitem__&#39;} Mixin Methods: {&#39;__setitem__&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;__delitem__&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;clear&#39;} . The output statements reveal the structure of the VeboseDict class and the abstract and mixin methods associated with it. The following snippet will print the special methods used internally by the custom data structure (also in the built-in one) while performing different operations. . # check __getitem__ print(&quot; n ==== Checking __setitem__ ====&quot;) vd[&quot;a&quot;] = &quot;orange&quot; print(vd) # check popitem print(&quot; n ==== Checking popitem ====&quot;) vd.popitem() print(vd) # check update print(&quot; n ==== Checking update ====&quot;) vd.update({&quot;d&quot;: &quot;dog&quot;}) print(vd) # check clear print(&quot; n ==== Checking clear ====&quot;) vd.clear() print(vd) # check setdefault print(f&quot; n ==== Checking setdefault ====&quot;) x = vd.setdefault(&quot;a&quot;, &quot;pepsi&quot;) print(x) print(vd) . ==== Checking __setitem__ ==== Method: __setitem__, Key: a VerboseDict({&#39;a&#39;: &#39;orange&#39;, &#39;b&#39;: &#39;ball&#39;, &#39;c&#39;: &#39;cat&#39;}) ==== Checking popitem ==== Method: __iter__ Method: __getitem__, Key: a Method: __delitem__, Key: a VerboseDict({&#39;b&#39;: &#39;ball&#39;, &#39;c&#39;: &#39;cat&#39;}) ==== Checking update ==== Method: __setitem__, Key: d VerboseDict({&#39;b&#39;: &#39;ball&#39;, &#39;c&#39;: &#39;cat&#39;, &#39;d&#39;: &#39;dog&#39;}) ==== Checking clear ==== Method: __iter__ Method: __getitem__, Key: b Method: __delitem__, Key: b Method: __iter__ Method: __getitem__, Key: c Method: __delitem__, Key: c Method: __iter__ Method: __getitem__, Key: d Method: __delitem__, Key: d Method: __iter__ VerboseDict({}) ==== Checking setdefault ==== Method: __getitem__, Key: a Method: __setitem__, Key: a pepsi VerboseDict({&#39;a&#39;: &#39;pepsi&#39;}) . Going Ballistic with Custom Data Structures . This section discusses two advanced data structures that I mentioned at the beginning of the post. . BitSet : Mutable set-like data structure that doesn’t perform hashing . | SQLAlchemyDict: Mutable dict-like data structure that can store key-value pairs in any SQLAlchemy supported relational database . | . BitSet . This mutable set-like data structure doesn’t perform hashing to store data. It can store integers in a fixed range. While storing integers, BitSet objects use less memory compared to built-in sets. . However, since no hashing happens, it’s slower to perform addition and retrieval compared to built-in sets. The following code snippet was taken directly from Raymond Hettinger’s 2019 PyCon Russia talk on advanced data structures. . from collections.abc import MutableSet class BitSet(MutableSet): &quot;Ordered set with compact storage for integers in a fixed range&quot; def __init__(self, limit, iterable=()): self.limit = limit num_bytes = (limit + 7) // 8 self.data = bytearray(num_bytes) self |= iterable def _get_location(self, elem): if elem &lt; 0 or elem &gt;= self.limit: raise ValueError(f&quot;{elem!r} must be in range 0 &lt;= elem &lt; {self.limit}&quot;) return divmod(elem, 8) def __contains__(self, elem): bytenum, bitnum = self._get_location(elem) return bool((self.data[bytenum] &gt;&gt; bitnum) &amp; 1) def add(self, elem): bytenum, bitnum = self._get_location(elem) self.data[bytenum] |= 1 &lt;&lt; bitnum def discard(self, elem): bytenum, bitnum = self._get_location(elem) self.data[bytenum] &amp;= ~(1 &lt;&lt; bitnum) def __iter__(self): for elem in range(self.limit): if elem in self: yield elem def __len__(self): return sum(1 for elem in self) def __repr__(self): return f&quot;{type(self).__name__}(limit={self.limit}, iterable={list(self)})&quot; def _from_iterable(self, iterable): return type(self)(self.limit, iterable) . Let’s inspect the above data structure to understand exactly how much memory we can save. I’ll digress a little here. Normally, you’d use sys.getsizeof to measure the memory footprint of an object where the function reveals the size in bytes. . But there’s a problem. The function sys.getsizeof only reveals the size of the target object, excluding the objects the target objects might be referring to. To understand what I mean, consider the following situation: . Suppose, you have a nested list that looks like this: . lst = [[1], [2, 3], [[4, 5], 6, 7], 8, 9] . When you apply sys.getsizeof function on the list, it shows 96 bytes. This means only the outermost list consumes 96 bytes of memory. Here, sys.getsizeof doesn’t include the size of the nested lists. . The same is true for other data structures. In case of nested dictionaries, sys.getsizeof will not include the size of nested data structures. I’ll only reveal the size of the outermost dictionary object. The following snippet will traverse through the reference tree of a nested object and reveal the true size of it. . from collections.abc import Mapping, Container from sys import getsizeof def deep_getsizeof(o: object, ids: None = None) -&gt; int: &quot;&quot;&quot;Find the memory footprint of a Python object. This is a recursive function that drills down a Python object graph like a dictionary holding nested dictionaries with lists of lists and tuples and sets. The sys.getsizeof function does a shallow size of only. It counts each object inside a container as pointer only regardless of how big it really is. Params o: object The object ids: None Later an iterable is assigned to store the object ids Returns -- int Returns the size of object in bytes &quot;&quot;&quot; if ids is None: ids = set() d = deep_getsizeof if id(o) in ids: return 0 r = getsizeof(o) ids.add(id(o)) if isinstance(o, str): return r if isinstance(o, Mapping): return r + sum(d(k, ids) + d(v, ids) for k, v in o.iteritems()) if isinstance(o, Container): return r + sum(d(x, ids) for x in o) return r . Let’s use the deep_getsizeof to inspect the size differences between built-in set and BitSet objects. . bs = BitSet(limit=5, iterable=[0, 4]) s = {0, 4} print(f&quot;Normal Set object: {s}&quot;) print(f&quot;BitSet object: {bs}&quot;) print(f&quot;Size of a normal Set object: {deep_getsizeof(s)} bytes&quot;) print(f&quot;Size of a BitSet object: {deep_getsizeof(bs)} bytes&quot;) . Normal Set object: {0, 4} BitSet object: BitSet(limit=5, iterable=[0, 4]) Size of a normal Set object: 268 bytes Size of a BitSet object: 100 bytes . The output of the print statements reveal that the BitSet object uses less than half the memory compared to its built-in counterpart! . SQLAlchemyDict . Here goes the second type of custom data structure that I mentioned in the introduction. It’s also a mutable dict-like structure that can automatically store key-value pairs to any SQLAlchemy supported relational database when initialized. . I was inspired to write this one from the same Raymond Hettinger talk that I mentioned before. For demonstration purposes, I’ve chosen SQLite databse to store the key value pairs. . This structure gives you immense power since you can abstract away the entire process of database communication inside the custom object. You’ll perform get-set-delete operations on the object just like you’d do so with built-in dictionary objects and the custom object will take care of storing and updating the data to the target database. . Before running the code snippet below, you’ll need to install SQLAlchemy as an external dependency. . sqla_dict.py &quot;&quot;&quot; This is a self contained custom data structure with dict like key-value storage capabilities. * Can store the key-value pairs in any sqlalchemy supported db * Employs thread safe transactional scope * Modular, just change the session_scope to use a different db * This example uses sqlite db for demonstration purpose The code is inspired by Raymond Hettinger&#39;s talk `Build powerful, new data structures with Python&#39;s abstract base classes`. https://www.youtube.com/watch?v=S_ipdVNSFlo MIT License Copyright (c) 2020 Redowan Delowar Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &quot;Software&quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. &quot;&quot;&quot; from collections.abc import MutableMapping from contextlib import contextmanager from operator import itemgetter from sqlalchemy import create_engine, text from sqlalchemy.exc import OperationalError from sqlalchemy.orm import sessionmaker def create_transaction_session(dburl): # an engine, which the Session will use for connection resources some_engine = create_engine(dburl) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except OperationalError: pass except Exception: session.rollback() raise finally: session.close() return session_scope session_scope = create_transaction_session(&quot;sqlite:///foo.db&quot;) class SQLAlechemyDict(MutableMapping): def __init__(self, dbname, session_scope, items=None, **kwargs): self.dbname = dbname self.session_scope = session_scope if items is None: items = [] with self.session_scope() as session: session.execute(&quot;CREATE TABLE Dict (key text, value text)&quot;) session.execute(&quot;CREATE UNIQUE INDEX KIndx ON Dict (key)&quot;) self.update(items, **kwargs) def __setitem__(self, key, value): if key in self: del self[key] with self.session_scope() as session: session.execute( text(&quot;INSERT INTO Dict VALUES (:key, :value)&quot;), {&quot;key&quot;: key, &quot;value&quot;: value}, ) def __getitem__(self, key): with self.session_scope() as session: r = session.execute( text(&quot;SELECT value FROM Dict WHERE key=:key&quot;), {&quot;key&quot;: key} ) row = r.fetchone() if row is None: raise KeyError(key) return row[0] def __delitem__(self, key): if key not in self: raise KeyError(key) with self.session_scope() as session: session.execute(text(&quot;DELETE FROM Dict WHERE key=:key&quot;), {&quot;key&quot;: key}) def __len__(self): with self.session_scope() as session: r = session.execute(&quot;SELECT COUNT(*) FROM Dict&quot;) return next(r)[0] def __iter__(self): with self.session_scope() as session: r = session.execute(&quot;SELECT key FROM Dict&quot;) return map(itemgetter(0), r.fetchall()) def __repr__(self): return ( f&quot;{type(self).__name__}(dbname={self.dbname!r}, items={list(self.items())})&quot; ) def vacuum(self): with self.session_scope() as session: session.execute(&quot;VACUUM;&quot;) if __name__ == &quot;__main__&quot;: # test the struct sqladict = SQLAlechemyDict( dbname=&quot;foo.db&quot;, session_scope=session_scope, items={&quot;hello&quot;: &quot;world&quot;} ) print(sqladict) sqladict[&quot;key&quot;] = &quot;val&quot; for key in sqladict: print(key) # &gt;&gt;&gt; SQLAlechemyDict(dbname=&#39;foo.db&#39;, items=[(&#39;hello&#39;, &#39;world&#39;), (&#39;key&#39;, &#39;val&#39;)]) # &gt;&gt;&gt; hello # &gt;&gt;&gt; key . SQLAlechemyDict(dbname=&#39;foo.db&#39;, items=[(&#39;hello&#39;, &#39;world&#39;), (&#39;key&#39;, &#39;val&#39;)]) hello key . Running the above code snippet will create a SQLite database named foo.db in your current working directory. You can inspect the database with any database viewer and find your key-value pairs there. Everything else is the same as a built-in dictionary object. . Remarks . All the pieces of codes in the blog were written and tested with Python 3.8 on a machine running Ubuntu 20.04. Familiarity with built-in data structures, basic object-oriented protocols, decorators, dunder methods etc is assumed. . References . Implementing an Interface in Python - Real Python | What is a mixin, and why are they useful? - Stack Overflow | Mixins for Fun and Profit - Dan Hillard | Mixins in the Requests Library | Build powerful, new data structures with Python’s abstract base classes - Raymond Hettinger | .",
            "url": "https://rednafi.github.io/digressions/python/2020/07/03/python-mixins.html",
            "relUrl": "/python/2020/07/03/python-mixins.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deciphering Python's Metaclasses",
            "content": "In Python, metaclass is one of the few tools that enables you to inject metaprogramming capabilities into your code. The term metaprogramming refers to the potential for a program to manipulate itself in a self referential manner. However, messing with metaclasses is often considered an arcane art that’s beyond the grasp of the proletariats. Heck, even Tim Peters advices you to tread carefully while dealing with these. . Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don’t (the people who actually need them know with certainty that they need them, and don’t need an explanation about why). . Metaclasses are an esoteric OOP concept, lurking behind virtually all Python code. Every Python class that you create is attached to a default metaclass and Python cleverly abstracts away all the meta-magics. So, you’re indirectly using them all the time whether you are aware of it or not. For the most part, you don’t need to be aware of it. Most Python programmers rarely, if ever, have to think about metaclasses. This makes metaclasses exciting for me and I want to explore them in this post to formulate my own judgement. Let’s dive in. . Metaclasses . A metaclass is a class whose instances are classes. Like an “ordinary” class defines the behavior of the instances of the class, a metaclass defines the behavior of classes and their instances. . . Metaclasses aren’t supported by every object oriented programming language. Those programming language, which support metaclasses, considerably vary in way they implement them. Python provides you a way to get under the hood and define custom metaclasses. . Understanding Type and Class . In Python, everything is an object. Classes are objects as well. As a result, all classes must have corresponding types. You deal with built in types like int, float, list etc all the time. Consider this example: . a = 5 print(type(a)) print(type(int)) . &lt;class &#39;int&#39;&gt; &lt;class &#39;type&#39;&gt; . In the above example, variable a is an instance of the built in class int. Type of a is int and the type of int is type. User defined classes also show similar behavior. For example: . class Foo: pass a = Foo() print(type(a)) print(type(Foo)) . &lt;class &#39;__main__.Foo&#39;&gt; &lt;class &#39;type&#39;&gt; . Here, I’ve defined another class named Foo and created an instance a of the class. Applying type on instance a reveals its type as __main__.Foo and applying type on class Foo reveals the type as type. So here, we can use the term class and type interchangeably. This brings up the question: . What on earth this type (function? class?) thing actually is and what is the type of type? . Let’s apply type on type: . print(type(type)) . &lt;class &#39;type&#39;&gt; . Whaaaat? The type of any class (not instance of a class) in Python is type and the type of type is also type. By now, you’ve probably guessed that type is a very special class in Python that can reveal the type of itself and of any other class or object. In fact, type is a metaclass and all the classes in Python are instances of it. You can inspect that easily: . class Foo: pass for klass in [int, float, list, dict, Foo, type]: print(type(klass)) print(isinstance(Foo, type)) print(isinstance(type, type)) . &lt;class &#39;type&#39;&gt; &lt;class &#39;type&#39;&gt; &lt;class &#39;type&#39;&gt; &lt;class &#39;type&#39;&gt; &lt;class &#39;type&#39;&gt; &lt;class &#39;type&#39;&gt; True True . The the last line of the above code snippet demonstrates that type is also an instance of metaclass type. Normally, you can’t write self referential classes like that in pure Python. However, you can circumvent this limitation by subclassing from type. This enables you to write custom metaclasses that you can use to dictate and mutate the way classes are created and instantiated. From now on, I’ll be referring to the instance class of a metaclass as target class. Let’s create a custom metaclass that just prints the name of the target class while creating it: . class PrintMeta(type): def __new__(cls, name, bases, attrs): &quot;&quot;&quot;__new__ gets executed before the target is created. Parameters - name : str name of the class being defined (Point in this example) bases : tuple base classes for constructed class, empty tuple in this case attrs : dict dict containing methods and fields defined in the class Returns - instance class of this metaclass &quot;&quot;&quot; print(f&quot;Name of this class is: {name}&quot;) return super().__new__(cls, name, bases, attrs) class A(metaclass=PrintMeta): pass . Name of this class is A . Despite the fact that we haven’t called class A or created an instance of it, the __new__ method of metaclass PrintMeta got executed and printed the name of the target class. In the return statement of __new__ method, super() was used to call the __new__ method of the base class (type) of the metaclass PrintMeta. . Special Methods Used by Metaclasses . Type type, as the default metaclass in Python, defines a few special methods that new metaclasses can override to implement unique code generation behavior. Here is a brief overview of these “magic” methods that exist on a metaclass: . __new__: This method is called on the Metaclass before an instance of a class based on the metaclass is created | __init__: This method is called to set up values after the instance/object is created | __prepare__: Defines the class namespace in a mapping that stores the attributes | __call__: This method is called when the constructor of the new class is to be used to create an object | . These are the methods to override in your custom metaclass to give your classes behaviors different from that of type. The following example shows the default behaviors of these special methods and their execution order. . Some people immediately think of __init__, and I’ve occasionally called it “the constructor” myself; but in actuality, as its name indicates, it’s an initializer and by the time it’s invoked, the object has already been created, seeing as it’s passed in as self. The real constructor is a far less famous function: __new__. The reason you might never hear about it or use it- is that allocation doesn’t mean that much in Python, which manages memory on its own. So if you do override __new__, it’d be just like your __init__ —except you’ll have to call into Python to actually create the object, and then return that object afterward. . class ExampleMeta(type): &quot;&quot;&quot;Simple metaclass showing the execution flow of the special methods.&quot;&quot;&quot; @classmethod def __prepare__(cls, name, bases): &quot;&quot;&quot;Defines the class namespace in a mapping that stores the attributes Parameters - name : str name of the class being defined (Point in this example) bases : tuple base classes for constructed class, empty tuple in this case &quot;&quot;&quot; print(f&quot;Calling __prepare__ method of {super()}!&quot;) return super().__prepare__(name, bases) def __new__(cls, name, bases, attrs): &quot;&quot;&quot;__new__ is a classmethod, even without @classmethod decorator Parameters - name : str name of the class being defined (Point in this example) bases : tuple base classes for constructed class, empty tuple in this case attrs : dict dict containing methods and fields defined in the class &quot;&quot;&quot; print(f&quot;Calling __new__ method of {super()}!&quot;) return super().__new__(cls, name, bases, attrs) def __init__(self, name, bases, attrs): &quot;&quot;&quot;This method is called to set up values after the instance/object is created.&quot;&quot;&quot; print(f&quot;Calling __init__ method of {super()}!&quot;) super().__init__(name, bases, attrs) def __call__(self, *args, **kwargs): &quot;&quot;&quot;This method is called when the constructor of the new class is to be used to create an object Parameters - args : tuple position only arguments of the new class kwargs : dict keyward only arguments of the new class &quot;&quot;&quot; print(f&quot;Calling __call__ method of {super()}!&quot;) print(f&quot;Printing {self} args:&quot;, args) print(f&quot;Printing {self} kwargs&quot;, kwargs) return super().__call__(*args, **kwargs) class A(metaclass=ExampleMeta): def __init__(self, x, y): self.x = x self.y = y print(f&quot;Calling __init__ method of {self}&quot;) a = A(1, 3) . Calling __prepare__ method of &lt;super: &lt;class &#39;ExampleMeta&#39;&gt;, &lt;ExampleMeta object&gt;&gt;! Calling __new__ method of &lt;super: &lt;class &#39;ExampleMeta&#39;&gt;, &lt;ExampleMeta object&gt;&gt;! Calling __init__ method of &lt;super: &lt;class &#39;ExampleMeta&#39;&gt;, &lt;ExampleMeta object&gt;&gt;! Calling __call__ method of &lt;super: &lt;class &#39;ExampleMeta&#39;&gt;, &lt;ExampleMeta object&gt;&gt;! Printing &lt;class &#39;__main__.A&#39;&gt; args: (1, 3) Printing &lt;class &#39;__main__.A&#39;&gt; kwargs {} Calling __init__ method of &lt;__main__.A object at 0x7febe710a130&gt; . Pay attention to the execution order of the special methods of the custom metaclass ExampleMeta. The __prepare__ method is called first and is followed by __new__, __init__ and __call__ respectively. Only after that the first method __init__ of the target class A is called. This is important since it’ll help you to decide how you’ll mutate and change the behavior of your target class. . Metaclass Conflicts . Note that the metaclass argument is singular – you can’t attach more than one metaclass to a class. However, through multiple inheritance you can accidentally end up with more than one metaclass, and this produces a conflict which must be resolved. . class FirstMeta(type): pass class SecondMeta(type): pass class First(metaclass=FirstMeta): pass class Second(metaclass=SecondMeta): pass class Third(First, Second): pass third = Third() . TypeError Traceback (most recent call last) &lt;ipython-input-340-6afe6bc8f8bc&gt; in &lt;module&gt; 11 pass 12 &gt; 13 class Third(First, Second): 14 pass 15 TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases . Class First and Second are attached to different metaclasses and class Third inherits from both of them. Since metaclasses trickle down to subclasses, class Third is now effective attached to two metaclasses (FirstMeta and SecondMeta). This will raise TypeError. Attachment with only one metaclass is allowed here. . Examples in the Wild . In this section, I’ll go through a few real life examples where metaclasses can provide viable solutions to several tricky problems that you might encounter during software development. The solutions might appear over-engineered in some cases and almost all of them can be solved without using metaclasses. However, the purpose is to peek into the inner wirings of metaclasses and see how they can offer alternative solutions. . Simple Logging with Metaclasses . The goal here is to log a few basic information about a class without directly adding any logging statements to it. Instead, you can whip up a custom metaclass to perform some metaprogramming and add those statements to the target class without mutating it explicitly. . import logging logging.basicConfig(level=logging.INFO) class LittleMeta(type): def __new__(cls, name, bases, attrs): logging.info(f&quot;classname: {name}&quot;) logging.info(f&quot;baseclasses: {bases}&quot;) logging.info(f&quot;attrs: {attrs}&quot;) return super().__new__(cls, name, bases, attrs) class Point(metaclass=LittleMeta): def __init__(self, x: float, y: float) -&gt; None: self.x = x self.y = y def __repr__(self) -&gt; str: return f&quot;Point({self.x}, {self.y})&quot; p = Point(5, 10) print(p) . INFO:root:classname: Point INFO:root:baseclasses: () INFO:root:attrs: {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;Point&#39;, &#39;__init__&#39;: &lt;function Point.__init__ at 0x7f436c2db790&gt;, &#39;__repr__&#39;: &lt;function Point.__repr__ at 0x7f436c2db4c0&gt;} Point(5, 10) . In the above example, I’ve created a metaclass called LittleMeta and added the necessary logging statements to record the information about the target class. Since the logging statements are residing in the __new__ method of the metaclass, these information will be logged before the creation of the target class. In the target class Point, LittleMeta replaces the default type metaclass and produces the desired result by mutating the class. . Returning Class Attributes in a Custom List . In this case, I want to dynamically attach a new attribute to the target class called __attrs_ordered__. Accessing this attribute from the target class (or instance) will give out an alphabetically sorted list of the attribute names. Here, the __prepare__ method inside the metaclass AttrsListMeta returns an OrderDict instead of a simple dict - so all attributes gathered before the __new__ method call will be ordered. Just like the previous example, here, the __new__ method inside the metaclass implements the logic required to get the sorted list of the attribute names. . from collections import OrderedDict class AttrsListMeta(type): @classmethod def __prepare__(cls, name, bases): return OrderedDict() def __new__(cls, name, bases, attrs, **kwargs): attrs_names = [k for k in attrs.keys()] attrs_names_ordered = sorted(attrs_names) attrs[&quot;__attrs_ordered__&quot;] = attrs_names_ordered return super().__new__(cls, name, bases, attrs, **kwargs) class A(metaclass=AttrsListMeta): def __init__(self, x, y): self.y = y self.x = x a = A(1, 2) a.__attrs_ordered__ . [&#39;__init__&#39;, &#39;__module__&#39;, &#39;__qualname__&#39;] . You can access the __attrs_ordered__ attribute from both class A and an instance of class A. Try removing the sorted() function inside the __new__ method of the metaclass and see what happens! . Creating Singleton Class . In OOP term, a singleton class is a class that can have only one object (an instance of the class) at a time. . After the first time, if you try to instantiate a Singleton class, it will basically return the same instance of the class that was created before. So any modifications done to this apparently new instance will mutate the original instance since they’re basically the same instance. . class Singleton(type): _instance = {} def __call__(cls, *args, **kwargs): if cls not in cls._instance: cls._instance[cls] = super().__call__(*args, **kwargs) return cls._instance[cls] class A(metaclass=Singleton): pass a = A() b = A() a is b . True . In the above example, at first, I’ve created a singleton class A by attaching the Singleton metaclass to it. Secondly, I’ve instantiated class A and assigned the instance of the class to a variable a. Thirdly, I’ve instantiated the class again and assigned variable a b to this seemingly new instance. Checking the identity of the two variables a and b reveals that both of them actually point to the same instance. . Implementing a Class that Can’t be Subclassed . Suppose you want to create a base class where the users of your class won’t be able to create any subclasses from the base class. In that case, you can write a metaclass and attach that your base class. The base class will raise RuntimeError if someone tries to create a subclass from it. . class TerminateMeta(type): def __new__(cls, name, bases, attrs): type_list = [type(base) for base in bases] for typ in type_list: if typ is cls: raise RuntimeError( f&quot;Subclassing a class that has &quot; + f&quot;{cls.__name__} metaclass is prohibited&quot; ) return super().__new__(cls, name, bases, attrs) class A(metaclass=TerminateMeta): pass class B(A): pass a = A() . RuntimeError Traceback (most recent call last) &lt;ipython-input-438-ccba42f1f95b&gt; in &lt;module&gt; 20 21 &gt; 22 class B(A): 23 pass 24 ... RuntimeError: Subclassing a class that has TerminateMeta metaclass is prohibited . Disallowing Multiple Inheritance . Multiple inheritance can be fragile and error prone. So, if you don’t want to allow the users to use a base class with any other base classes to form multiple inheritance, you can do so by attaching a metaclass to that target base class. . class NoMultiMeta(type): def __new__(cls, name, bases, attrs): if len(bases) &gt; 1: raise TypeError(&quot;Inherited multiple base classes!&quot;) return super().__new__(cls, name, bases, attrs) class Base(metaclass=NoMultiMeta): pass # no error is raised class A(Base): pass # no error is raised class B(Base): pass # This will raise an error! class C(A, B): pass . TypeError Traceback (most recent call last) &lt;ipython-input-404-36c323db1ea0&gt; in &lt;module&gt; 18 19 # This will raise an error! &gt; 20 class C(A, B): 21 pass ... TypeError: Inherited multiple base classes! . Timing Classes with Metaclasses . Suppose you want to measure the execution time of different methods of a class. One way of doing that is to define a timer decorator and decorating all the methods to measure and show the execution time. However, by using a metaclass, you can avoid decorating the methods in the class individually and the metaclass will dynamically apply the timer decorator to all of the methods of your target class. This can reduce code repetition and improve code readability. . from types import FunctionType, MethodType from functools import wraps def timefunc(func): @wraps(func) def wrapper(*args, **kwargs): start_time = time.time() ret = func(*args, **kwargs) end_time = time.time() run_time = end_time - start_time print(f&quot;Executing {func.__qualname__} took {run_time} seconds.&quot;) return ret return wrapper class TimerMeta(type): def __new__(cls, name, bases, attrs): new_cls = super().__new__(cls, name, bases, attrs) # key is attribute name and val is attribute value in attribute dict for key, val in attrs.items(): if isinstance(val, FunctionType) or isinstance(val, MethodType): setattr(new_cls, key, timefunc(val)) return new_cls class Shouter(metaclass=TimerMeta): def __init__(self): pass def intro(self): print(&quot;I shout!&quot;) s = Shouter() s.intro() . Executing Shouter.__init__ took 1.1920928955078125e-06 seconds. I shout! Executing Shouter.intro took 6.747245788574219e-05 seconds. . Registering Plugins With Metaclasses . Suppose a specific single class represents a plugin in your code. You can write a metaclass to keep track of all of the plugins so than you don’t have to count them manually. . registry = {} class RegisterMeta(type): def __new__(cls, name, bases, attrs): new_cls = super().__new__(cls, name, bases, attrs) registry[new_cls.__name__] = new_cls return new_cls class A(metaclass=RegisterMeta): pass class B(A): pass class C(A): pass class D(B): pass b = B() registry . {&#39;A&#39;: __main__.A, &#39;B&#39;: __main__.B, &#39;C&#39;: __main__.C, &#39;D&#39;: __main__.D} . Debugging Methods with Metaclasses . Debugging a class often involves inspecting the individual methods and adding extra debugging logic to those. However, this can get tedious if you’ve do this over an over again. Instead, you can write an inspection decorator and use a metaclass to dynamically apply the decorator to all of the methods of your target class. Later on, you can simply detach the metaclass once you’re done with debugging and don’t want the extra logic in your target class. . from functools import wraps from types import FunctionType, MethodType def debug(func): &quot;&quot;&quot;Decorator for debugging passed function.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(&quot;Full name of this method:&quot;, func.__qualname__) return func(*args, **kwargs) return wrapper class DebugMeta(type): def __new__(cls, name, bases, attrs): new_cls = super().__new__(cls, name, bases, attrs) # key is attribute name and val is attribute value in attribute dict for key, val in attrs.items(): if isinstance(val, FunctionType) or isinstance(val, MethodType): setattr(new_cls, key, debug(val)) return new_cls class Base(metaclass=DebugMeta): pass class Calc(Base): def add(self, x, y): return x + y class CalcAdv(Calc): def mul(self, x, y): return x * y mycal = CalcAdv() print(mycal.mul(2, 3)) . Full name of this method: CalcAdv.mul 6 . Exception Handling with Metaclasses . Sometimes you need to handle exceptions in multiple methods of a class in a generic manner. That means all the methods of the class have the same exception handling, logging logic etc. Metaclasses can help you avoid adding repetitive exception handling and logging logics to your methods. . from functools import wraps def exc_handler(func): &quot;&quot;&quot;Decorator for custom exception handling.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): try: ret = func(*args, **kwargs) except: print(f&quot;Exception Occured!&quot;) print(f&quot;Method name: {func.__qualname__}&quot;) print(f&quot;Args: {args}, Kwargs: {kwargs}&quot;) raise return ret return wrapper class ExceptionMeta(type): def __new__(cls, name, bases, attrs): new_cls = super().__new__(cls, name, bases, attrs) # key is attribute name and val is attribute value in attribute dict for key, val in attrs.items(): if isinstance(val, FunctionType) or isinstance(val, MethodType): setattr(new_cls, key, exc_handler(val)) return new_cls class Base(metaclass=ExceptionMeta): pass class Calc(Base): def add(self, x, y): return x + y class CalcAdv(Calc): def div(self, x, y): return x / y mycal = CalcAdv() print(mycal.div(2, 0)) . Exception Occured! Method name: CalcAdv.div Args: (&lt;__main__.CalcAdv object at 0x7febe692d1c0&gt;, 2, 0), Kwargs: {} ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-467-accaebe919a8&gt; in &lt;module&gt; 43 44 mycal = CalcAdv() &gt; 45 print(mycal.div(2, 0)) ... ZeroDivisionError: division by zero . Abstract Base Classes . An abstract class can be regarded as a blueprint for other classes. It allows you to provide a set of methods that must be implemented within any child classes built from the abstract class. Abstract classes usually house multiple abstract methods. An abstract method is a method that has a declaration but does not have an implementation. When you want to provide a common interface for different implementations of a component, abstract classes are the way to go. You can’t directly initialize or use an abstract class. Rather, you’ve to subclass the abstract base class and provide concrete implementations of all the abstract methods. Python has a dedicated abc module to help you create abstract classes. Let’s see how you can define a simple abstract class that provides four abstract methods: . from abc import ABC, abstractmethod class ICalc(ABC): &quot;&quot;&quot;Interface for a simple calculator.&quot;&quot;&quot; @abstractmethod def add(self, a, b): pass @abstractmethod def sub(self, a, b): pass @abstractmethod def mul(self, a, b): pass @abstractmethod def div(self, a, b): pass intrf = ICalc() . TypeError Traceback (most recent call last) &lt;ipython-input-21-7be58e3a2a92&gt; in &lt;module&gt; 21 22 &gt; 23 intrf = ICalc() TypeError: Can&#39;t instantiate abstract class ICalc with abstract methods add, div, mul, sub . Although it seems like interface ICalc is simply inheriting from the class ABC, in fact, ABC is attaching a metaclass ABCMeta to ICalc. This metaclass transforms the ICalc class into an abstract class. You can see that the class ICalc gives TypeError when you take an attempt to initialize it. The only way to use this interface is via creating subclasses from ICalc base class and implementing all the abstract methods there. The snippet below shows that: . class Calc(ICalc): &quot;&quot;&quot;Concrete class that uses Icalc interface.&quot;&quot;&quot; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b calc = Calc() print(calc.add(1, 2)) print(calc.sub(2, 3)) print(calc.mul(3, 4)) print(calc.div(4, 5)) . 3 -1 12 0.8 . Metaclasses &amp; Dataclasses . Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Dataclasses automatically create __init__, __repr__, __eq__, __gt__, __lt__ etc methods without you having to add them explicitly. This can be very handy when you need to create custom containers for your data. You can create dataclasses in the following manner: . Creating Multiple DataClasses . from dataclasses import dataclass from datetime import datetime @dataclass(unsafe_hash=True, frozen=True) class Event: created_at: datetime @dataclass(unsafe_hash=True, frozen=True) class InvoiceIssued(Event): invoice_uuid: int customer_uuid: int total_amount: float due_date: datetime @dataclass(unsafe_hash=True, frozen=True) class InvoiceOverdue(Event): invoice_uuid: int customer_uuid: int inv = InvoiceIssued( **{ &quot;invoice_uuid&quot;: 22, &quot;customer_uuid&quot;: 34, &quot;total_amount&quot;: 100.0, &quot;due_date&quot;: datetime(2020, 6, 19), &quot;created_at&quot;: datetime.now(), } ) print(inv) . InvoiceIssued(created_at=datetime.datetime(2020, 6, 20, 1, 3, 24, 967633), invoice_uuid=22, customer_uuid=34, total_amount=100.0, due_date=datetime.datetime(2020, 6, 19, 0, 0)) . Avoiding Dataclass Decorator with Metaclasses . Now, one thing that I find cumbersome while creating multiple dataclasses is having to attach the @dataclasses.dataclass decorator to each of the dataclasses. Also, the decorator takes multiple arguments to customize the dataclass behavior and it can quickly get cumbersome when you’ve to create multiple dataclasses with custom behavior. Moreover, this goes against the DRY (Don’t Repeat Yourself) principle in software engineering. . To avoid this, you can write a metaclass that will automatically apply the customized dataclass decorator to all of the target classes implicitly. All you have to do is to attach the metaclass to a base dataclass and inherit from it in the later dataclasses that need to be created. . from dataclasses import dataclass from datetime import datetime class EventMeta(type): def __new__(cls, name, bases, attrs): &quot;&quot;&quot;__new__ is a classmethod, even without @classmethod decorator Parameters - name : str name of the class being defined (Event in this example) bases : tuple base classes for constructed class, empty tuple in this case attrs : dict dict containing methods and fields defined in the class &quot;&quot;&quot; new_cls = super().__new__(cls, name, bases, attrs) return dataclass(unsafe_hash=True, frozen=True)(new_cls) class Event(metaclass=EventMeta): created_at: datetime class InvoiceIssued(Event): invoice_uuid: int customer_uuid: int total_amount: float due_date: datetime class InvoiceOverdue(Event): invoice_uuid: int customer_uuid: int inv = InvoiceIssued( **{ &quot;invoice_uuid&quot;: 22, &quot;customer_uuid&quot;: 34, &quot;total_amount&quot;: 100.0, &quot;due_date&quot;: datetime(2020, 6, 19), &quot;created_at&quot;: datetime.now(), } ) print(inv) . InvoiceIssued(created_at=datetime.datetime(2020, 6, 24, 12, 57, 22, 543328), invoice_uuid=22, customer_uuid=34, total_amount=100.0, due_date=datetime.datetime(2020, 6, 19, 0, 0)) . Should You Use It? . Almost all of the problems you’ve encountered above can be solved without using metaclasses. Decorators can also be exclusively used to perform metaprogramming in a more manageable and subjectively cleaner way. One case where you absolutely have to use metaclasses is to avoid applying decorators to multiple classes or methods repetitively. . Also, metaclasses can easily veer into the realm of being a “solution in search of a problem“. If the problem at hand can be solved in a simpler way, it probably should be. However, I still think that you should at least try to understand how metaclasses work to have a better grasp on how Python classes work in general and can recognize when a metaclass really is the appropriate tool to use. . Remarks . Wrapping your mind around metaclasses can be tricky. So, to avoid any unnecessary confusion, I’ve entirely evaded any discussion regarding the behavioral difference between old style classes and new style classes in Python. Also, I’ve intentionally excluded mentioning the differences between type in Python 2 and type in Python 3 entirely. Python 2.x has reached its EOL. Save yourself some trouble and switch to Python 3.x if you already haven’t done so. . All the pieces of codes in the blog were written and tested with Python 3.8 on a machine running Ubuntu 20.04. . This article assumes familiarity with decorators, dataclasses etc. If your knowledge on them is rusty, checkout these posts on decorators and dataclasses. . References . Understanding Python’s Metaclasses | What are metaclasses in Python - Stack Overflow | Python Metaclasses - Real Python | Metaprogramming with Metaclasses in Python - Geeksforgeeks | Metaclasses - Python Course EU | When to Use Metaclasses in Python | A Primer on Python Metaclasses | .",
            "url": "https://rednafi.github.io/digressions/python/2020/06/26/python-metaclasses.html",
            "relUrl": "/python/2020/06/26/python-metaclasses.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Implementing Proxy Pattern in Python",
            "content": "In Python, there’s a saying that design patterns are anti-patterns. Also, in the realm of dynamic languages, design patterns have the notoriety of injecting additional abstraction layers to the core logic and making the flow gratuitously obscure. Python’s dynamic nature and the treatment of functions as first-class objects often make Java-ish design patterns redundant. Instead of littering your code with seemingly over-engineered patterns, you can almost always take the advantage of Python’s first-class objects, duck-typing, monkey-patching etc to accomplish the task at hand. However, recently there is one design pattern that I find myself using over and over again to write more maintainable code and that is the Proxy pattern. So I thought I’d document it here for future reference. . The Proxy Pattern . Before diving into the academic definition, let’s try to understand the Proxy pattern from an example. . Have you ever used an access card to go through a door? There are multiple options to open that door i.e. it can be opened either using access card or by pressing a button that bypasses the security. The door’s main functionality is to open but there is a proxy added on top of it to add some functionality. Let me better explain it using the code example below. . class Door: def open_method(self) -&gt; None: pass class SecuredDoor: def __init__(self) -&gt; None: self._klass = Door() def open_method(self) -&gt; None: print(f&quot;Adding security measure to the method of {self._klass}&quot;) secured_door = SecuredDoor() secured_door.open_method() . &gt;&gt;&gt; Adding security measure to the method of &lt;__main__.Door object at 0x7f9dab3b6670&gt; . The above code snippet concretizes the example given before. Here, the Door class has a single method called open_method which denotes the action of opening on the Door object. This method gets extended in the SecuredDoor class and in this case, I’ve just added a print statement to the method of the latter class. . Notice how the class Door was called from SecuredDoor via composition. In the case of proxy pattern, you can substitute primary object with the proxy object without any additional changes in the code. This conforms to the Liskov Substitution Principle. It states that: . Objects of a superclass shall be replaceable with objects of its subclasses without breaking the application. That requires the objects of your subclasses to behave in the same way as the objects of your superclass. . The Door object can be replaced by the SecuredDoor and the SecuredDoor class does not introduce any new methods, it only extends the functionality of the open_method of the Door class. . In plain words, . Using the proxy pattern, a class represents the functionality of another class. . Wikipedia says, . A proxy, in its most general form, is a class functioning as an interface to something else. A proxy is a wrapper or agent object that is being called by the client to access the real serving object behind the scenes. Use of the proxy can simply be forwarding to the real object, or can provide additional logic. In the proxy extra functionality can be provided, for example caching when operations on the real object are resource intensive, or checking preconditions before operations on the real object are invoked. . Pedagogically, the proxy pattern belongs to a family of patterns called the structural pattern. . Why Use It? . Loose Coupling . Proxy pattern let’s you easily decouple your core logic from the added functionalities that might be needed on top of that. The modular nature of the code makes maintaining and extending the functionalities of your primary logic a lot quicker and easier. . Suppose, you’re defining a division function that takes takes two integer as arguments and returns the result of the division between them. It also handles edge cases like ZeroDivisionError or TypeError and logs them properly. . import logging from typing import Union logging.basicConfig(level=logging.INFO) def division(a: Union[int, float], b: Union[int, float]) -&gt; float: try: result = a / b return result except ZeroDivisionError: logging.error(f&quot;Argument b cannot be {b}&quot;) except TypeError: logging.error(f&quot;Arguments must be integers/floats&quot;) print(division(1.9, 2)) . &gt;&gt;&gt; 0.95 . You can see this function is already doing three things at once which violates the Single Responsibility Principle. SRP says that a function or class should have only one reason to change. In this case, a change in any of the three responsibilities can force the function to change. Also this means, changing or extending the function can be difficult to keep track of. . Instead, you can write two classes. The primary class Division will only implement the core logic while another class ProxyDivision will extend the functionality of Division by adding exception handlers and loggers. . import logging from typing import Union logging.basicConfig(level=logging.INFO) class Division: def div(self, a: Union[int, float], b: Union[int, float]) -&gt; float: return a / b class ProxyDivision: def __init__(self) -&gt; None: self._klass = Division() def div(self, a: Union[int, float], b: Union[int, float]) -&gt; float: try: result = self._klass.div(a, b) return result except ZeroDivisionError: logging.error(f&quot;Argument b cannot be {b}&quot;) except TypeError: logging.error(f&quot;Arguments must be integers/floats&quot;) klass = ProxyDivision() print(klass.div(2, 0)) . &gt;&gt;&gt; ERROR:root:Argument b cannot be 0 None . In the example above, since both Division and ProxyDivision class implement the same interface, you can swap out the Division class with ProxyDivision and vice versa. The second class neither inherits directly from the first class nor it adds any new method to it. This means you can easily write another class to extend the functionalities of Division or DivisionProxy class without touching their internal logics directly. . Enhanced Testability . Another great advantage of using the proxy pattern is enhanced testability. Since your core logic is loosely coupled with the extended functionalities, you can test them out separately. This makes the test more succinct and modular. It’s easy to demonstrate the benefits with our previously mentioned Division and ProxyDivision classes. Here, the logic of the primary class is easy to follow and since this class only holds the core logic, it’s crucial to write unit test for this before testing the added functionalities. Testing out the Division class is much cleaner than testing the previously defined division function that tries to do multiple things at once. Once you’re done testing the primary class, you can proceed with the additional functionalities. Usually, this decoupling of core logic from the cruft and the encapsulation of additional functionalities result in more reliable and rigorous unit tests. . Proxy Pattern with Interface . In the real world, your class won’t look like the simple Division class having only a single method. Usually your primary class will have multiple methods and they will carry out multiple sophisticated tasks. By now, you probably have grasped the fact that the proxy classes need to implement all of the methods of the primary class. While writing a proxy class for a complicated primary class, the author of that class might forget to implement all the methods of the primary class.This will lead to a violation of the proxy pattern. Also, it can be hard to follow all the methods of the primary class if the class is large and complicated. . Here, the solution is an interface that can signal the author of the proxy class about all the methods that need to be implemented. An interface is nothing but an abstract class that dictates all the methods a concrete class needs to implement. However, interfaces can’t be initialized independently. You’ll have to make a subclass of the interface and implement all the methods defined there. Your subclass will raise error if it fails to implement any of the methods of the interface. Let’s look at a minimal example of how you can write an interface using Python’s abc.ABC and abc.abstractmethod and achieve proxy pattern with that. . from abc import ABC, abstractmethod class Interface(ABC): &quot;&quot;&quot;Interfaces of Interface, Concrete &amp; Proxy should be the same, because the client should be able to use Concrete or Proxy without any change in their internals. &quot;&quot;&quot; @abstractmethod def job_a(self, user: str) -&gt; None: pass @abstractmethod def job_b(self, user: str) -&gt; None: pass class Concrete(Interface): &quot;&quot;&quot;This is the main job doer. External services like payment gateways can be a good example. &quot;&quot;&quot; def job_a(self, user: str) -&gt; None: print(f&quot;I am doing the job_a for {user}&quot;) def job_b(self, user: str) -&gt; None: print(f&quot;I am doing the job_b for {user}&quot;) class Proxy(Interface): def __init__(self) -&gt; None: self._concrete = Concrete() def job_a(self, user: str) -&gt; None: print(f&quot;I&#39;m extending job_a for user {user}&quot;) def job_b(self, user: str) -&gt; None: print(f&quot;I&#39;m extending job_b for user {user}&quot;) if __name__ == &quot;__main__&quot;: klass = Proxy() print(klass.job_a(&quot;red&quot;)) print(klass.job_b(&quot;nafi&quot;)) . &gt;&gt;&gt; I&#39;m extending job_a for user red None I&#39;m extending job_b for user nafi None . It’s evident from the above workflow that you’ll need to define an Interface class first. Python provides abstract base classes as ABC in the abc module. Abstract class Interface inherits from ABC and defines all the methods that the concrete class will have to implement later. Concrete class inherits from the interface and implements all the methods defined in it. Notice how each method in the Interface class is decorated with the @abstractmethod decorator. If your knowledge on decorator is fuzzy, then checkout this post on Python decorators. The @abstractmethod decorator turns a normal method into an abstract method which means that the method is nothing but a blueprint of the required methods that the concrete subclass will have to implement later. You can’t directly instantiate Interface or use any of the abstract methods without making subclasses of the interface and implementing the methods. . The second class Concrete is the actual class that inherits from the abstract base class (interface) Interface and implements all the methods mentioned as abstract methods. This is a real class that you can instantiate and the methods can be used directly. However, if you forget to implement any of the abstract methods defined in the Interface then you’ll invoke TypeError. . The third class Proxy extends the functionalities of the base concrete class Concrete. It calls the Concrete class using the composition pattern and implements all the methods. However, in this case, I used the results from the concrete methods and extended their functionalities without code duplication. . Another Practical Example . Let’s play around with one last real-world example to concretize the concept. Suppose, you want to collect data from an external API endpoint. To do so, you hit the endpoint with GET requests from your http client and collect the responses in json format. Then say, you also want to inspect the response header and the arguments that were passed while making the request. . Now, in the real world, public APIs will often impose rate limits and when you go over the limit with multiple get requests, your client will likely throw an http connection-timeout error. Say, you want to handle this exceptions outside of the core logic that will send the http GET requests. . Again, let’s say you also want to cache the responses if the client has seen the arguments in the requests before. This means, when you send requests with the same arguments multiple times, instead of hitting the APIs with redundant requests, the client will show you the responses from the cache. Caching improves API response time dramatically. . For this demonstration, I’ll be using Postman’s publicly available GET API. . https://postman-echo.com/get?foo1=bar_1&amp;foo2=bar_2 . This API is perfect for the demonstration since it has a rate limiter that kicks in arbitrarily and make the client throw ConnectTimeOut and ReadTimeOutError. See how this workflow is going to look like: . Define an interface called IFetchUrl that will implement three abstract methods. The first method get_data will fetch data from the URL and serialize them into json format. The second method get_headers will probe the data and return the header as a dictionary. The third method get_args will also probe the data like the second method but this time it will return the query arguments as a dictionary. However, in the interface, you won’t be implementing anything inside the methods. . | Make a concrete class named FetchUrl that will derive from interface IFetchUrl. This time you’ll implement all three methods defined in the abstract class. However, you shouldn’t handle any edge cases here. The method should contain pure logic flow without any extra fluff. . | Make a proxy class called ExcFetchUrl. It will also inherit from the interface but this time you’ll add your exception handling logics here. This class also adds logging functionality to all the methods. Here you call the concrete class FetchUrl in a composition format and avoid code repetition by using the methods that’s been already implemented in the concrete class. Like the FetchUrl class, here too, you’ve to implement all the methods found in the abstract class. . | The fourth and the final class will extend the ExcFetchUrl and add caching functionality to the get_data method. It will follow the same pattern as the ExcFetchUrl class. . | . Since, by now, you’re already familiar with the workflow of the proxy pattern, let’s dump the entire 110 line solution all at once. . import logging import sys from abc import ABC, abstractmethod from datetime import datetime from pprint import pprint import httpx from httpx._exceptions import ConnectTimeout, ReadTimeout from functools import lru_cache logging.basicConfig(level=logging.INFO) class IFetchUrl(ABC): &quot;&quot;&quot;Abstract base class. You can&#39;t instantiate this independently.&quot;&quot;&quot; @abstractmethod def get_data(self, url: str) -&gt; dict: pass @abstractmethod def get_headers(self, data: dict) -&gt; dict: pass @abstractmethod def get_args(self, data: dict) -&gt; dict: pass class FetchUrl(IFetchUrl): &quot;&quot;&quot;Concrete class that doesn&#39;t handle exceptions and loggings.&quot;&quot;&quot; def get_data(self, url: str) -&gt; dict: with httpx.Client() as client: response = client.get(url) data = response.json() return data def get_headers(self, data: dict) -&gt; dict: return data[&quot;headers&quot;] def get_args(self, data: dict) -&gt; dict: return data[&quot;args&quot;] class ExcFetchUrl(IFetchUrl): &quot;&quot;&quot;This class can be swapped out with the FetchUrl class. It provides additional exception handling and logging.&quot;&quot;&quot; def __init__(self) -&gt; None: self._fetch_url = FetchUrl() def get_data(self, url: str) -&gt; dict: try: data = self._fetch_url.get_data(url) return data except ConnectTimeout: logging.error(&quot;Connection time out. Try again later.&quot;) sys.exit(1) except ReadTimeout: logging.error(&quot;Read timed out. Try again later.&quot;) sys.exit(1) def get_headers(self, data: dict) -&gt; dict: headers = self._fetch_url.get_headers(data) logging.info(f&quot;Getting the headers at {datetime.now()}&quot;) return headers def get_args(self, data: dict) -&gt; dict: args = self._fetch_url.get_args(data) logging.info(f&quot;Getting the args at {datetime.now()}&quot;) return args class CacheFetchUrl(IFetchUrl): def __init__(self) -&gt; None: self._fetch_url = ExcFetchUrl() @lru_cache(maxsize=32) def get_data(self, url: str) -&gt; dict: data = self._fetch_url.get_data(url) return data def get_headers(self, data: dict) -&gt; dict: headers = self._fetch_url.get_headers(data) return headers def get_args(self, data: dict) -&gt; dict: args = self._fetch_url.get_args(data) return args if __name__ == &quot;__main__&quot;: # url = &quot;https://postman-echo.com/get?foo1=bar_1&amp;foo2=bar_2&quot; fetch = CacheFetchUrl() for arg1, arg2 in zip([1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]): url = f&quot;https://postman-echo.com/get?foo1=bar_{arg1}&amp;foo2=bar_{arg2}&quot; print(f&quot; n {&#39;-&#39;*75} n&quot;) data = fetch.get_data(url) print(f&quot;Cache Info: {fetch.get_data.cache_info()}&quot;) pprint(fetch.get_headers(data)) pprint(fetch.get_args(data)) . INFO:root:Getting the headers at 2020-06-16 16:54:36.214562 INFO:root:Getting the args at 2020-06-16 16:54:36.220221 Cache Info: CacheInfo(hits=0, misses=1, maxsize=32, currsize=1) {&#39;accept&#39;: &#39;*/*&#39;, &#39;accept-encoding&#39;: &#39;gzip, deflate&#39;, &#39;content-length&#39;: &#39;0&#39;, &#39;host&#39;: &#39;postman-echo.com&#39;, &#39;user-agent&#39;: &#39;python-httpx/0.13.1&#39;, &#39;x-amzn-trace-id&#39;: &#39;Root=1-5ee8a4eb-4341ae58365e4090660dfaa4&#39;, &#39;x-b3-parentspanid&#39;: &#39;044bd10726921994&#39;, &#39;x-b3-sampled&#39;: &#39;0&#39;, &#39;x-b3-spanid&#39;: &#39;503e6ceaa2a4f493&#39;, &#39;x-b3-traceid&#39;: &#39;77d5b03fe98fcc1a044bd10726921994&#39;, &#39;x-envoy-external-address&#39;: &#39;10.100.91.201&#39;, &#39;x-forwarded-client-cert&#39;: &#39;By=spiffe://cluster.local/ns/pm-echo-istio/sa/default;Hash=2ed845a68a0968c80e6e0d0f49dec5ce15ee3c1f87408e56c938306f2129528b;Subject=&quot;&quot;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account&#39;, &#39;x-forwarded-port&#39;: &#39;443&#39;, &#39;x-forwarded-proto&#39;: &#39;http&#39;, &#39;x-request-id&#39;: &#39;295d0b6c-7aa0-4481-aa4d-f47f5eac7d57&#39;} {&#39;foo1&#39;: &#39;bar_1&#39;, &#39;foo2&#39;: &#39;bar_1&#39;} .... . In the get_data method of the FetchUrl class, I’ve used the awesome httpx client to fetch the data from the URL. Pay attention to the fact that I’ve practically ignored all the additional logics of error handling and logging here. The exception handling and logging logics were added via ExcFetchUrl proxy class. Another class CacheFetchUrl further extents the proxy class ExcFetchUrl by adding cache functionality to the get_data method. . In the main section, you can use any of the FetchUrl, ExcFetchUrl or CacheFetchUrl without any additional changes to the logic of these classes. The FetchUrl is the barebone class that will fail in case of the occurrence of any exceptions. The later classes appends additional functionalities while maintaining the same interface. . The output basically prints out the results returned by the get_headers and get_args methods. Also notice, how I picked the endpoint arguments to simulate caching. The Cache Info: on the third line of the output shows when data is served from the cache. Here hits=0 means data is served directly from the external API. However, if you inspect the later outputs, you’ll see when the query arguments get repeated ([1, 2, 3, 1, 2, 3]), Cache Info: will show higher hit counts. This means that the data is being served from the cache. . Should You Use It? . Well, yes obviously. But not always. You see, you need a little bit of planning before orchestrating declarative solution with the proxy pattern. It’s not viable to write code in this manner in a throwaway script that you don’t have to maintain in the long run. Also, this OOP-cursed additional layers of abstraction can make your code subjectively unreadable. So use the pattern wisely. On the flip side, proxy pattern can come extremely handy when you need to extend the functionality of some class arbitrarily as it can work a gateway to the El Dorado of loose coupling. . Remarks . All the pieces of codes in the blog were written and tested with Python 3.8 on a machine running Ubuntu 20.04. . References . Python Patterns | Design Patterns for Humans | Design Patterns- Refactoring Guru | Design Patterns &amp; Idioms | .",
            "url": "https://rednafi.github.io/digressions/python/2020/06/16/python-proxy-pattern.html",
            "relUrl": "/python/2020/06/16/python-proxy-pattern.html",
            "date": " • Jun 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Pedantic Configuration Management with Pydantic",
            "content": "Updated on 2020-06-10: Removed duplicates . Managing configurations in your Python applications isn’t something you think about much often, until complexity starts to seep in and forces you to re-architect your initial approach. Ideally, your config management flow shouldn’t change across different applications or as your application begins to grow in size and complexity. Even if you’re writing a library, there should be a consistent config management process that scales up properly. Since I primarily spend my time writing data-analytics, data-science applications and expose them using Flask or FastAPI framework, I’ll be tacking config management from an application development perspective. . Few Ineffective Approaches . In the past, while exposing APIs with Flask, I used to use .env, .flaskenv and Config class approach to manage configs which is pretty much a standard in the Flask realm. However, it quickly became cumbersome to maintain and juggle between configs depending on development, staging or production environments. There were additional application specific global constants to deal with too. So I tried using *.json, *.yaml or *.toml based config management approaches but those too, quickly turned into a tangled mess. I was constantly accessing variables buried into 3-4 levels of nested toml data structure and it wasn’t pretty. Then there are config management libraries like Dynaconf or environ-config that aim to ameliorate the issue. While these are all amazing tools but they also introduce their own custom workflow that can feel over-engineered while dealing with maintenance and extension. . A Pragmatic Wishlist . I wanted to take a declarative approach while designing a config management pipleline that will be modular, scalable and easy to maintain. To meet my requirements, the system should be able to: . Read configs from .env files and shell environment at the same time. | Handle dependency injection for introducing passwords or secrets. | Convert variable types automatically in the appropriate cases, e.g. string to integer conversion. | Keep development, staging and production configs separate. | Switch between the different environments e.g development, staging effortlessly. | Inspect the active config values | Create arbitrarily nested config structure if required (Not encouraged though. Constraints fosters creativity, remember?) | . Building the Config Management Pipeline . Preparation . The code block that appears in this section is self contained. It should run without any modifications. If you want to play along, then just spin up a Python virtual environment and install Pydantic and python-dotenv. The following commands works on any *nix based system: . python3.8 -m venv venv source venv/bin/activate pip install pydantic python-dotenv . Make sure you have fairly a recent version of Python 3 installed, preferably Python 3.8. You might need to install python3.8 venv. If you’re on Ubuntu Linux, you can do so with sudo apt install python3.8-venv command. . Introduction to Pydantic . To check off all the boxes of the wishlist above, I made a custom config management flow using Pydantic, python-dotenv and the .env file. Pydantic is a fantastic data validation library that can be used for validating and implicitly converting data types using Python’s type hints. Type hinting is a formal solution to statically indicate the type of a value within your Python code. It was specified in PEP 484 and introduced in Python 3.5. Let’s define and validate the attributes of a class named User: . from Pydantic import BaseModel class User(BaseModel): name: str username: str password: int user = User(name=&quot;Redowan Delowar&quot;, username=&quot;rednafi&quot;, password=&quot;123&quot;) print(user) . This will give you: . &gt;&gt;&gt; User(name=&#39;Redowan Delowar&#39;, username=&#39;rednafi&#39;, password=123) . In the above example, I defined a simple class named User and used Pydantic for data validation. Pydantic will make sure that the data you assign to the class attributes conform with the types you’ve annotated. Notice, how I’ve assigned a string type data in the password field and Pydantic converted it to integer type without complaining. That’s because the corresponding type annotation suggests that the password attribute of the User class should be an integer. When implicit conversion is not possible or the hinted value of an attribute doesn’t conform to its assigned type, Pydantic will throw a ValidationError. . The Orchestration . Now let’s see how you can orchestrate your config management flow with the tools mentioned above. For simplicity, let’s say you’ve 3 sets of configurations. . Configs of your app’s internal logic | Development environment configs | Production environment configs | In this case, other than the first set of configs, all should go into the .env file. . I’ll be using this .env file for demonstration. If you’re following along, then go ahead, create an empty .env file there and copy the variables mentioned below: . #.env ENV_STATE=&quot;dev&quot; # or prod DEV_REDIS_HOST=&quot;127.0.0.1&quot; DEV_REDIS_PORT=&quot;4000&quot; PROD_REDIS_HOST=&quot;127.0.0.2&quot; PROD_REDIS_PORT=&quot;5000&quot; . Notice how I’ve used the DEV_ and PROD_ prefixes before the environment specific configs. These help you discern between the variables designated for different environments. . Configs related to your application’s internal logic should either be explicitly mentioned in the same configs.py or imported from a different app_configs.py file. You shouldn’t pollute your .env files with the internal global variables necessitated by your application’s core logic. . Now let’s dump the entire config orchestration and go though the building blocks one by one: . # configs.py from typing import Optional from pydantic import BaseSettings, Field, BaseModel class AppConfig(BaseModel): &quot;&quot;&quot;Application configurations.&quot;&quot;&quot; VAR_A: int = 33 VAR_B: float = 22.0 class GlobalConfig(BaseSettings): &quot;&quot;&quot;Global configurations.&quot;&quot;&quot; # This variable will be loaded from the .env file. However, if there is a # shell environment variable having the same name, that will take precedence. APP_CONFIG: AppConfig = AppConfig() ENV_STATE: Optional[str] = Field(None, env=&quot;ENV_STATE&quot;) REDIS_HOST: Optional[str] = None REDIS_PORT: Optional[int] = None REDIS_PASS: Optional[str] = None class Config: &quot;&quot;&quot;Loads the dotenv file.&quot;&quot;&quot; env_file: str = &quot;.env&quot; class DevConfig(GlobalConfig): &quot;&quot;&quot;Development configurations.&quot;&quot;&quot; class Config: env_prefix: str = &quot;DEV_&quot; class ProdConfig(GlobalConfig): &quot;&quot;&quot;Production configurations.&quot;&quot;&quot; class Config: env_prefix: str = &quot;PROD_&quot; class FactoryConfig: &quot;&quot;&quot;Returns a config instance dependending on the ENV_STATE variable.&quot;&quot;&quot; def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == &quot;dev&quot;: return DevConfig() elif self.env_state == &quot;prod&quot;: return ProdConfig() cnf = FactoryConfig(GlobalConfig().ENV_STATE)() print(cnf.__repr__()) . The print statement of the last line in the above code block is to inspect the active configuration class. You’ll soon learn what I meant by the term active configuration. You can comment out the last line while using the code in production. Let’s explain what’s going on with each of the classes defined above. . AppConfig . The AppConfig class defines the config variables required for you API’s internal logic. In this case I’m not loading the variables from the .env file, rather defining them directly in the class. You can also define and import them from another app_configs.py file if necessary but they shouldn’t be placed in the .env file. For data validation to work, you’ve to inherit from Pydantic’s BaseModel and annotate the attributes using type hints while constructing the AppConfig class. Later, this class is called from the GlobalConfig class to build a nested data structure. . GlobalConfig . GlobalConfig defines the variables that propagates through other environment classes and the attributes of this class are globally accessible from all other environments. In this class, the variables are loaded from the .env file. In the .env file, global variables don’t have any environment specific prefixes like DEV_ or PROD_ before them. The class GlobalConfig inherits from Pydantic’s BaseSettings which helps to load and read the variables from the .env file. The .env file itself is loaded in the nested Config class. Although the environment variables are loaded from the .env file, Pydantic also loads your actual shell environment variables at the same time. From Pydantic’s [documentation]: . Even when using a dotenv file, Pydantic will still read environment variables as well as the dotenv file, environment variables will always take priority over values loaded from a dotenv file. . This means you can keep the sensitive variables in your .bashrc or zshrc and Pydantic will inject them during runtime. It’s a powerful feature, as it implies that you can easily keep the insensitive variables in your .env file and include that to the version control system. Meanwhile the sensitive information should be injected as a shell environment variable. For example, although I’ve defined an attribute called REDIS_PASS in the GlobalConfig class, there is no mention of any REDIS_PASS variable in the .env file. So normally, it returns None but you can easily inject a password into the REDIS_PASS variable from the shell. Assuming that you’ve set up your venv and installed the dependencies, you can test it by copying the contents of the above code snippet in file called configs.py and running the commands below: . export DEV_REDIS_PASS=ubuntu python configs.py . This should printout: . &gt;&gt;&gt; DevConfig(ENV_STATE=&#39;dev&#39;, APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), REDIS_PASS=&#39;ubuntu&#39;, REDIS_HOST=&#39;127.0.0.1&#39;, REDIS_PORT=4000) . Notice how your injected REDIS_PASS has appeared in the printed config class instance. Although I injected DEV_REDIS_PASS into the environment variable, it appeared as REDIS_PASS inside the DevConfig instance. This is convenient because you won’t need to change the name of the variables in your codebase when you change the environment. To understand why it printed an instance of the DevConfig class, refer to the FactoryConfig section. . DevConfig . DevConfig class inherits from the GlobalConfig class and it can define additional variables specific to the development environment. It inherits all the variables defined in the GlobalConfig class. In this case, the DevConfig class doesn’t define any new variable. . The nested Config class inside DevConfig defines an attribute env_prefix and assigns DEV_ prefix to it. This helps Pydantic to read your prefixed variables like DEV_REDIS_HOST, DEV_REDIS_PORT etc without you having to explicitly mention them. . ProdConfig . ProdConfig class also inherits from the GlobalConfig class and it can define additional variables specific to the production environment. It inherits all the variables defined in the GlobalConfig class. In this case, like DevConfig this class doesn’t define any new variable. . The nested Config class inside ProdConfig defines an attribute env_prefix and assigns PROD_ prefix to it. This helps Pydantic to read your prefixed variables like PROD_REDIS_HOST, PROD_REDIS_PORT etc without you having to explicitly mention them. . FactoryConfig . FactoryConfig is the controller class that dictates which config class should be activated based on the environment state defined as ENV_STATE in the .env file. If it finds ENV_STATE=&quot;dev&quot; then the control flow statements in the FactoryConfig class will activate the development configs (DevConfig). Similarly, if ENV_STATE=&quot;prod&quot; is found then the control flow will activate the production configs (ProdConfig). Since the current environment state is ENV_STATE=&quot;dev&quot;, when you run the code, it prints an instance of the activated DevConfig class. This way, you can assign different values to the same variable based on different environment contexts. . You can also dynamically change the environment by changing the value of ENV_STATE on your shell. Run: . EXPORT ENV_STATE=&quot;prod&quot; python configs.py . This time the config instance should change and print the following: . &gt;&gt;&gt; ProdConfig(ENV_STATE=&#39;prod&#39;, APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), REDIS_PASS=&#39;ubuntu&#39;, REDIS_HOST=&#39;127.0.0.2&#39;, REDIS_PORT=5000) . Accessing the Configs . Using the config variables is easy. Suppose you want use the variables in file called app.py. You can easily do so as shown in the following code block: . # app.py from configs import cnf APP_CONFIG = cnf.APP_CONFIG VAR_A = APP_CONFIG.VAR_A # this is a nested config VAR_B = APP_CONFIG.VAR_B REDIS_HOST = cnf.REDIS_HOST # this is a top-level config REDIS_PORT = cnf.REDIS_PORT print(APP_CONFIG) print(VAR_A) print(VAR_B) print(REDIS_HOST) print(REDIS_PORT) . This should print out: . &gt;&gt;&gt; ProdConfig(ENV_STATE=&#39;prod&#39;, APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), REDIS_PASS=&#39;ubuntu&#39;, REDIS_HOST=&#39;127.0.0.2&#39;, REDIS_PORT=5000) VAR_A=33 VAR_B=22.0 33 22.0 127.0.0.2 5000 . Extending the Pipeline . The modular design demonstrated above is easy to maintain and extend in my opinion. Previously, for simplicity, I’ve defined only two environment scopes; development and production. Let’s say you want to add the configs for your staging environment. . First you’ll need to add those staging variables to the .env file. . ... STAGE_REDIS_HOST=&quot;127.0.0.3&quot; STAGE_REDIS_PORT=&quot;6000&quot; ... . | Then you’ve to create a class named StageConfig that inherits from the GlobalConfig class. The architecture of the class is similar to that of the DevConfig or ProdConfig class. . # configs.py ... class StageConfig(GlobalConfig): &quot;&quot;&quot;Staging configurations.&quot;&quot;&quot; class Config: env_prefix : str = &quot;STAGE_&quot; ... . | Finally, you’ll need to insert an ENV_STATE logic into the control flow of the FactoryConfig class. See how I’ve appended another if-else block to the previous (prod) block. . # configs.py ... class FactoryConfig: &quot;&quot;&quot;Returns a config instance depending on the ENV_STATE variable.&quot;&quot;&quot; def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == &quot;dev&quot;: return DevConfig() elif self.env_state == &quot;prod&quot;: return ProdConfig() elif self.env_state == &quot;stage&quot; return StageConfig() ... . | . To see your new addition in action just change the ENV_STATE to “stage” in the .env file or export it to your shell environment. . export ENV_STATE=&quot;stage&quot; python configs.py . This will print out an instance of the class StageConfig. . Remarks . The above workflow works perfectly for my usage scenario. So subjectively, I feel like it’s an elegant solution to a very icky problem. Your mileage may vary. All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 20.04. . Resources . Settings management with Pydantic | Flask config management | .",
            "url": "https://rednafi.github.io/digressions/python/2020/06/03/python-configs.html",
            "relUrl": "/python/2020/06/03/python-configs.html",
            "date": " • Jun 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Effortless API Request Caching with Python & Redis",
            "content": "Updated on 2020-07-07: Replaced data[&quot;code&quot;] with data.get(&quot;code&quot;) . Recently, I was working with MapBox’s Route Optimization API. Basically, it tries to solve the traveling salesman problem where you provide the API with coordinates of multiple places and it returns a duration-optimized route between those locations. This is a perfect usecase where Redis caching can come handy. Redis is a blazingly fast, lightweight in-memory database with additional persistence options; making it a perfect candidate for the task at hand. Here, caching can save you from making redundant API requests and also, it can dramatically improve the response time as well. . I found that in my country, the optimized routes returned by the API do not change dramatically for at least for a couple of hours. So the workflow will look something like this: . Caching the API response in Redis using the key-value data structure. Here the requested coordinate-string will be the key and the response will be the corresponding value | Setting a timeout on the records | Serving new requests from cache if the records exist | Only send a new request to MapBox API if the response is not cached and then add that response to cache | . Setting Up Redis &amp; RedisInsight . To proceed with the above workflow, you’ll need to install and setup Redis database on your system. For monitoring the database, I’ll be using RedisInsight. The easiest way to setup Redis and RedisInsight is through docker and docker-compose. Here’s a docker-compose that you can use to setup everything with a single command. . # docker-compose.yml version: &quot;3.2&quot; services: redis: container_name: redis-cont image: &quot;redis:alpine&quot; command: redis-server --requirepass ubuntu environment: - REDIS_PASSWORD=ubuntu - REDIS_REPLICATION_MODE=master ports: - &quot;6379:6379&quot; volumes: # save redisearch data to your current working directory - ./redis-data:/data command: # Save if 100 keys are added in every 10 seconds - &quot;--save 10 100&quot; # Set password - &quot;--requirepass ubuntu&quot; redisinsight: # redis db visualization dashboard container_name: redisinsight-cont image: redislabs/redisinsight ports: - 8001:8001 volumes: - redisinsight:/db volumes: redis-data: redisinsight: . The above docker-compose file has two services, redis and redisinsight. I’ve set up the database with a dummy password ubuntu and made it persistent using a folder named redis-data in the current working directory. The database listens in localhost’s port 6379. You can monitor the database using redisinsight in port 8000. To spin up Redis and RedisInsight containers, run: . docker-compose up -d . This command will start the database and monitor accordingly. You can go to this localhost:8000 link using your browser and connect redisinsight to your database. After connecting your database, you should see a dashboard like this in your redisinsight panel: . . Preparing Python Environment . For local development, you can set up your python environment and install the dependencies using pip. Here, I’m on a Linux machine and using virtual environment for isolation. The following commands will work if you’re on a *nix based system and have python 3.8 installed on your system. This will install the necessary dependencies in a virtual environment: . python3.8 -m venv venv source venv/bin/activate pip install redis httpx . Workflow . Connecting Python Client to Redis . Assuming the database server is running and you’ve installed the dependencies, the following snippet connects redis-py client to the database. . import redis import sys def redis_connect() -&gt; redis.client.Redis: try: client = redis.Redis( host=&quot;localhost&quot;, port=6379, password=&quot;ubuntu&quot;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(&quot;AuthenticationError&quot;) sys.exit(1) client = redis_connect() . The above excerpt tries to connect to the Redis database server using the port 6379. Notice, how I’m providing the password ubuntu via the password argument. Here, client.ping() helps you determine if a connection has been established successfully. It returns True if a successful connection can be established or raises specific errors in case of failures. The above function handles AuthenticationError and prints out an error message if the error occurs. If everything goes well, running the redis_connect() function will return an instance of the redis.client.Redis class. This instance will be used later to set and retrieve data to and from the redis database. . Getting Route Data From MapBox API . The following function strikes the MapBox Route Optimization API and collects route data. . import httpx def get_routes_from_api(coordinates: str) -&gt; dict: &quot;&quot;&quot;Data from mapbox api.&quot;&quot;&quot; with httpx.Client() as client: base_url = &quot;https://api.mapbox.com/optimized-trips/v1/mapbox/driving&quot; geometries = &quot;geojson&quot; access_token = &quot;Your-MapBox-API-token&quot; url = f&quot;{base_url}/{coordinates}?geometries={geometries}&amp;access_token={access_token}&quot; response = client.get(url) return response.json() . The above code uses Python’s httpx library to make the get request. Httpx is almost a drop-in replacement for the ubiquitous Requests library but way faster and has async support. Here, I’ve used context manager httpx.Client() for better resource management while making the get request. You can read more about context managers and how to use them for hassle free resource management here. . The base_url is the base url of the route optimization API and the you’ll need to provide your own access token in the access_token field. Notice, how the url variable builds up the final request url. The coordinates are provided using the lat0,lon0;lat1,lon1;lat2,lon2... format. Rest of the function sends the http requests and converts the response into a native dictionary object using the response.json() method. . Setting &amp; Retrieving Data to &amp; from Redis Database . The following two functions retrieves data from and sets data to redis database respectively. . from datetime import timedelta def get_routes_from_cache(key: str) -&gt; str: &quot;&quot;&quot;Get data from redis.&quot;&quot;&quot; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -&gt; bool: &quot;&quot;&quot;Set data to redis.&quot;&quot;&quot; state = client.setex(key, timedelta(seconds=3600), value=value, ) return state . Here, both the keys and the values are strings. In the second function, set_routes_to_cache, the client.setex() method sets a timeout of 1 hour on the key. After that the key and its associated value get deleted automatically. . The Central Orchestration . The route_optima function is the primary agent that orchestrates and executes the caching and returning of responses against requests. It roughly follows the execution flow shown below. . . When a new request arrives, the function first checks if the return-value exists in the Redis cache. If the value exists, it shows the cached value, otherwise, it sends a new request to the MapBox API, cache that value and then shows the result. . def route_optima(coordinates: str) -&gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[&quot;cache&quot;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(&quot;code&quot;) == &quot;Ok&quot;: data[&quot;cache&quot;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data . Exposing As an API . This part of the code wraps the original Route Optimization API and exposes that as a new endpoint. I’ve used fastAPI to build the wrapper API. Doing this also hides the underlying details of authentication and the actual endpoint of the MapBox API. . from fastapi import FastAPI app = FastAPI() @app.get(&quot;/route-optima/{coordinates}&quot;) def view(coordinates): &quot;&quot;&quot;This will wrap our original route optimization API and incorporate Redis Caching. You&#39;ll only expose this API to the end user. &quot;&quot;&quot; # coordinates = &quot;90.3866,23.7182;90.3742,23.7461&quot; return route_optima(coordinates) . Putting It All Together . # app.py import json import sys from datetime import timedelta import httpx import redis from fastapi import FastAPI def redis_connect() -&gt; redis.client.Redis: try: client = redis.Redis( host=&quot;localhost&quot;, port=6379, password=&quot;ubuntu&quot;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(&quot;AuthenticationError&quot;) sys.exit(1) client = redis_connect() def get_routes_from_api(coordinates: str) -&gt; dict: &quot;&quot;&quot;Data from mapbox api.&quot;&quot;&quot; with httpx.Client() as client: base_url = &quot;https://api.mapbox.com/optimized-trips/v1/mapbox/driving&quot; geometries = &quot;geojson&quot; access_token = &quot;Your-MapBox-API-token&quot; url = f&quot;{base_url}/{coordinates}?geometries={geometries}&amp;access_token={access_token}&quot; response = client.get(url) return response.json() def get_routes_from_cache(key: str) -&gt; str: &quot;&quot;&quot;Data from redis.&quot;&quot;&quot; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -&gt; bool: &quot;&quot;&quot;Data to redis.&quot;&quot;&quot; state = client.setex(key, timedelta(seconds=3600), value=value,) return state def route_optima(coordinates: str) -&gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[&quot;cache&quot;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(&quot;code&quot;) == &quot;Ok&quot;: data[&quot;cache&quot;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data app = FastAPI() @app.get(&quot;/route-optima/{coordinates}&quot;) def view(coordinates: str) -&gt; dict: &quot;&quot;&quot;This will wrap our original route optimization API and incorporate Redis Caching. You&#39;ll only expose this API to the end user. &quot;&quot;&quot; # coordinates = &quot;90.3866,23.7182;90.3742,23.7461&quot; return route_optima(coordinates) . You can copy the complete code to a file named app.py and run the app using the command below (assuming redis, redisinsight is running and you’ve installed the dependencies beforehand): . uvicorn app.app:app --host 0.0.0.0 --port 5000 --reload . This will run a local server where you can send new request with coordinates. . Go to your browser and hit the endpoint with a set of new coordinates. For example: . http://localhost:5000/route-optima/90.3866,23.7182;90.3742,23.7461 . This should return a response with the coordinates of the optimized route. . { &quot;code&quot;:&quot;Ok&quot;, &quot;waypoints&quot;:[ { &quot;distance&quot;:26.041809241776583, &quot;name&quot;:&quot;&quot;, &quot;location&quot;:[ 90.386855, 23.718213 ], &quot;waypoint_index&quot;:0, &quot;trips_index&quot;:0 }, { &quot;distance&quot;:6.286653078791968, &quot;name&quot;:&quot;&quot;, &quot;location&quot;:[ 90.374253, 23.746129 ], &quot;waypoint_index&quot;:1, &quot;trips_index&quot;:0 } ], &quot;trips&quot;:[ { &quot;geometry&quot;:{ &quot;coordinates&quot;:[ [ 90.386855, 23.718213 ], &quot;... ...&quot; ], &quot;type&quot;:&quot;LineString&quot; }, &quot;legs&quot;:[ { &quot;summary&quot;:&quot;&quot;, &quot;weight&quot;:3303.1, &quot;duration&quot;:2842.8, &quot;steps&quot;:[ ], &quot;distance&quot;:5250.2 }, { &quot;summary&quot;:&quot;&quot;, &quot;weight&quot;:2536.5, &quot;duration&quot;:2297, &quot;steps&quot;:[ ], &quot;distance&quot;:4554.8 } ], &quot;weight_name&quot;:&quot;routability&quot;, &quot;weight&quot;:5839.6, &quot;duration&quot;:5139.8, &quot;distance&quot;:9805 } ], &quot;cache&quot;:false } . If you’ve hit the above URL for the first time, the cache attribute of the json response should show false. This means that the response is being served from the original MapBox API. However, hitting the same URL with the same coordinates again will show the cached response and this time the cache attribute should show true. . Inspection . Once you’ve got everything up and running you can inspect the cache via redis insight. To do so, go to the link below while your app server is running: . http://localhost:8000/ . Select the Browser panel from the left menu and click on a key of your cached data. It should show something like this: . . Also you can play around with the API in the swagger UI. To do so, go to the following link: . http://localhost:5000/docs . This will take you to the swagger dashboard. Here you can make requests using the interactive UI. Go ahead and inspect how the caching works for new coordinates. . . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 20.04. You can find the complete source code of the app here. . Disclaimer . This app has been made for demonstration purpose only. So it might not reflect the best practices of production ready applications. Using APIs without authentication like this is not recommended. . Resources . Http Request Caching with Redis | Httpx | Redis | RedisInsight | FastAPI | .",
            "url": "https://rednafi.github.io/digressions/python/database/2020/05/25/python-redis-cache.html",
            "relUrl": "/python/database/2020/05/25/python-redis-cache.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Untangling Python Decorators",
            "content": "Updated on 2020-06-25: Replaced time.time() with time.perf_counter() . When I first learned about Python decorators, using them felt like doing voodoo magic. Decorators can give you the ability to add new functionalities to any callable without actually touching or changing the code inside it. This can typically yield better encapsulation and help you write cleaner and more understandable code. However, decorator is considered as a fairly advanced topic in Python since understanding and writing it requires you to have command over multiple additional concepts like first class objects, higher order functions, closures etc. First, I’ll try to introduce these concepts as necessary and then unravel the core concept of decorator layer by layer. So let’s dive in. . First Class Objects . In Python, basically everything is an object and functions are regarded as first-class objects. It means that functions can be passed around and used as arguments, just like any other object (string, int, float, list, and so on). You can assign functions to variables and treat them like any other objects. Consider this example: . def func_a(): return &quot;I was angry with my friend.&quot; def func_b(): return &quot;I told my wrath, my wrath did end&quot; def func_c(*funcs): for func in funcs: print(func()) main_func = func_c main_func(func_a, func_b) . &gt;&gt;&gt; I was angry with my friend. &gt;&gt;&gt; I told my wrath, my wrath did end . The above example demonstrates how Python treats functions as first class citizens. First, I defined two functions, func_a and func_b and then func_c takes them as parameters. func_c runs the functions taken as parameters and prints the results. Then we assign func_c to variable main_func. Finally, we run main_func and it behaves just like func_c. . Higher Order Functions . Python also allows you to use functions as return values. You can take in another function and return that function or you can define a function within another function and return the inner function. . def higher(func): &quot;&quot;&quot;This is a higher order function. It returns another function. &quot;&quot;&quot; return func def lower(): return &quot;I&#39;m hunting high and low&quot; higher(lower) . &gt;&gt;&gt; &lt;function __main__.lower()&gt; . Now you can assign the result of higher to another variable and execute the output function. . h = higher(lower) h() . &gt;&gt;&gt; &quot;I&#39;m hunting high and low&quot; . Let’s look into another example where you can define a nested function within a function and return the nested function instead of its result. . def outer(): &quot;&quot;&quot;Define and return a nested function from another function.&quot;&quot;&quot; def inner(): return &quot;Hello from the inner func&quot; return inner inn = outer() inn() . &gt;&gt;&gt; &#39;Hello from the inner func&#39; . Notice how the nested function inner was defined inside the outer function and then the return statement of the outer function returned the nested function. After definition, to get to the nested function, first we called the outer function and received the result as another function. Then executing the result of the outer function prints out the message from the inner function. . Closures . You saw examples of inner functions at work in the previous section. Nested functions can access variables of the enclosing scope. In Python, these non-local variables are read only by default and we must declare them explicitly as non-local (using nonlocal keyword) in order to modify them. Following is an example of a nested function accessing a non-local variable. . def burger(name): def ingredients(): if name == &quot;deli&quot;: return (&quot;steak&quot;, &quot;pastrami&quot;, &quot;emmental&quot;) elif name == &quot;smashed&quot;: return (&quot;chicken&quot;, &quot;nacho cheese&quot;, &quot;jalapeno&quot;) else: return None return ingredients . Now run the function, . ingr = burger(&quot;deli&quot;) ingr() . &gt;&gt;&gt; (&#39;steak&#39;, &#39;pastrami&#39;, &#39;emmental&#39;) . Well, that’s unusual. . The burger function was called with the string deli and the returned function was bound to the name ingr. On calling ingr(), the message was still remembered and used to derive the outcome although the outer function burger had already finished its execution. . This technique by which some data (“deli”) gets attached to the code is called closure in Python. The value in the enclosing scope is remembered even when the variable goes out of scope or the function itself is removed from the current namespace. Decorators uses the idea of non-local variables multiple times and soon you’ll see how. . Writing a Basic Decorator . With these prerequisites out of the way, let’s go ahead and create your first simple decorator. . def deco(func): def wrapper(): print(&quot;This will get printed before the function is called.&quot;) func() print(&quot;This will get printed after the function is called.&quot;) return wrapper . Before using the decorator, let’s define a simple function without any parameters. . def ans(): print(42) . Treating the functions as first-class objects, you can use your decorator like this: . ans = deco(ans) ans() . &gt;&gt;&gt; This will get printed before the function is called. 42 This will get printed after the function is called. . In the above two lines, you can see a very simple decorator in action. Our deco function takes in a target function, manipulates the target function inside a wrapper function and then returns the wrapper function. Running the function returned by the decorator, you will get your modified result. To put it simply, decorators wraps a function and modifies its behavior. . The decorator function runs at the time the decorated function is imported/defined, not when it is called. . Before moving onto the next section, let’s see how we can get the return value of target function instead of just printing it. . def deco(func): &quot;&quot;&quot;This modified decorator also returns the result of func.&quot;&quot;&quot; def wrapper(): print(&quot;This will get printed before the function is called.&quot;) ret = func() print(&quot;This will get printed after the function is called.&quot;) return ret return wrapper def ans(): return 42 . In the above example, the wrapper function returns the result of the target function and the wrapper itself. This makes it possible to get the result of the modified function. . ans = deco(ans) print(ans()) . &gt;&gt;&gt; This will get printed before the function is called. This will get printed after the function is called. 42 . Can you guess why the return value of the decorated function appeared in the last line instead of in the middle like before? . The @ Syntactic Sugar . The way you’ve used decorator in the last section might feel a little clunky. First, you have to type the name ans three times to call and use the decorator. Also, it becomes harder to tell apart where the decorator is actually working. So Python allows you to use decorator with the special syntax @. You can apply your decorators while defining your functions, like this: . @deco def func(): ... # Now call your decorated function just like a normal one func() . Sometimes the above syntax is called the pie syntax and it’s just a syntactic sugar for func = deco(func). . Decorating Functions with Arguments . The naive decorator that we’ve implemented above will only work for functions that take no arguments. It’ll fail and raise TypeError if your try to decorate a function having arguments with deco. Now let’s create another decorator called yell which will take in a function that returns a string value and transform that string value to uppercase. . def yell(func): def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + &quot;!&quot; return ret return wrapper . Create the target function that returns string value. . @yell def hello(name): return f&quot;Hello {name}&quot; . hello(&quot;redowan&quot;) . &gt;&gt;&gt; &#39;HELLO REDOWAN!&#39; . Function hello takes a name:string as parameter and returns a message as string. Look how the yell decorator is modifying the original return string, transforming that to uppercase and adding an extra ! sign without directly changing any code in the hello function. . Solving Identity Crisis . In Python, you can introspect any object and its properties via the interactive shell. A function knows its identity, docstring etc. For instance, you can inspect the built in print function in the following ways: . print . &gt;&gt;&gt; &lt;function print&gt; . print.__name__ . &gt;&gt;&gt; &#39;print&#39; . print.__doc__ . &gt;&gt;&gt; &quot;print(value, ..., sep=&#39; &#39;, end=&#39; n&#39;, file=sys.stdout, flush=False) n nPrints the values to a stream, or to sys.stdout by default. nOptional keyword arguments: nfile: a file-like object (stream); defaults to the current sys.stdout. nsep: string inserted between values, default a space. nend: string appended after the last value, default a newline. nflush: whether to forcibly flush the stream.&quot; . help(print) . &gt;&gt;&gt; Help on built-in function print in module builtins: print(...) print(value, ..., sep=&#39; &#39;, end=&#39; n&#39;, file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. . This introspection works similarly for functions that you defined yourself. I’ll be using the previously defined hello function. . hello.__name__ . &gt;&gt;&gt; &#39;wrapper&#39; . help(hello) . &gt;&gt;&gt; Help on function wrapper in module __main__: wrapper(*args, **kwargs) . Now what’s going on there. The decorator yell has made the function hello confused about its own identity. Instead of reporting its own name, it takes the identity of the inner function wrapper. This can be confusing while doing debugging. You can fix this by using builtin wraps decorator from the functools module. This will make sure that the original identity of the decorated function stays preserved. . from functools import wraps def yell(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + &quot;!&quot; return ret return wrapper @yell def hello(name): &quot;Hello from the other side.&quot; return f&quot;Hello {name}&quot; . hello(&quot;Galaxy&quot;) . &gt;&gt;&gt; &#39;HELLO GALAXY!&#39; . Introspecting the hello function decorated with modified decorator will give you the desired result. . hello.__name__ . &gt;&gt;&gt; &#39;hello&#39; . help(hello) . &gt;&gt;&gt; Help on function hello in module __main__: hello(name) Hello from the other side. . Decorators in the Wild . Before moving on to the next section let’s see a few real world examples of decorators. To define all the decorators, we’ll be using the following template that we’ve perfected so far. . from functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): # Do something before ret = func(*args, **kwargs) # Do something after return ret return wrapper . Timer . Timer decorator will help you time your callables in a non-intrusive way. It can help you while debugging and profiling your functions. . from time import perf_counter from functools import wraps def timer(func): &quot;&quot;&quot;This decorator prints out the execution time of a callable.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): start_time = perf_counter() ret = func(*args, **kwargs) end_time = perf_counter() run_time = end_time - start_time print(f&quot;Finished running {func.__name__} in {run_time:.4f} seconds.&quot;) return ret return wrapper @timer def dothings(n_times): for _ in range(n_times): return sum((i ** 3 for i in range(100_000))) . In the above way, we can introspect the time it requires for function dothings to complete its execution. . dothings(100_000) . &gt;&gt;&gt; Finished running dothings in 0.0353 seconds. 24999500002500000000 . Exception Logger . Just like the timer decorator, we can define a logger decorator that will log the state of a callable. For this demonstration, I’ll be defining a exception logger that will show additional information like timestamp, argument names when an exception occurs inside of the decorated callable. . from functools import wraps from datetime import datetime def logexc(func): @wraps(func) def wrapper(*args, **kwargs): # Stringify the arguments args_rep = [repr(arg) for arg in args] kwargs_rep = [f&quot;{k}={v!r}&quot; for k, v in kwargs.items()] sig = &quot;, &quot;.join(args_rep + kwargs_rep) # Try running the function try: return func(*args, **kwargs) except Exception as e: print(&quot;Time: &quot;, datetime.now().strftime(&quot;%Y-%m-%d [%H:%M:%S]&quot;)) print(&quot;Arguments: &quot;, sig) print(&quot;Error: n&quot;) raise return wrapper @logexc def divint(a, b): return a / b . Let’s invoke ZeroDivisionError to see the logger in action. . divint(1, 0) . &gt;&gt;&gt; Time: 2020-05-12 [12:03:31] Arguments: 1, 0 Error: ZeroDivisionError Traceback (most recent call last) .... . The decorator first prints a few info regarding the function and then raises the original error. . Validation &amp; Runtime Checks . Python’s type system is strongly typed, but very dynamic. For all its benefits, this means some bugs can try to creep in, which more statically typed languages (like Java) would catch at compile time. Looking beyond even that, you may want to enforce more sophisticated, custom checks on data going in or out. Decorators can let you easily handle all of this, and apply it to many functions at once. . Imagine this: you have a set of functions, each returning a dictionary, which (among other fields) includes a field called “summary.” The value of this summary must not be more than 30 characters long; if violated, that’s an error. Here is a decorator that raises a ValueError if that happens: . from functools import wraps def validate_summary(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) if len(ret[&quot;summary&quot;]) &gt; 30: raise ValueError(&quot;Summary exceeds 30 character limit.&quot;) return ret return wrapper @validate_summary def short_summary(): return {&quot;summary&quot;: &quot;This is a short summary&quot;} @validate_summary def long_summary(): return {&quot;summary&quot;: &quot;This is a long summary that exceeds character limit.&quot;} print(short_summary()) print(long_summary()) . &gt;&gt;&gt; {&#39;summary&#39;: &#39;This is a short summary&#39;} - ValueError Traceback (most recent call last) &lt;ipython-input-178-7375d8e2a623&gt; in &lt;module&gt; 19 20 print(short_summary()) &gt; 21 print(long_summary()) ... . Retry . Imagine a situation where your defined callable fails due to some I/O related issues and you’d like to retry that again. Decorator can help you to achieve that in a reusable manner. Let’s define a retry decorator that will rerun the decorated function multiple times if an http error occurs. . from functools import wraps import requests def retry(func): &quot;&quot;&quot;This will rerun the decorated callable 3 times if the callable encounters http 500/404 error.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): n_tries = 3 tries = 0 while True: resp = func(*args, **kwargs) if resp.status_code == 500 or resp.status_code == 404 and tries &lt; n_tries: print(f&quot;retrying... ({tries})&quot;) tries += 1 continue break return resp return wrapper @retry def getdata(url): resp = requests.get(url) return resp resp = getdata(&quot;https://httpbin.org/get/1&quot;) resp.text . &gt;&gt;&gt; retrying... (0) retrying... (1) retrying... (2) &#39;&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt; n&lt;title&gt;404 Not Found&lt;/title&gt; n&lt;h1&gt;Not Found&lt;/h1&gt; n&lt;p&gt;The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.&lt;/p&gt; n&#39; . Applying Multiple Decorators . You can apply multiple decorators to a function by stacking them on top of each other. Let’s define two simple decorators and use them both on a function. . from functools import wraps def greet(func): &quot;&quot;&quot;Greet in English.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return &quot;Hello &quot; + ret + &quot;!&quot; return wrapper def flare(func): &quot;&quot;&quot;Add flares to the string.&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return &quot;🎉 &quot; + ret + &quot; 🎉&quot; return wrapper @flare @greet def getname(name): return name getname(&quot;Nafi&quot;) . &gt;&gt;&gt; &#39;🎉 Hello Nafi! 🎉&#39; . The decorators are called in a bottom up order. First, the decorator greet gets applied on the result of getname function and then the result of greet gets passed to the flare decorator. The decorator stack above can be written as flare(greet(getname(name))). Change the order of the decorators and see what happens! . Decorators with Arguments . While defining the retry decorator in the previous section, you may have noticed that I’ve hard coded the number of times I’d like the function to retry if an error occurs. It’d be handy if you could inject the number of tries as a parameter into the decorator and make it work accordingly. This is not a trivial task and you’ll need three levels of nested functions to achieve that. . Before doing that let’s cook up a trivial example of how you can define decorators with parameters. . from functools import wraps def joinby(delimiter=&quot; &quot;): &quot;&quot;&quot;This decorator splits the string output of the decorated function by a single space and then joins them using a user specified delimiter.&quot;&quot;&quot; def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(&quot; &quot;) ret = delimiter.join(ret) return ret return inner_wrapper return outer_wrapper @joinby(delimiter=&quot;,&quot;) def hello(name): return f&quot;Hello {name}!&quot; @joinby(delimiter=&quot;&gt;&quot;) def greet(name): return f&quot;Greetings {name}!&quot; @joinby() def goodbye(name): return f&quot;Goodbye {name}!&quot; print(hello(&quot;Nafi&quot;)) print(greet(&quot;Redowan&quot;)) print(goodbye(&quot;Delowar&quot;)) . &gt;&gt;&gt; Hello,Nafi! Greetings&gt;Redowan! Goodbye Delowar! . The decorator joinby takes a single parameter called delimiter. It splits the string output of the decorated function by a single space and then joins them using the user defined delimiter specified in the delimiter argument. The three layer nested definition looks scary but we’ll get to that in a moment. Notice how you can use the decorator with different parameters. In the above example, I’ve defined three different functions to demonstrate the usage of joinby. It’s important to note that in case of a decorator that takes parameters, you’ll always need to pass something to it and even if you don’t want to pass any parameter (run with the default), you’ll still need to decorate your function with deco() instead of deco. Try changing the decorator on the goodbye function from joinby() to joinby and see what happens. . Typically, a decorator creates and returns an inner wrapper function but here in the repeat decorator, there is an inner function within another inner function. This almost looks like a dream within a dream from the movie Inception. . There are a few subtle things happening in the joinby() function: . Defining outer_wrapper() as an inner function means that repeat() will refer to a function object outer_wrapper. . | The delimiter argument is seemingly not used in joinby() itself. But by passing delimiter a closure is created where the value of delimiter is stored until it will be used later by inner_wrapper() . | . Decorators with &amp; without Arguments . You saw earlier that a decorator specifically designed to take parameters can’t be used without parameters; you need to at least apply parenthesis after the decorator deco() to use it without explicitly providing the arguments. But what if you want to design one that can used both with and without arguments. Let’s redefine the joinby decorator so that you can use it with parameters or just like an ordinary parameter-less decorator that we’ve seen before. . from functools import wraps def joinby(_func=None, *, delimiter=&quot; &quot;): &quot;&quot;&quot;This decorator splits the string output of a function by a single space and then joins that using a user specified delimiter.&quot;&quot;&quot; def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(&quot; &quot;) ret = delimiter.join(ret) return ret return inner_wrapper # This part enables you to use the decorator with/without arguments if _func is None: return outer_wrapper else: return outer_wrapper(_func) @joinby(delimiter=&quot;,&quot;) def hello(name): return f&quot;Hello {name}!&quot; @joinby def greet(name): return f&quot;Greetings {name}!&quot; print(hello(&quot;Nafi&quot;)) print(greet(&quot;Redowan&quot;)) . &gt;&gt;&gt; Hello,Nafi! Greetings Redowan! . Here, the _func argument acts as a marker, noting whether the decorator has been called with arguments or not: . If joinby has been called without arguments, the decorated function will be passed in as _func. If it has been called with arguments, then _func will be None. The * in the argument list means that the remaining arguments can’t be called as positional arguments. This time you can use joinby with or without arguments and function hello and greet above demonstrate that. . A Generic Pattern . Personally, I find it cumbersome how you need three layers of nested functions to define a generalized decorator that can be used with or without arguments. David Beazly in his book Python Cookbook shows an excellent way to define generalized decorators without writing three levels of nested functions. It uses the built in functools.partial function to achieve that. The following is a pattern you can use to define generalized decorators in a more elegant way: . from functools import wraps def decorator(func=None, foo=&quot;spam&quot;): if func is None: return functools.partial(decorator, foo=foo) @wraps(func) def wrapper(*args, **kwargs): # Do something with `func` and `foo`, if you&#39;re so inclined pass return wrapper # Applying decorator without any parameter @decorator def f(*args, **kwargs): pass # Applying decorator with extra parameter @decorator(foo=&quot;buzz&quot;) def f(*args, **kwargs): pass . Let’s redefine our retry decorator using this pattern. . from functools import wraps def retry(func=None, n_tries=4): if func is None: return functools.partial(retry, n_tries=n_tries) @wraps(func) def wrapper(*args, **kwargs): tries = 0 while True: ret = func(*args, **kwargs) if ret.status_code == 500 or ret.status_code == 404 and tries &lt; n_tries: print(f&quot;retrying... ({tries})&quot;) tries += 1 continue break return ret return wrapper @retry def getdata(url): resp = requests.get(url) return resp @retry(n_tries=2) def getdata_(url): resp = requests.get(url) return resp resp1 = getdata(&quot;https://httpbin.org/get/1&quot;) print(&quot;--&quot;) resp2 = getdata_(&quot;https://httpbin.org/get/1&quot;) . &gt;&gt;&gt; retrying... (0) retrying... (1) retrying... (2) retrying... (3) -- retrying... (0) retrying... (1) . In this case, you do not have to write three level nested functions and the functools.partial takes care of that. Partials can be used to make new derived functions that have some input parameters pre-assigned.Roughly partial does the following: . def partial(func, *part_args): def wrapper(*extra_args): args = list(part_args) args.extend(extra_args) return func(*args) return wrapper . This eliminates the need to write multiple layers of nested factory function get a generalized decorator. . Defining Decorators with Classes . This time, I’ll be using a class to compose a decorator. Classes can be handy to avoid nested architecture while writing decorators. Also, it can be helpful to use a class while writing stateful decorators. You can follow the pattern below to compose decorators with classes. . from functools import update_wrapper class ClassDeco: def __init__(self, func): update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): # You can add some code before the function call ret = self.func(*args, **kwargs) # You can also add some code after the function call return ret . Let’s use the above template to write a decorator named Emphasis that will add bold tags &lt;b&gt;&lt;/b&gt;to the string output of a function. . from functools import update_wrapper class Emphasis: def __init__(self, func): update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): ret = self.func(*args, **kwargs) return &quot;&lt;b&gt;&quot; + ret + &quot;&lt;/b&gt;&quot; @Emphasis def hello(name): return f&quot;Hello {name}&quot; print(hello(&quot;Nafi&quot;)) print(hello(&quot;Redowan&quot;)) . &gt;&gt;&gt; &lt;b&gt;Hello Nafi&lt;/b&gt; &lt;b&gt;Hello Redowan&lt;/b&gt; . The init() method stores a reference to the function num_calls and can do other necessary initialization. The call() method will be called instead of the decorated function. It does essentially the same thing as the wrapper() function in our earlier examples. Note that you need to use the functools.update_wrapper() function instead of @functools.wraps. . Before moving on, let’s write a stateful decorator using classes. Stateful decorators can remember the state of their previous run. Here’s a stateful decorator called Tally that will keep track of the number of times decorated functions are called in a dictionary. The keys of the dictionary will hold the names of the functions and the corresponding values will hold the call count. . from functools import update_wrapper class Tally: def __init__(self, func): update_wrapper(self, func) self.func = func self.tally = {} self.n_calls = 0 def __call__(self, *args, **kwargs): self.n_calls += 1 self.tally[self.func.__name__] = self.n_calls print(&quot;Callable Tally:&quot;, self.tally) return self.func(*args, **kwargs) @Tally def hello(name): return f&quot;Hello {name}!&quot; print(hello(&quot;Redowan&quot;)) print(hello(&quot;Nafi&quot;)) . &gt;&gt;&gt; Callable Tally: {&#39;hello&#39;: 1} Hello Redowan! Callable Tally: {&#39;hello&#39;: 2} Hello Nafi! . A Few More Examples . Caching Return Values . Decorators can provide an elegant way of memoizing function return values. Imagine you have an expensive API and you’d like call that as few times as possible. The idea is to save and cache values returned by the API for particular arguments, so that if those arguments appear again, you can serve the results from the cache instead of calling the API again. This can dramatically improve your applications’ performance. Here I’ve simulated an expensive API call and provided caching with a decorator. . import time def api(a): &quot;&quot;&quot;API takes an integer and returns the square value of it. To simulate a time consuming process, I&#39;ve added some time delay to it.&quot;&quot;&quot; print(&quot;The API has been called...&quot;) # This will delay 3 seconds time.sleep(3) return a * a api(3) . &gt;&gt;&gt; The API has been called... 9 . You’ll see that running this function takes roughly 3 seconds. To cache the result , we can use Python’s built in functools.lru_cache to save the result against an argument in a dictionary and serve that when it encounters the same argument again. The only drawback here is, all the arguments need to be hashable. . from functools import lru_cache @lru_cache(maxsize=32) def api(a): &quot;&quot;&quot;API takes an integer and returns the square value of it. To simulate a time consuming process, I&#39;ve added some time delay to it.&quot;&quot;&quot; print(&quot;The API has been called...&quot;) # This will delay 3 seconds time.sleep(3) return a * a api(3) . &gt;&gt;&gt; 9 . Least Recently Used (LRU) Cache organizes items in order of use, allowing you to quickly identify which item hasn’t been used for the longest amount of time. In the above case, the parameter max_size refers to the maximum numbers of responses to be saved up before it starts deleting the earliest ones. While you run the decorated function, you’ll see first time it’ll take roughly 3 seconds to return the result. But if you rerun the function again with the same parameter it’ll spit the result from the cache almost instantly. . Unit Conversion . The following decorator converts length from SI units to multiple other units without polluting your target function with conversion logics. . from functools import wraps, partial def convert(func=None, convert_to=None): &quot;&quot;&quot;This converts value from meter to others.&quot;&quot;&quot; if func is None: return partial(convert, convert_to=convert_to) @wraps(func) def wrapper(*args, **kwargs): print(f&quot;Conversion unit: {convert_to}&quot;) ret = func(*args, **kwargs) # Adding conversion rules if convert_to is None: return ret elif convert_to == &quot;km&quot;: return ret / 1000 elif convert_to == &quot;mile&quot;: return ret * 0.000621371 elif convert_to == &quot;cm&quot;: return ret * 100 elif convert_to == &quot;mm&quot;: return ret * 1000 else: raise ValueError(&quot;Conversion unit is not supported.&quot;) return wrapper . Let’s use that on a function that returns the area of a rectangle. . @convert(convert_to=&quot;mile&quot;) def area(a, b): return a * b area(1, 2) . &gt;&gt;&gt; Conversion unit: mile 0.001242742 . Using the convert decorator on the area function shows how it prints out the transformation unit before returning the desired result. Experiment with other conversion units and see what happens. . Function Registration . The following is an example of registering logger function in Flask framework. The decorator register_logger doesn’t make any change to the decorated logger function. Rather it takes the function and registers it in a list called logger_list every time it’s invoked. . from flask import Flask, request app = Flask(__name__) logger_list = [] def register_logger(func): logger_list.append(func) return func def run_loggers(request): for logger in logger_list: logger(request) @register_logger def logger(request): print(request.method, request.path) @app.route(&quot;/&quot;) def index(): run_loggers(request) return &quot;Hello World!&quot; if __name__ == &quot;__main__&quot;: app.run(host=&quot;localhost&quot;, port=&quot;5000&quot;) . If you run the server and hit the http://localhost:5000/ url, it’ll greet you with a Hello World! message. Also you’ll able to see the printed method and path of your http request on the terminal. Moreover, if you inspect the logger_list, you’ll find the registered logger there. You’ll find a lot more real life usage of decorators in the Flask framework. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 20.04. . References . Primer on Python Decorator - Real Python | Decorators in Python - DataCamp | 5 Reasons You Need to Write Python Decorators | .",
            "url": "https://rednafi.github.io/digressions/python/2020/05/13/python-decorators.html",
            "relUrl": "/python/2020/05/13/python-decorators.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Migrating From Medium & Other Maladies",
            "content": "I picked up Medium primarily for dumping my thoughts and learnings immediately after discovering it back in 2016. I have published several pieces of writings on software engineering and garnered a healthy amount of audience there. Up until very recently, it has served me quite well. Medium aims to democratize online content writing by lowering the access barrier for the non-technical writers and does a fairly good job in doing so. It’s extremely easy to fire up Medium’s built-in editor and just start writing. There’s nothing to install or configure as everything is embedded into their website. The formatting tools are not fancy but adequate and the process of publishing involves merely adding a few tags and hitting the designated button. This reduction of friction usually means less focus on formatting, deployment, hosting and more focus on the actual writing. May be that’s why people outside of the tech sphere have embraced the platform wholeheartedly. Also, the platform has fantastic SEO and no matter what you are writing about, it’s almost guaranteed to gather a few eyeballs around it. There are a number of publications dedicated to different topics and they make the process of building up an audience even easier. However, the good stuffs probably end there. . Although Medium has been adopted by numerous technical writers and publications, it wasn’t necessarily built targeting this group and doesn’t consider them as first class citizens of the platform. I primarily write about Python, data science, software development and infosec in general. There’re multiple pain points that eventually forced me to steer away from Medium and look for better alternatives. First, It doesn’t support markdown syntax and the bare-bone formatting tools can get in the way of writing contents that have code snippets or require custom formatting. Secondly, there’s no built-in support for code syntax highlighting and you have to embed your code snippets as github gists. Managing and maintaining all these random gists can be a lot of work if you have many sizeable blogs that contain code snippets. Also, there’s no proper support for Latex syntax to render mathematical equations. That’s a huge deal breaker for me since many of my Data Science blogs use mathematical equations to explain disparate concepts. Usually, you get around this by converting the equations to images and embedding them in the blogs. But it completely borks mobile device readability. Another thing is that you don’t control your contents in the platform, Medium hosts and manages them for you. It can be both a blessing and a curse. Blessing because you don’t have to worry about deploying, hosting or managing your writings but at the same time you lose control over them too. Medium can censor you without you even knowing and as of writing this rant, you can only export your content to html, not to markdown format. So, if you ever think of leaving the platform, migrating can be big pain in the rear. . For me, the final nail in the coffin was their introduction of the premium tier. Until then reading and writing on Medium was, although problematic but quite manageable. Then they started blasting these premium subscription banners right at your face. This incessant pestering for subscribing turned into an unavoidable nuisance pretty quickly. Now, I don’t know about you, but most of the contents Medium displays in my feed are premium contents. Often I open an article just to arrive at a dead end that hides the rest of the content behind a paywall. Then I started paying 5$ per month to get rid of these, only to be disappointed by a plethora of low quality articles that were previously hidden behind the paywall. I’m all in for a subscription based business model but it seems like Medium tries its best to make you feel almost sorry for yourself if you’re not paying. . So, I finally took the step and wanted to get out of the platform even at the cost of losing a few visitors in the process and started looking for other options. I had a few almost non-negotiable requirements in my mind. . First, I needed absolute control over all of my contents, which means writing them using my favorite text editor (VSCode). Secondly, complete access to the contents in their raw format is mandatory. Markdown had to be the format of choice since it’s fast and easy to write without losing focus while tinkering with bolts and knobs of different custom menus of the in-built editor that usually these blogging platform offers. Also, Latex support and image rendering was crucial for me as several of my blogs contained charts and mathematical equations in them. It also had to be mobile friendly. Finally, the deployment and publication procedures should be almost as easy as Medium, means, I wouldn’t have to deal with the complexities that might arise while hosting and deployment when I just wanted to get the job done. . So, I started exploring multiple options. Since I primarily work with Python, I was looking for a Python based blogging framework and discovered Pelican. However, to my disappointment, almost all of the themes pelican offered were either ugly, didn’t work on phones, unmaintained or several years old. So, I shifted my focus on some of the newer frameworks like Hugo and Gatsby. Hugo is written in Go and the ecosystem is pretty darn cool. It has a huge collection of official and community built themes that work almost out of the box. Gatsby is even better if you are comfortable with Javascript and React. I picked up Hugo and started a building a brand new blog. Unfortunately, I quickly became obsessed with the process of building a new blog and began tinkering and exploring the framework a little too much. Changing the themes multiple times, customizing them with CSS, changing colors and stuff etc. I focused more on the building process than the writings itself. Then there was this messy process of deployment. Since I wanted full control over my contents, I decided to deploy the blog using Github pages. Hugo supports deployments using gh-pages but I couldn’t find any straight-forward CI formula that worked out of the box. So, I wrote something myself to automate the deployment but wasn’t very happy with the overall speed of the entire process. Dealing with deployment issues weren’t exactly the best experience when I just wanted to push my darn contents. . While I was again in search of a simple tool to make my own blog, I found this tweet where Hamel Hussain and Jeremy P Howard announced fastpages. It has pretty much everything that I want in a tool to architect my blog. The UI is super simple, it supports blogging in multiple formats, I can write and preserve my blogs as markdown files, jupyter notebooks or even as microsoft word documents. The UI also looks great on mobile devices and the it hosts the blog in Github. I don’t have to get out of my text editor to write a blog and the CI works out of the box. This makes the entire hosting and deployment completely automatic and hassle free. Now I just write my contents in markdown or jupyter notebook format and push it to the master branch. The CI takes care of the rest of the things. . Fastpages is very opinionated. It exposes a few loose strands for you to customize and the philosophy here is to bootstrap you with a minimum set of toolkit required to perform the tasks at hand. Also, it’s the perfect blogging tool if you work in the data science paradigm since it has the ability to directly turn your jupyter notebooks into beautifully formatted static blogs. However, the process of migrating my previous blogs was a chore. I haven’t yet fully migrated all of the writings as Medium doesn’t export the contents to markdown format. I tried a few CLIs to convert the html documents to markdown but the results weren’t satisfactory. So copy paste is my only friend for now. However, I’m quite enjoying the process of writing new posts and publishing them with a single git push origin master. Also, my Google Analytics is already showing a pretty encouraging amount of traffic here. Not regretting the journey and definitely not going back. . I still read Medium blogs from time to time, especially when the contents come from someone I already know. However, my days of writing anything there is over. Adios, Medium, It’s been a pleasure! .",
            "url": "https://rednafi.github.io/digressions/meta/2020/04/24/leaving-medium.html",
            "relUrl": "/meta/2020/04/24/leaving-medium.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Effortless Concurrency with Python's concurrent.futures",
            "content": "Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all these icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect: . In Python, if the task at hand is I/O bound, you can use use standard library’s threading module or if the task is CPU bound then multiprocessing module can be your friend. These threading and multiprocessing APIs give you a lot of control and flexibility but they come at the cost of having to write relatively low-level verbose code that adds extra layers of complexity on top of your core logic. Sometimes when the target task is complicated, it’s often impossible to avoid complexity while adding concurrency. However, a lot of simpler tasks can be made concurrent without adding too much extra overhead. . Python standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for providing the developers a high-level interface to launch asynchronous tasks. It’s a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It’s the perfect tool when you just want to run a piece of eligible code concurrently and don’t need the added modularity that the threading and multiprocessing APIs expose. . Anatomy of concurrent.futures . From the official docs, . The concurrent.futures module provides a high-level interface for asynchronously executing callables. . What it means is you can run your subroutines asynchronously using either threads or processes through a common high-level interface. Basically, the module provides an abstract class called Executor. You can’t instantiate it directly, rather you need to use one of two subclasses that it provides to run your tasks. . Executor (Abstract Base Class) │ ├── ThreadPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage I/O bound tasks with threading underneath │ ├── ProcessPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage CPU bound tasks with multiprocessing underneath . Internally, these two classes interact with the pools and manage the workers. Futures are used for managing results computed by the workers. To use a pool of workers, an application creates an instance of the appropriate executor class and then submits them for it to run. When each task is started, a Future instance is returned. When the result of the task is needed, an application can use the Future object to block until the result is available. Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects do not need to be managed directly. . Executor Objects . Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I’ll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim. . submit(fn, args, *kwargs) . Schedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable. . with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(pow, 323, 1235) print(future.result()) . map(func, *iterables, timeout=None, chunksize=1) . Similar to map(func, *iterables) except: . the iterables are collected immediately rather than lazily; | func is executed asynchronously and several calls to func may be made concurrently. . The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn’t available after timeout seconds from the original call to Executor.map(). Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time. . If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator. . When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . | . Generic Workflows for Running Tasks Concurrently . A lot of my scripts contains some variants of the following: . for task in get_tasks(): perform(task) . Here, get_tasks returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. Tasks are usually blocking callables and they run one after another, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming. . A general rule of thumb is using ThreadPoolExecutor when the tasks are primarily I/O bound like - sending multiple http requests to many urls, saving a large number of files to disk etc. ProcessPoolExecutor should be used in tasks that are primarily CPU bound like - running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc. . Running Tasks with Executor.submit . When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task) for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): print(f&quot;The outcome is {fut.result()}&quot;) . Here you start by creating an Executor, which manages all the tasks that are running – either in separate processes or threads. Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling the executor.shutdown() method implicitly when you’re done. . In real code, you’d would need to replace the Executor with ThreadPoolExecutor or a ProcessPoolExecutor depending on the nature of the callables. Then a set comprehension has been used here to start all the tasks. The executor.submit() method schedules each task. This creates a Future object, which represents the task to be done. Once all the tasks have been scheduled, the method concurrent.futures_as_completed() is called, which yields the futures as they’re done – that is, as each task completes. The fut.result() method gives you the return value of perform(task), or throws an exception in case of failure. . The executor.submit() method schedules the tasks asynchronously and doesn’t hold any contexts regarding the original tasks. So if you want to map the results with the original tasks, you need to track those yourself. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task): task for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): original_task = futures[fut] print(f&quot;The result of {original_task} is {fut.result()}&quot;) . Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary. . Running Tasks with Executor.map . Another way the results can be collected in the same order they’re scheduled is via using executor.map() method. . import concurrent.futures with concurrent.futures.Executor() as executor: for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())): print(f&quot;The result of {arg} is {res}&quot;) . Notice how the map function takes the entire iterable at once. It spits out the results immediately rather than lazily and in the same order they’re scheduled. If any unhandled exception occurs during the operation, it’ll also be raised immediately and the execution won’t go any further. . In Python 3.5+, executor.map() receives an optional argument: chunksize. While using ProcessPoolExecutor, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . A Few Real World Examples . Before proceeding with the examples, let’s write a small decorator that’ll be helpful to measure and compare the execution time between concurrent and sequential code. . import time from functools import wraps def timeit(method): @wraps(method) def wrapper(*args, **kwargs): start_time = time.time() result = method(*args, **kwargs) end_time = time.time() print(f&quot;{method.__name__} =&gt; {(end_time-start_time)*1000} ms&quot;) return result return wrapper . The decorator can be used like this: . @timeit def func(n): return list(range(n)) . This will print out the name of the method and how long it took to execute it. . Download &amp; Save Files from URLs with Multi-threading . First, let’s download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we’ll be using the ThreadPoolExecutor class to carry out the operation. But before that, let’s do this sequentially first. . from pathlib import Path import urllib.request def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): return [download_one(url) for url in urls] if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 22850.6863117218 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk. The second function download_all just iterates through a sequence of URLs and applies the download_one function on each of them. The sequential code takes about 22.8 seconds to run. Now let’s see how our threaded version of the same code performs. . from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor, as_completed def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): &quot;&quot;&quot; Create a thread pool and download specified urls &quot;&quot;&quot; with ThreadPoolExecutor(max_workers=13) as executor: return executor.map(download_one, urls, timeout=60) if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 5042.651653289795 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . The concurrent version of the code takes only about 1/4 th the time of it’s sequential counterpart. Notice in this concurrent version, the download_one function is the same as before but in the download_all function, a ThreadPoolExecutor context manager wraps the executor.map() method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using 2 * multiprocessing.cpu_count() + 1. My machine has 6 physical cores with 12 threads. So 13 is the value I chose. . Note: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task. . There is one small problem with the example above. The executor.map() method returns a generator which allows to iterate through the results once ready. That means if any error occurs inside map, it’s not possible to handle that and resume the generator after the exception occurs. From PEP255: . If an unhandled exception– including, but not limited to, StopIteration –is raised by, or passes through, a generator function, then the exception is passed on to the caller in the usual way, and subsequent attempts to resume the generator function raise StopIteration. In other words, an unhandled exception terminates a generator’s useful life. . To get around that, you can use the executor.submit() method to create futures, accumulated the futures in a list, iterate through the futures and handle the exceptions manually. See the following example: . from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): &quot;&quot;&quot; Create a thread pool and download specified urls &quot;&quot;&quot; futures_list = [] results = [] with ThreadPoolExecutor(max_workers=13) as executor: for url in urls: futures = executor.submit(download_one, url) futures_list.append(futures) for future in futures_list: try: result = future.result(timeout=60) results.append(result) except Exception: results.append(None) return results if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . The above snippet should print out similar messages as before. . Running Multiple CPU Bound Subroutines with Multi-processing . The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. Let’s run the function sequentially first. . import hashlib def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; for i in range(n): hsh = hash_one(n) return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 18317.330598831177 ms . If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. The above code takes roughly 18 seconds to run in sequential mode. Now let’s run it parallelly using ProcessPoolExecutor. . import hashlib from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; with ProcessPoolExecutor(max_workers=10) as executor: for arg, res in zip(range(n), executor.map(hash_one, range(n), chunksize=2)): pass return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 1673.842430114746 ms . If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance. As you can see the concurrent version of the above CPU intensive operation is about 11 times faster than its sequential counterpart. . Avoiding Concurrency Pitfalls . Since the concurrent.futures provides such a simple API, you might be tempted to apply concurrency to every simple tasks at hand. However, that’s not a good idea. First, the simplicity has its fair share of constraints. In this way, you can apply concurrency only to the simplest of the tasks, usually mapping a function to an iterable or running a few subroutines simultaneously. If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules. . Another pitfall of using concurrency is deadlock situations that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let’s see a slightly modified example from the official docs. . import time from concurrent.futures import ThreadPoolExecutor def wait_on_b(): time.sleep(5) print(b.result()) # b will never complete because it is waiting on a. return 5 def wait_on_a(): time.sleep(5) print(a.result()) # a will never complete because it is waiting on b. return 6 with ThreadPoolExecutor(max_workers=2) as executor: # here, the future from a depends on the future from b # and vice versa # so this is never going to be completed a = executor.submit(wait_on_b) b = executor.submit(wait_on_a) print(&quot;Result from wait_on_b&quot;, a.result()) print(&quot;Result from wait_on_a&quot;, b.result()) . In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function’s result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. Let’s explain another deadlock situation from the official docs. . from concurrent.futures import ThreadPoolExecutor def wait_on_future(): f = executor.submit(pow, 5, 2) # This will never complete because there is only one worker thread and # it is executing this function. print(f.result()) with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(wait_on_future) print(future.result()) . The above situation usually happens when a subroutine produces nested Future object and runs on a single thread. In the function wait_on_future, the executor.submit(pow, 5, 2) creates another Future object. Since I’m running the entire thing using a single thread, the internal future object is blocking the thread and the external executor.submit() method inside the context manager can not use any threads. This situation can be avoided using multiple threads but in general, this is a bad design itself. . Then there’re situations when you might be getting lower performance with concurrent code than its sequential counterpart. This could happen for multiple reasons. . Threads were used to perform CPU bound tasks | Multiprocessing were used to perform I/O bound tasks | The tasks were too trivial to justify using either threads or multiple processes | Spawning and squashing multiple threads or processes bring extra overheads. Usually threads are much faster than processes to spawn and squash. However, using the wrong type of concurrency can actually slow down your code rather than making it any performant. Below is a trivial example where both ThreadPoolExecutor and ProcessPoolExecutor perform worse than their sequential counterpart. . import math PRIMES = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): for number in PRIMES: print(f&quot;{number} is prime: {is_prime(number)}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 67.65174865722656 ms . The above examples verifies whether a number in a list is prime or not. We ran the function on 1000 numbers to determine if they’re prime or not. The sequential version took roughly 67ms to do that. However, look below where the threaded version of the same code takes more than double the time (140ms) to so the same task. . from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ThreadPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 140.17250061035156 ms . The multiprocessing version of the same code is even slower. The tasks doesn’t justify opening so many processes. . from concurrent.futures import ProcessPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ProcessPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 311.3126754760742 ms . Although intuitively, it may seem like the task of checking prime numbers should be a CPU bound operation, it’s also important to determine if the task itself is computationally heavy enough to justify spawning multiple threads or processes. Otherwise you might end up with complicated code that performs worse than the simple solutions. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 20.04. . References . concurrent.futures- the official documentation | Easy Concurrency in Python | Adventures in Python with concurrent.futures |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/21/python-concurrent-futures.html",
            "relUrl": "/python/2020/04/21/python-concurrent-futures.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "No Really, Python's Pathlib is Great",
            "content": "When I first encountered Python’s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when this ticket confirmed that Django was replacing os.path with pathlib, I got curious. . The os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I’ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top. . Problem with Python’s Path Handling . Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome . However, paths are not actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src. . from glob import glob import os import shutil for fname in glob(&quot;*.py&quot;): new_path = os.path.join(&quot;src&quot;, fname) shutil.copy(fname, new_path) . The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let’s have a look at a few more examples of performing the same tasks using os.path and pathlib modules. . Joining &amp; Creating New Paths . Say you want to achieve the following goals: . There is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory . | Then you want to save the absolute path of file_another.txt in a new variable. . | . Let’s see how you’d usually do this via the os module. . from os.path import abspath, dirname, join file_path = abspath(&quot;./file.txt&quot;) base_dir = dirname(file_path) file_another_path = join(base_dir, &quot;file_another.txt&quot;) . The variables file_path, base_dir, file_another_path look like this on my machine: . print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . You can use the usual string methods to transform the paths but generally, that’s not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems do not define paths in the same way. Windows uses &quot; &quot; while Mac and *nix based OSes use &quot;/&quot; as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses &quot;/&quot; operator overloading and make this a little less painful. . from pathlib import Path file_path = Path(&quot;file.txt&quot;).resolve() base_dir = file_path.parent file_another_path = base_dir / &quot;another_file.txt&quot; print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly. . Making Directories &amp; Renaming Files . Here’s a piece of code that: . Tries to make a src/stuff/ directory when it already exists | Renames a file in the src directory called .config to .stuffconfig: | . import os import os.path os.makedirs(os.path.join(&quot;src&quot;, &quot;stuff&quot;), exist_ok=True) os.rename(&quot;src/.config&quot;, &quot;src/.stuffconfig&quot;) . Here is the same thing done using the pathlib module: . from pathlib import Path Path(&quot;src/stuff&quot;).mkdir(parents=True, exist_ok=True) Path(&quot;src/.config&quot;).rename(&quot;src/.stuffconfig&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/.stuffconfig&#39;) . Notice the output where the renamed file path is printed. It’s not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably. . Listing Specific Types of Files in a Directory . Let’s say you want to recursively visit nested directories and list .py files in a directroy called source. The directory looks like this: . src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py . Usually, glob module is used to resolve this kind of situation: . from glob import glob top_level_py_files = glob(&quot;src/*.py&quot;) all_py_files = glob(&quot;src/**/*.py&quot;, recursive=True) print(top_level_py_files) print(all_py_files) . &gt;&gt;&gt; [&#39;src/module.py&#39;] &gt;&gt;&gt; [&#39;src/module.py&#39;, &#39;src/stuff/__init__.py&#39;, &#39;src/stuff/submodule.py&#39;] . The above approach works perfectly. However, if you don’t want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way: . from pathlib import Path top_level_py_files = Path(&quot;src&quot;).glob(&quot;*.py&quot;) all_py_files = Path(&quot;src&quot;).rglob(&quot;*.py&quot;) print(list(top_level_py_files)) print(list(all_py_files)) . This will also print the same as before: . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;)] &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh? . Opening Multiple Files &amp; Reading their Contents . Now let’s open the .py files and read their contents that you recursively discovered in the previous example. . from glob import glob contents = [] for fname in glob(&quot;src/**/*.py&quot;, recursive=True): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib ...&#39;] . The pathlib implementation is almost identical as above. . from pathlib import Path contents = [] for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . You can also cook up a more robust implementation with generator comprehension and context manager. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . Anatomy of the Pathlib Module . Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are: . PurePath (Useful when you want to work with windows path on a Linux machine) | PurePosixPath (Subclass of PurePath) | PureWindowsPath (Subclass of PurePath) | Path (Concrete path object, most of the time, you’ll be dealing with this one) | PosixPath (Concrete posix path, subclass of Path) | WindowsPath (Concrete windows path, subclass of Path) | This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes. . . Unless you are doing cross platform path manipulation, most of the time you’ll be working with the concrete Path object. So I’ll focus on the methods and properties of Path class only. . Operators . Instead of using os.path.join you can use / operator to create child paths. . from pathlib import Path base_dir = Path(&quot;src&quot;) child_dir = base_dir / &quot;stuff&quot; file_path = child_dir / &quot;__init__.py&quot; print(file_path) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Attributes &amp; Methods . The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We’ll linearly traverse through the tree and provide necessary examples to grasp their usage. . Path │ ├── Attributes │ ├── parts │ ├── parent &amp; parents │ ├── name │ ├── suffix &amp; suffixes │ └── stem │ │   └── Methods    ├── joinpath(*other) ├── cwd() ├── home() ├── exists() ├── expanduser() ├── glob() ├── rglob(pattern) ├── is_dir() ├── is_file() ├── is_absolute() ├── iterdir() ├── mkdir(mode=0o777, parents=False, exist_ok=False) ├── open(mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None) ├── rename(target) ├── replace(target) ├── resolve(strict=False) └── rmdir() . Let’s dive into their usage one by one. For all the examples, We’ll be using the previously seen directory structure. . src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefile.tar.gz └── module.py . Path.parts . Returns a tuple containing individual components of a path. . from pathlib import Path file_path = Path(&quot;src/stuff/__init__.py&quot;) file_path.parts . &gt;&gt;&gt; (&#39;src&#39;, &#39;stuff&#39;, &#39;__init__.py&#39;) . Path.parents &amp; Path.parent . Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path. . file_path = Path(&quot;src/stuff/__init__.py&quot;) for parent in file_path.parents: print(parent) . &gt;&gt;&gt; src/stuff ... src ... . . file_path.parent . &gt;&gt;&gt; PosixPath(&#39;src/stuff&#39;) . Path.name . Returns the last component of a path as string. Usually used to extract file name from a path. . from pathlib import Path file_path = Path(&quot;src/module.py&quot;) file_path.name . &gt;&gt;&gt; &#39;module.py&#39; . Path.suffixes &amp; Path.suffix . Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.suffixes . &gt;&gt;&gt; [&#39;.tar&#39;, &#39;.gz&#39;] . file_path.suffix . &gt;&gt;&gt;&#39;.gz&#39; . Path.stem . Returns the final path component without the suffix. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.stem . &gt;&gt;&gt; &#39;somefile.tar&#39; . Path.is_absolute . Checks if a path is absolute or not. Return boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.is_absolute() . &gt;&gt;&gt; False . Path.joinpath(*other) . This method is used to combine multiple components into a complete path. This can be used as an alternative to &quot;/&quot; operator for joining path components. . from pathlib import Path file_path = Path(&quot;src&quot;).joinpath(&quot;stuff&quot;).joinpath(&quot;__init__.py&quot;) file_path . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Path.cwd() . Returns the current working directory. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.cwd() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo&#39;) . Path.home() . Returns home directory. . from pathlib import Path Path.home() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi&#39;) . Path.exists() . Checks if a path exists or not. Returns boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/thisisabsent.py&quot;) file_path.exists() . &gt;&gt;&gt; False . Path.expanduser() . Returns a new path with expanded ~ symbol. . from pathlib import Path file_path = Path(&quot;~/code/demo/src/stuff/somefile.tar.gz&quot;) file_path.expanduser() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/somefile.tar.gz&#39;) . Path.glob() . Globs and yields all file paths matching a specific pattern. Let’s discover all the files in src/stuff/ directory that have .py extension. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) file_paths = dir_path.glob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.rglob(pattern) . This is like Path.glob method but matches the file pattern recursively. . from pathlib import Path dir_path = Path(&quot;src&quot;) file_paths = dir_path.rglob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.is_dir() . Checks if a path points to a directory or not. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_dir() . &gt;&gt;&gt; True . Path.is_file() . Checks if a path points to a file. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_file() . &gt;&gt;&gt; False . Path.is_absolute() . Checks if a path is absolute or relative. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_absolute() . &gt;&gt;&gt; False . Path.iterdir() . When the path points to a directory, this yields the content path objects. . from pathlib import Path base_path = Path(&quot;src&quot;) contents = [content for content in base_path.iterdir()] print(contents) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff&#39;), PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/.stuffconfig&#39;)] . Path.mkdir(mode=0o777, parents=False, exist_ok=False) . Creates a new directory at this given path. . Parameters: . mode:(str) Posix permissions (mimicking the POSIX mkdir -p command) . | parents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised. . | exist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored. . | . from pathlib import Path dir_path = Path(&quot;src/other/side&quot;) dir_path.mkdir(parents=True) . Path.open(mode=’r’, buffering=-1, encoding=None, errors=None, newline=None) . This is same as the built in open function. . from pathlib import Path with Path(&quot;src/module.py&quot;) as f: contents = open(f, &quot;r&quot;) for line in contents: print(line) . &gt;&gt;&gt; from contextlib import contextmanager ... from time import time ... ..... . Path.rename(target) . Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found. . from pathlib import Path file_path = Path(&quot;src/stuff/submodule.py&quot;) file_path.rename(file_path.parent / &quot;anothermodule.py&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/anothermodule.py&#39;) . Path.replace(target) . Replaces a file or directory to the given target. Returns the new path instance. . from pathlib import Path file_path = Path(&quot;src/stuff/anothermodule.py&quot;) file_path.replace(file_path.parent / &quot;Dockerfile&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/Dockerfile&#39;) . Path.resolve(strict=False) . Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn’t exist, FileNotFoundError will be raised. . from pathlib import Path file_path = Path(&quot;src/./stuff/Dockerfile&quot;) file_path.resolve() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/Dockerfile&#39;) . Path.rmdir() . Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised. . from pathlib import Path file_path = Path(&quot;src/stuff&quot;) file_path.rmdir() . So, Should You Use It? . Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems. . The ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 20.04. . References . pathlib — Object-oriented filesystem paths | Python 3’s pathlib Module: Taming the File System | Why you should be using pathlib |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/13/python-pathlib.html",
            "relUrl": "/python/2020/04/13/python-pathlib.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Running Python Linters with Pre-commit Hooks",
            "content": "Pre-commit hooks can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let’s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase. . To keep my sanity, I only use three linters in all of my python projects: . Isort: Isort is a Python utility to sort imports alphabetically, and automatically separate them by sections and type. . It parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import: . Future | Python Standard Library | Third Party | Current Python Project | Explicitly Local (. before import, as in: from . import x) | Custom Separate Sections (Defined by forced_separate list in the configuration file) | Custom Sections (Defined by sections list in configuration file) | . Inside each section, the imports are sorted alphabetically. This also automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79). . | Black: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you’re reading. . | Flake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder’s McCabe script. The combination of these three linters makes sure that your code is compliant with PEP 8 and free of some obvious code smells. . | . Installing Pre-commit . Install using pip: . pip install pre-commit . | Install via curl: . curl https://pre-commit.com/install-local.py | python - . | . Defining the Pre-commit Config File . Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m &quot;Commit message&quot;. The following example shows how black and a few other linters can be added as hooks to the config: . # .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black . Installing the Git Hook scripts . Run . pre-commit install . This will set up the git hook scripts and should show the following output in your terminal: . pre-commit installed at .git/hooks/pre-commit . Now you’ll be able to implicitly or explicitly run the hooks before each commit. . Running the Hooks Against All the Files . By default, the hooks will run every time you say: . git commit -m &quot;Commit message&quot; . However, if you wish to run the hooks manually on every file, you can do so via: . pre-commit run --all-files . Running the Linters as Pre-commit Hooks . To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there’re a few minor issues that need to be taken care of. . The default line length of black formatter is 88 (you should embrace that) but flake8 caps the line at 79 characters. This raises conflict and can cause failures. . | Flake8 can be overly strict at times. You’ll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors. . | . The following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts. . # .pre-commit-config.yaml # isort - repo: https://github.com/asottile/seed-isort-config rev: v1.9.3 hooks: - id: seed-isort-config - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include=&#39; .pyi?$&#39; # these folders wont be formatted by black - --exclude=&quot;&quot;&quot; .git | .__pycache__| .hg| .mypy_cache| .tox| .venv| _build| buck-out| build| dist&quot;&quot;&quot; language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - &quot;--max-line-length=88&quot; - &quot;--max-complexity=18&quot; - &quot;--select=B,C,E,F,W,T4,B9&quot; # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - &quot;--ignore=E203,E266,E501,W503,F403,F401,E402&quot; . You can add the above lines to your configuration and run . pre-commit run --all-files . This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters. .",
            "url": "https://rednafi.github.io/digressions/python/2020/04/06/python-precommit.html",
            "relUrl": "/python/2020/04/06/python-precommit.html",
            "date": " • Apr 6, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Generic Functions with Python's Singledispatch",
            "content": "Recently, I was refactoring a portion of a Python function that somewhat looked like this: . def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 &amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 &amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 &amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... . This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism comes into play. The idea is, you have to define a single function that will be dynamically overloaded with alternative implementations based on the type of the function arguments. . Function Overloading &amp; Generic Functions . Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. When function overloading happens based on its argument types, the resulting function is known as generic function. Let’s see how Python’s singledispatch decorator can help to design generic functions and refactor the icky code above. . Singledispatch . Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. . A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. . As PEP-443 said, singledispatch only happens based on the first argument’s type. Let’s take a look at an example to see how this works! . Example-1: Singledispatch with built-in argument type . Let’s consider the following code: . # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; def process_float(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this code will return . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let’s see how the same thing can be achieved with singledispatch. . # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(&quot;Implement process function.&quot;) @process.register(int) def sub_process(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; @process.register(float) def sub_process(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this will return the same result as before. . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . Example-2: Singledispatch with custom argument type . Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: . def process(data: dict): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return process_cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return process_dog(data) def process_cat(data: dict): # processing cat return &quot;Cat data has been processed successfully!&quot; def process_dog(data: dict): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} # using process print(process(cat_data)) print(process(dog_data)) . Running this snippet will print out: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . To refactor this with singledispatch, you can create two data types Cat and Dog.When you make Cat and Dog objects from the classes and pass them through the process function, singledispatch will take care of dispatching the appropriate implementation of that function. . from functools import singledispatch from dataclasses import dataclass @dataclass class Cat: genus: str species: str @dataclass class Dog: genus: str species: str @singledispatch def process(obj=None): raise NotImplementedError(&quot;Implement process for bucket&quot;) @process.register(Cat) def sub_process(obj): # processing cat return &quot;Cat data has been processed successfully!&quot; @process.register(Dog) def sub_process(obj): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_obj = Cat(genus=&quot;Felis&quot;, species=&quot;catus&quot;) dog_obj = Dog(genus=&quot;Canis&quot;, species=&quot;familiaris&quot;) print(process(cat_obj)) print(process(dog_obj)) . Running this will print out the same output as before: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . References . Transform a function into a single dispatch generic function | Function overloading | Parametric polymorphism |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/05/python-singledispatch.html",
            "relUrl": "/python/2020/04/05/python-singledispatch.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The Curious Case of Python's Context Manager",
            "content": "Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: . with open(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: . try: f = open(&quot;file.txt&quot;, &quot;wt&quot;) text = f.write(&quot;contents go here&quot;) finally: f.close() . Writing Custom Context Managers . To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above. . class CustomFileOpen: &quot;&quot;&quot;Custom context manager for opening files.&quot;&quot;&quot; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() . You can use the above class just like a regular context manager. . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . From Generators to Context Managers . Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: . @contextmanager def some_generator(&lt;arguments&gt;): &lt;setup&gt; try: yield &lt;value&gt; finally: &lt;cleanup&gt; . When you use the context manager with the with statement: . with some_generator(&lt;arguments&gt;) as &lt;variable&gt;: &lt;body&gt; . It roughly translates to: . &lt;setup&gt; try: &lt;variable&gt; = &lt;value&gt; &lt;body&gt; finally: &lt;cleanup&gt; . The setup code goes before the try..finally block. Notice the point where the generator yields. This is where the code block nested in the with statement gets executed. After the completion of the code block, the generator is then resumed. If an unhandled exception occurs in the block, it’s re-raised inside the generator at the point where the yield occurred and then the finally block is executed. If no unhandled exception occurs, the code gracefully proceeds to the finally block where you run your cleanup code. . Let’s implement the same CustomFileOpen context manager with contextmanager decorator. . from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): &quot;&quot;&quot;Custom context manager for opening a file.&quot;&quot;&quot; f = open(filename, method) try: yield f finally: f.close() . Now use it just like before: . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . Writing Context Managers as Decorators . You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let’s make a RunTime decorator that will be applied on a file-opening function. The decorator will: . Print a user provided description of the function | Print the time it takes to run the function | . from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): &quot;&quot;&quot;Timing decorator.&quot;&quot;&quot; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . You can use the decorator like this: . @RunTime(&quot;This function opens a file&quot;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) . Using the function like this should return: . print(custom_file_write(&quot;file.txt&quot;, &quot;wt&quot;, &quot;jello&quot;)) . This function opens a file The function took 0.0005390644073486328 seconds to run. None . You can also create the same decorator via contextlib.contextmanager decorator. . from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . Nesting Contexts . You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: . from contextlib import contextmanager @contextmanager def get_state(name): print(&quot;entering:&quot;, name) yield name print(&quot;exiting :&quot;, name) # multiple get_state can be nested like this with get_state(&quot;A&quot;) as A, get_state(&quot;B&quot;) as B, get_state(&quot;C&quot;) as C: print(&quot;inside with statement:&quot;, A, B, C) . entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A . Notice the order they’re closed. Context managers are treated as a stack, and should be exited in reverse order in which they’re entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. . Combining Multiple Context Managers . You can combine multiple context managers too. Let’s consider these two managers. . from contextlib import contextmanager @contextmanager def a(name): print(&quot;entering a:&quot;, name) yield name print(&quot;exiting a:&quot;, name) @contextmanager def b(name): print(&quot;entering b:&quot;, name) yield name print(&quot;exiting b:&quot;, name) . Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. . @contextmanager def ab(a, b): with a(&quot;A&quot;) as A, b(&quot;B&quot;) as B: yield (A, B) . This can be used as: . with ab(a, b) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: (&#39;A&#39;, &#39;B&#39;) exiting b: B exiting a: A . If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let’s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. . from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] . with ab((a, b), (&quot;A&quot;, &quot;B&quot;)) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: [&#39;A&#39;, &#39;B&#39;] exiting b: B exiting a: A . ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let’s see, how you can do so while avoiding accidental memory leakage with robust resource management. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] . Using Context Managers to Create SQLAlchemy Session . If you are familiar with SQLALchemy, Python’s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: . from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(&quot;sqlite://&quot;) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() . The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of committing and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: . with session_scope() as session: myobject = MyObject(&quot;foo&quot;, &quot;bar&quot;) session.add(myobject) . Abstract Away Exception Handling Monstrosity with Context Managers . This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let’s write a decorator that will handle ZeroDivisionError and TypeError simultaneously. . from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(&quot;This is a custom ZeroDivisionError message.&quot;) raise except TypeError: print(&quot;This is a custom TypeError message.&quot;) raise . Now use this in a function where these exceptions occur. . @errhandler() def div(a, b): return a // b . div(&quot;b&quot;, 0) . This is a custom TypeError message. TypeError Traceback (most recent call last) &lt;ipython-input-43-65497ed57253&gt; in &lt;module&gt; -&gt; 1 div(&#39;b&#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): &gt; 75 return func(*args, **kwds) 76 return inner 77 &lt;ipython-input-42-b7041bcaa9e6&gt; in div(a, b) 1 @errhandler() 2 def div(a, b): -&gt; 3 return a // b TypeError: unsupported operand type(s) for //: &#39;str&#39; and &#39;int&#39; . You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? . The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logics from the main method. . import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=&quot; n(asctime)s [%(levelname)s] %(message)s&quot;, handlers=[logging.FileHandler(&quot;./debug.log&quot;), logging.StreamHandler()], ) class Calculation: &quot;&quot;&quot;Dummy class for demonstrating exception decoupling with contextmanager.&quot;&quot;&quot; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f&quot;Custom handling of Zero Division Error! Printing &quot; &quot;only 2 levels of traceback..&quot; ) logging.exception(&quot;ZeroDivisionError&quot;) def main_func(self): &quot;&quot;&quot;Function that we want to save from nasty error handling logic.&quot;&quot;&quot; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) . This will return . (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 25, in errorhandler yield File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None . Persistent Parameters Across Http Requests with Context Managers . Another great use case for context managers is making parameters persistent across multiple http requests. Python’s requests library has a Session object that will let you easily achieve this. So, if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. The following example is taken directly from requests’ official docs. Let’s persist some cookies across requests. . with requests.Session() as session: session.get(&quot;http://httpbin.org/cookies/set/sessioncookie/123456789&quot;) response = session.get(&quot;http://httpbin.org/cookies&quot;) print(response.text) . This should show: . { &quot;cookies&quot;: { &quot;sessioncookie&quot;: &quot;123456789&quot; } } . Remarks . All the code snippets are updated for python 3.8. To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. . Resources . Python Contextlib Documentation | Python with Context Manager - Jeff Knupp | SQLALchemy Session Creation | Scipy Lectures: Context Managers | Merging Context Managers |",
            "url": "https://rednafi.github.io/digressions/python/2020/03/26/python-contextmanager.html",
            "relUrl": "/python/2020/03/26/python-contextmanager.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Up and Running with MySQL in Docker",
            "content": ". Setting Up . Installation . This part describes the basic installation steps of setting up MySQL 5.7 server on Ubuntu Linux using docker. . Install docker on your Linux machine. See the instruction here. . | Install docker compose via following the instructions here. . | Create another folder on your project folder and make a docker-compose.yml file. Run the following instructions one by one: . mkdir mysql_dump cd mysql_dump touch docker-compose.yml . | Open the docker-compose.yml file and copy the following lines into it. . # docker-compose version version: &quot;3.3&quot; services: # images mysql-dev: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test_db ports: - &quot;3306:3306&quot; # making data persistent volumes: - db-data:/var/lib/mysql volumes: db-data: . | . Run MySQL Server . Run the docker-compose command. This will build and run the server in detached mode. . docker compose up -d . Connect Shell to Server . Check the name of the running container with docker ps command. In this case, the running container is called mysql_dumps_mysql-dev_1. Then run the following command to connect your shell to the running server. . # connect shell to server docker exec -it mysql_dumps_mysql-dev_1 mysql -uroot -p . Alter Root Password . If you want to change the root password, enter the following command in the MySQL shell. Replace MyNewPass with your new root password: . ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;; . You should see something like this in the command prompt: . Query OK, 0 rows affected (0.02 sec) . To make the change take effect, type the following command: . FLUSH PRIVILEGES; . View Users . MySQL stores the user information in its own database. The name of the database is mysql. If you want to see what users are set up in the MySQL user table, run the following command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like this: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | ++--+-+ 4 rows in set (0.00 sec) . Create a Database . According to the docker-compose.yml file, you already have created a database named test_db. You can create anotehr database named test_db_2 via the following command: . CREATE DATABASE test_db_2; . List your databases via the following command: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | mysql | | performance_schema | | sys | | test_db | | test_db_2 | +--+ 6 rows in set (0.01 sec) . To ensure the changes: . FLUSH PRIVILEGES; . Creating Dummy Table in the Database . -- create dummy table CREATE TABLE IF NOT EXISTS `student` ( `id` int(2) NOT NULL DEFAULT &#39;0&#39;, `name` varchar(50) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `class` varchar(10) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `mark` int(3) NOT NULL DEFAULT &#39;0&#39;, `sex` varchar(6) CHARACTER SET utf8 NOT NULL DEFAULT &#39;male&#39; ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- insert data into the dummy table INSERT INTO `student` (`id`, `name`, `class`, `mark`, `sex`) VALUES (1, &#39;John Deo&#39;, &#39;Four&#39;, 75, &#39;female&#39;), (2, &#39;Max Ruin&#39;, &#39;Three&#39;, 85, &#39;male&#39;), (3, &#39;Arnold&#39;, &#39;Three&#39;, 55, &#39;male&#39;), (4, &#39;Krish Star&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (5, &#39;John Mike&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (6, &#39;Alex John&#39;, &#39;Four&#39;, 55, &#39;male&#39;), (7, &#39;My John Rob&#39;, &#39;Fifth&#39;, 78, &#39;male&#39;), (8, &#39;Asruid&#39;, &#39;Five&#39;, 85, &#39;male&#39;), (9, &#39;Tes Qry&#39;, &#39;Six&#39;, 78, &#39;male&#39;), (10, &#39;Big John&#39;, &#39;Four&#39;, 55, &#39;female&#39;), (11, &#39;Ronald&#39;, &#39;Six&#39;, 89, &#39;female&#39;), (12, &#39;Recky&#39;, &#39;Six&#39;, 94, &#39;female&#39;), (13, &#39;Kty&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (14, &#39;Bigy&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (15, &#39;Tade Row&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (16, &#39;Gimmy&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (17, &#39;Tumyu&#39;, &#39;Six&#39;, 54, &#39;male&#39;), (18, &#39;Honny&#39;, &#39;Five&#39;, 75, &#39;male&#39;), (19, &#39;Tinny&#39;, &#39;Nine&#39;, 18, &#39;male&#39;), (20, &#39;Jackly&#39;, &#39;Nine&#39;, 65, &#39;female&#39;), (21, &#39;Babby John&#39;, &#39;Four&#39;, 69, &#39;female&#39;), (22, &#39;Reggid&#39;, &#39;Seven&#39;, 55, &#39;female&#39;), (23, &#39;Herod&#39;, &#39;Eight&#39;, 79, &#39;male&#39;), (24, &#39;Tiddy Now&#39;, &#39;Seven&#39;, 78, &#39;male&#39;), (25, &#39;Giff Tow&#39;, &#39;Seven&#39;, 88, &#39;male&#39;), (26, &#39;Crelea&#39;, &#39;Seven&#39;, 79, &#39;male&#39;), (27, &#39;Big Nose&#39;, &#39;Three&#39;, 81, &#39;female&#39;), (28, &#39;Rojj Base&#39;, &#39;Seven&#39;, 86, &#39;female&#39;), (29, &#39;Tess Played&#39;, &#39;Seven&#39;, 55, &#39;male&#39;), (30, &#39;Reppy Red&#39;, &#39;Six&#39;, 79, &#39;female&#39;), (31, &#39;Marry Toeey&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (32, &#39;Binn Rott&#39;, &#39;Seven&#39;, 90, &#39;female&#39;), (33, &#39;Kenn Rein&#39;, &#39;Six&#39;, 96, &#39;female&#39;), (34, &#39;Gain Toe&#39;, &#39;Seven&#39;, 69, &#39;male&#39;), (35, &#39;Rows Noump&#39;, &#39;Six&#39;, 88, &#39;female&#39;); . Show Tables . USE test_db; SHOW tables; . Delete a Database . To delete a database test_db run the following command: . DROP DATABASE test_db, FLUSH PRIVILEGES; . Add a Database User . To create a new user (here, we created a new user named redowan with the password password), run the following command in the MySQL shell: . CREATE USER &#39;redowan&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; FlUSH PRIVILEGES; . Ensure that the changes has been saved via running FLUSH PRIVILEGES;. Verify that a user has been successfully created via running the previous command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like below. Notice that a new user named redowan has been created: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | | redowan | localhost | *0756A562377EDF6ED3AC45A00B356AAE6D3C6BB6 | ++--+-+ . Delete a Database User . To delete a database user (here, I’m deleting the user-redowan) run: . DELETE FROM mysql.user WHERE user=&#39;&lt;redowan&gt;&#39; AND host = &#39;localhost&#39; FlUSH PRIVILEGES; . Grant Database User Permissions . Give the user full permissions for your new database by running the following command (Here, I provided full permission of test_db to the user redowan: . GRANT ALL PRIVILEGES ON test_db.table TO &#39;redowan&#39;@&#39;localhost&#39;; . If you want to give permission to all the databases, type: . GRANT ALL PRIVILEGES ON *.* TO &#39;redowan&#39;@&#39;localhost&#39;; FlUSH PRIVILEGES; . Loading Sample Database to Your Own MySQL Server . To load mysqlsampledatabase.sql to your own server (In this case the user is redowan. Provide database password in the prompt), first fireup the server and type the following commands: . mysql -u redowan -p test_db &lt; mysqlsampledatabase.sql; . Now run: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | classicmodels | | mysql | | performance_schema | | sys | | test_db | +--+ 6 rows in set (0.00 sec) . Stop the Server . The following command stops the server. . docker-compose down . Notice that a new database named classicmodels has been added to the list. . Connecting to a Third Party Client . We will be using DBeaver as a third party client. While you can use the mysql shell to work on your data, a third partly client that make the experience much better with auto formatting, earsier import features, syntax highlighting etc. . Installing DBeaver . You can install DBeaver installer from here. Installation is pretty straight forward. . Connecting MySQL Database to DBeaver . Fire up DBeaver and you should be presented with this screen. Select MySQL 8+ and go next. . . The dialogue box will ask for credentials to connect to a database. In this case, I will log into previously created local database test_db with the username redowan, corresponding password password and press test connection tab. A dialogue box might pop up, prompting you download necessary drivers. . . If everything is okay, you should see a success message. You can select the SQL Editor and start writing your MySQL scripts right away. . Connecting to MySQL Server via Python . PyMySQL and DBUtils can be used to connect to MySQL Server. . import pymysql import os from dotenv import load_dotenv from DBUtils.PooledDB import PooledDB load_dotenv(verbose=True) MYSQL_REPLICA_CONFIG = { &quot;host&quot;: os.environ.get(&quot;SQL_HOST&quot;), &quot;port&quot;: int(os.environ.get(&quot;SQL_PORT&quot;)), &quot;db&quot;: os.environ.get(&quot;SQL_DB&quot;), &quot;password&quot;: os.environ.get(&quot;SQL_PASSWORD&quot;), &quot;user&quot;: os.environ.get(&quot;SQL_USER&quot;), &quot;charset&quot;: os.environ.get(&quot;SQL_CHARSET&quot;), &quot;cursorclass&quot;: pymysql.cursors.DictCursor, } # class to create database connection pooling POOL = PooledDB(**configs.MYSQL_POOL_CONFIG, **configs.MYSQL_REPLICA_CONFIG) class SqlPooled: &quot;&quot;&quot;Sql connection with pooling.&quot;&quot;&quot; def __init__(self): self._connection = POOL.connection() self._cursor = self._connection.cursor() def fetch_one(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchone() return result def fetch_all(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchall() return result def __del__(self): self._connection.close() .",
            "url": "https://rednafi.github.io/digressions/database/2020/03/15/mysql-install.html",
            "relUrl": "/database/2020/03/15/mysql-install.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Reduce Boilerplate Code with Python's Dataclasses",
            "content": "Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: . class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; print(CartesianPoint(1, 2, 3)) . &gt;&gt;&gt; CartesianPoint(x = 1, y = 2, z = 3) . This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. . Dataclasses . Let’s see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) . &gt;&gt;&gt; CartesianPoint(x=1, y=2, z=3) . In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotation and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via python’s help. . help(CartesianPoint) . Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -&gt; None | | __repr__(self) | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes defined here: | | __annotations__ = {&#39;x&#39;: &lt;class &#39;float&#39;&gt;, &#39;y&#39;: &lt;class &#39;float&#39;&gt;, &#39;z&#39;: &lt;c... | | __dataclass_fields__ = {&#39;x&#39;: Field(name=&#39;x&#39;,type=&lt;class &#39;float&#39;&gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None . Using Default Values . You can provide default values to the fields in the following way: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 . Using Arbitrary Field Type . If you don’t want to specify your field type during type hinting, you can use Any type from python’s typing module. . from dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any . Instance Ordering . You can check if two instances are equal without making any modification to the class. . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) . &gt;&gt;&gt; False . However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually. . from dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 &gt; point_2) . &gt;&gt;&gt; True . By default, while comparing instances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function. . Suppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way: . from dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create intance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 &lt; point_2) . &gt;&gt;&gt; True &gt;&gt;&gt; False . You can see the above code prints out True despite the instances have different y and z attributes. . Adding Methods . Methods can be added to dataclasses just like normal classes. Let’s add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin. . from dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) # create a new instance and use method `abs_val` point = CartesianPoint(5, 6, 7) norm = point.abs_val() print(norm) . &gt;&gt;&gt; 10.488088481701515 . Making Instances Immutable . By default, instances of dataclasses are immutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass. . from dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float . If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError. . point = CartesianPoint(2, 4, 6) point.x = 23 . FrozenInstanceError Traceback (most recent call last) &lt;ipython-input-34-b712968bd0eb&gt; in &lt;module&gt; 1 point = CartesianPoint(2, 4, 6) -&gt; 2 point.x = 23 &lt;string&gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;x&#39; . Making Instances Hashable . You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perform set operation on them. However, if you are using unsafe_hash make sure that your dataclasses do not contain any mutable data structure in it. . from dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f&quot;{point}&quot;: &quot;origin&quot;}) . &gt;&gt;&gt; {&#39;CartesianPoint(x=0, y=0, z=0)&#39;: &#39;origin&#39;} . Converting Instances to Dicts . The asdict() function converts a dataclass instance to a dict of its fields. . from dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) . &gt;&gt;&gt; {&#39;x&#39;: 1, &#39;y&#39;: 5, &#39;z&#39;: 6} . Post-init Processing . When dataclass generates the __init__ method, internally it’ll call _post_init__ method. You can add additional processing in the __post_init__ method. Here, I have added another attribute tup that returns the cartesian point as a tuple. . from dataclasses import dataclass @dataclass class CartesianPoint: x : float y : float z : float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) . &gt;&gt;&gt; (4, 5, 6) . Refactoring the Entire Cartesian Point Class . The feature rich original CartesianPoint looks something like this: . import math class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): &quot;&quot;&quot;Print the instance neatly.&quot;&quot;&quot; return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; def __eq__(self, other): &quot;Checks if equal.&quot; return self.x == other.x def __nq__(self, other): &quot;&quot;&quot;Checks non equality.&quot;&quot;&quot; return self.x != other.x def __gt__(self, other): &quot;&quot;&quot;Checks if greater than.&quot;&quot;&quot; return self.x &gt; other.x def __ge__(self, other): &quot;&quot;&quot;Checks if greater than or equal.&quot;&quot;&quot; return self.x &gt;= other.x def __lt__(self, other): &quot;&quot;&quot;Checks if less than.&quot;&quot;&quot; return self.x &lt; other.x def __le__(self, other): &quot;&quot;&quot;Checks if less than or equal.&quot;&quot;&quot; return self.x &lt;= other.x def __hash__(self): &quot;&quot;&quot;Make the instances hashable.&quot;&quot;&quot; return hash(self) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Let’s see the class in action: . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x = 1, y = 2, z = 3) True True False True {&#39;CartesianPoint(x = 1, y = 2, z = 3)&#39;: 3.7416573867739413} . Below is the same class refactored using dataclass. . from dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Use this class like before. . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x=1, y=2, z=3) True True False True {&#39;CartesianPoint(x=1, y=2, z=3)&#39;: 3.7416573867739413} . References . Python Dataclasses: Official Doc | The Ultimate Guide to Data Classes in Python 3.7 | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/12/python-dataclasses.html",
            "relUrl": "/python/2020/03/12/python-dataclasses.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Python Virtual Environment Workflow for Sanity",
            "content": "There are multiple ways of installing Python, creating and switching between different virtual environments. Also, Python’s package manager hyperspace is a mess. So, things can quickly get out of hands while dealing with projects that require quick environment switching across multiple versions of Python. I use Debian linux in my primary development environment and this is how I keep the option explosion in check: . Installing Python . Run the following commands one by one: . # update the packages list and install the prerequisites sudo apt update sudo apt install software-properties-common # add deadsnakes ppa to your sources&#39; list (When prompted press Enter to continue) sudo add-apt-repository ppa:deadsnakes/ppa # install python3.7 sudo apt install python3.8 # verify python installation python3.8 --version . Creating Virtual Environment . There are multiple ways creating and switching between different environments can be done. I use venv for creating virtual environments. For demonstration, here I’m creating a virtual environment that uses python3.8. . Install python3-venv for creating virtual environment sudo apt install python3.8-venv . | Create virtual environment named venv in the project folder . python3.8 -m venv venv . | Activate venv . source venv/bin/activate . | Deactivate venv deactivate . | . Switching Between Different Environments . To create another environment with a different python version, you have to: . Install the desired version of python following the procedures stated above. | Install python3.7-venv specific for your python version, like if you are using python3.7, you should run: . sudo apt install python3.7-venv . | Create multiple environments with multiple versions and name them distinctively. i.e. venv3.7, venv3.8 etc. Follow the instructions above. | Activate and deactivate the desired virtual environment. | . Package Management . For local development, I use pip. | For production application and libraries poetry is preferred. | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/11/python-venv-workflow.html",
            "relUrl": "/python/2020/03/11/python-venv-workflow.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "A Minimalistic Approach to ZSH",
            "content": ". Although I’m on Debian Linux, Apple’s recent announcement about replacing Bash with Zsh on MacOS made me take a look at Z-shell aka zsh. It’s a POSIX compliant Bash alternative that has been around for quite a long time. While Bash shell’s efficiency and ubiquity make it hard to think about changing the default shell of your primary development machine, I find its features as an interactive shell to be somewhat limited. So I did some digging around and soon found out that zsh’s lackluster default configurations and bloated ecosystem make it difficult for someone who just want to switch without any extra overhead. So, let’s make the process quicker. Here is what we are aiming for: . A working shell that can (almost always) take bash commands without complaining (looking at you fish) | History based autocomplete | Syntax highlighting | Git branch annotations | . Instructions were applied and tested on debian based linux (Ubuntu) . Install Z Shell . GNU/Linux . To install on a debian based linux, type: . $ apt install zsh . MacOS . Use homebrew to install zsh on MacOs. Run: . $ brew install zsh . Make Zsh as Your Default Shell . Run: . $ chsh -s $(which zsh) . Install Oh-My-Zsh Framework . Oh-My-Zsh is the tool that makes zsh so much fun and overly configurable at the same time. So we’ll tread here carefully. To install oh-my-zsh , type: . $ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; . Set Firacode As the Default Terminal Font . Your selected theme may not display all the glyphs if the default terminal font doesn’t support them. Installing a font with glyphs and ligature support can solve this. I recommend installing firacode and setting that as your default terminal font. Install Fira Code From here. . Set Syntax Highlighting . Using zsh-syntax-highlighting to achieve this. . Clone this repository in oh-my-zsh’s plugins directory . $ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . | Activate the plugin in ~/.zshrc . plugins=( [plugins...] zsh-syntax-highlighting) . | Source ~/.zshrc . | . Set Suggestions . Using zsh-autosuggestions to achieve this. . Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) . $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . | Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc ) . plugins=(zsh-autosuggestions) . | Source ~/.zshrc . | . Start a new terminal session to see the effects!!! You might need to log out and log in again for the changes to be effective. . Load .profile from .zprofile . Add the following lines to ~/.zprofile and source via the command: source ~/.zprofile. Make sure you are on zsh before running the source command. . [[ -e ~/.profile ]] &amp;&amp; emulate sh -c &#39;source ~/.profile&#39; . A Barebone ~/.zshrc . Instead of adding the plugins individually, you can just install the plugins and then add this barebone config to your ~/.zshrc . Don’t forget to replace YourUserName with your username. Source your zshrc once you are done. . # ===================== # MINIMALIST ZSHRC # AUTHOR: REDNAFI # ===================== # omz path export ZSH=&quot;$HOME/.oh-my-zsh&quot; # theme settings ZSH_THEME=&quot;juanghurtado&quot; # pluging settings plugins=(git zsh-syntax-highlighting zsh-autosuggestions) # autosuggestion highlight ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=4&quot; # source omz source $ZSH/oh-my-zsh.sh #History setup HISTFILE=$HOME/.zsh_history HISTSIZE=100000 SAVEHIST=$HISTSIZ zstyle &#39;:completion:*&#39; menu select zstyle &#39;:completion:*&#39; group-name &#39;&#39; # group results by category zstyle &#39;:completion:::::&#39; completer _expand _complete _ignored _approximate #enable approximate matches for completion #disable auto correct unsetopt correct_all . Set Terminal Color (Optional) . Optionally you customize your terminal color and in this case I’ve used Gogh to achieve this. . Pre Install | . $ sudo apt-get install dconf-cli uuid-runtime . Install on Linux | . $ bash -c &quot;$(wget -qO- https://git.io/vQgMr)&quot; . Install on MacOS $ bash -c &quot;$(curl -sLo- https://git.io/vQgMr)&quot; . | Put the code associated with your desired color scheme. | . Updating OMZ . $ upgrade_oh_my_zsh . Uninstall Zsh . $ sudo apt-get --purge remove zsh . Uninstall OMZ . $ uninstall_oh_my_zsh . Switch Back to Bash . $ chsh -s $(which bash) . Reference . Oh-My-Zsh | FiraCode | Gogh |",
            "url": "https://rednafi.github.io/digressions/linux/2019/10/29/minimal-zsh.html",
            "relUrl": "/linux/2019/10/29/minimal-zsh.html",
            "date": " • Oct 29, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Essential Bash Scripting",
            "content": ". Shell . Several layers of events take place whenever a Linux command is entered into the terminal. The top layer of that is known as shell. . A shell is any user interface to the UNIX operating system, i.e., any program that takes input from the user, translates it into instructions that the operating system can understand, and conveys the operating system’s output back to the user. . Let’s look at an example: . sort -n src/files/numbers.txt &gt; src/files/sorted_numbers.txt . This command will perform the following tasks: . Go to the src/files directory | Sort the numbers in the numbers.txt files in ascending order | Save the result in a new file called sorted_numbers.txt in the same directory | . History . The first major shell was the Bourne shell (named after its inventor, Steven Bourne); it was included in the first popular version of UNIX, Version 7, starting in 1979. The Bourne shell is known on the system as sh. Although UNIX has gone through many, many changes, the Bourne shell is still popular and essentially unchanged. Several UNIX utilities and administration features depend on it. . Variants of some popular shells: . C Shell or csh (The syntax has resemblance with C programming language) | Korn Shell or ksh (Similar to Bourne Shell with features from both Bourne and C Shell) | The Bourne Again Shell or BASH (Started with the GNU project in 1988.) | . BASH is going to be our primary focus here. . A Few Basic Commands . List of most frequently used commands. All of these commands can be run directly from a bash command prompt: . cd | ls | cat | cp | mv | mkdir | rm | grep | lp | . All of the following command summaries can be found via: . curl cheat.sh/&lt;prompt&gt; . cd . cd is used to change directory . #Go to the given directory cd path/to/directory #Go to home directory of current user cd #Go up to the parent of the current directory cd .. #Go to the previously chosen directory cd - . ls . ls lists all the files and folders in a user-specified directory . # Displays everything in the target directory ls path/to/the/target/directory # Displays everything including hidden files ls -a # Displays all files, along with the size (with unit suffixes) and timestamp ls -lh # Display files, sorted by size ls -S # Display directories only ls -d */ # Display directories only, include hidden ls -d .*/ */ . cat . cat shows the contents of a user-specified file . # Display the contents of a file cat /path/to/foo # Display contents with line numbers cat -n /path/to/foo # Display contents with line numbers (blank lines excluded) cat -b /path/to/foo . cp . cp copies files or folders from one directory to another . # Create a copy of a file cp ~/Desktop/foo.txt ~/Downloads/foo.txt # Create a copy of a directory cp -r ~/Desktop/cruise_pics/ ~/Pictures/ # Create a copy but ask to overwrite if the destination file already exists cp -i ~/Desktop/foo.txt ~/Documents/foo.txt # Create a backup file with date cp foo.txt{,.&quot;$(date +%Y%m%d-%H%M%S)&quot;} . mv . mv moves files or folders from one directory to another and can also be used to rename files or folders . # Move a file from one place to another mv ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another and automatically overwrite if the destination file exists # (This will override any previous -i or -n args) mv -f ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but ask before overwriting an existing file # (This will override any previous -f or -n args) mv -i ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but never overwrite anything # (This will override any previous -f or -i args) mv -n ~/Desktop/foo.txt ~/Documents/foo.txt # Move listed files to a directory mv -t ~/Desktop/ file1 file2 file3 . mkdir . mkdir is used to create a folder in a directory . # Create a directory and all its parents mkdir -p foo/bar/baz # Create foo/bar and foo/baz directories mkdir -p foo/{bar,baz} # Create the foo/bar, foo/baz, foo/baz/zip and foo/baz/zap directories mkdir -p foo/{bar,baz/{zip,zap}} . rm . rm is mainly used to delete files or folders . # Remove files and subdirs rm -rf path/to/the/target/ # Ignore non existent files rm -f path/to/the/target # Remove a file with his inode find /tmp/ -inum 6666 -exec rm -i &#39;{}&#39; ; . grep . grep can be used to search through the output of another command . # Search a file for a pattern grep pattern file # Case insensitive search (with line numbers) grep -in pattern file # Recursively grep for string &lt;pattern&gt; in folder: grep -R pattern folder # Read search patterns from a file (one per line) grep -f pattern_file file # Find lines NOT containing pattern grep -v pattern file # You can grep with regular expressions grep &quot;^00&quot; file #Match lines starting with 00 grep -E &quot;[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3}&quot; file #Find IP add # Find all files which match {pattern} in {directory} # This will show: &quot;file:line my research&quot; grep -rnw &#39;directory&#39; -e &quot;pattern&quot; # Exclude grep from your grepped output of ps. # Add [] to the first letter. Ex: sshd -&gt; [s]shd ps aux | grep &#39;[h]ttpd&#39; # Colour in red {bash} and keep all other lines ps aux | grep -E --color &#39;bash|$&#39; . lp . lp prints the specified output via an available printer . # lp # Print files. # Print the output of a command to the default printer (see `lpstat` command): echo &quot;test&quot; | lp # Print a file to the default printer: lp path/to/filename # Print a file to a named printer (see `lpstat` command): lp -d printer_name path/to/filename # Print N copies of a file to the default printer (replace N with the desired number of copies): lp -n N path/to/filename # Print only certain pages to the default printer (print pages 1, 3-5, and 16): lp -P 1,3-5,16 path/to/filename # Resume printing a job: lp -i job_id -H resume . clear . clear is used to clear the CLI window . # clear # Clears the screen of the terminal. # Clear the screen (equivalent to typing Control-L when using the bash shell): clear . exit . exit closes the CLI window . # exit # Quit the current CMD instance or the current batch file. # More information: #&lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/exit&gt;. # Quit the current CMD instance: exit # Quit the current batch script: exit /b # Quit using a specific exit code: exit exit_code . Basic Scripting Examples . When you need to execute multiple shell commands sequentially or want to do more complex stuffs, it’s better to enclose the commands in a bash script. . Running a Shell Script . Create a file with .sh extension. I have used Ubuntu’s built-in nano editor for that. . $ nano script.sh . | Put your code in the .sh file | Make sure you put the shebang #!/bin/bash at the beginning of each script, otherwise, the system wouldn’t know which interpreter to use. . | Give permission to run: . $ chmod +x script.sh . | Run the script via: . $ ./script . | If the script takes in one or multiple arguments, then place those with spaces in between. . $ ./script arg1 arg2 . | . conditionals (if-else) . Example-1: This program, Takes in two integers as arguments | Compares if one number is greater than the other or if they are equal | Returns the greater of the two numbers or if they are equal, returns equal | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number1=&quot;$1&quot; number2=&quot;$2&quot; if [ $number1 -eq $number2 ] then echo &quot;The numbers are equal&quot; elif [ $number1 -gt $number2 ] then echo &quot;The greater number is $number1&quot; elif [ $number2 -gt $number1 ] then echo &quot;The greater number is $number2&quot; fi . $ ./script.sh 12 13 The greater number is 13 . | Example-2: This program, Takes a single number as an argument | Checks whether the number is Odd or Even | Returns Odd or Even accordingly | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number=&quot;$1&quot; if [ $(( number%2 )) -eq 0 ] then echo &quot;Even&quot; else echo &quot;Odd&quot; fi . $ ./script.sh 20 Even . | Example-3: This program, Takes in two integers and an operation instruction | Returns the value according to the operation | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail echo &quot;Enter two numbers and the intended operation: * for addition, write add * for subtraction, write sub * for multiplication, write mul * for division, write div (write quit to quit the program)&quot; num1=&quot;$1&quot; num2=&quot;$2&quot; operation=&quot;$3&quot; if [ $num1 == &quot;quit&quot; ] then break elif [ $operation == &quot;add&quot; ] then ans=$(( $num1 + $num2 )) echo &quot;addition: $ans&quot; elif [ $operation == &quot;sub&quot; ] then ans=$(( $num1 - $num2 )) echo &quot;subtraction: $ans&quot; elif [ $operation == &quot;mul&quot; ] then ans=$(( $num1 * $num2 )) echo &quot;multiplication: $ans&quot; elif [ $operation == &quot;div&quot; ] then ans=$(( $num1 / $num2 )) echo &quot;division: $ans&quot; fi . $ ./script.sh 12 13 add 25 . | . for loop . Example-1: Looping through 0 to 9 with increment 3 and printing the numbers . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for var in {0..9..3} do echo $var done . $ ./script.sh 0 3 6 9 . | Example-2: Looping through files in a folder and printing them one by one . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for file in $(ls ./files) do echo $file done . $ ./script.sh numbers.txt sorted_numbers.txt . | Example-3: This program, Doesn’t take any argument | Returns the summation of all the integers, starting from 0, up to 100 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail sum=0 for num in $(seq 0 100) do sum=$(($sum + $num)) done echo &quot;Total sum is $sum&quot; . $ ./script.sh Total sum is 5050 . | Example-4: This program, Takes in an integer as an argument | Prints all the numbers up to that number, starting from 0 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail input_number=&quot;$1&quot; for num in $(seq 0 $input_number) do if [ $num -lt $input_number ] then echo $num fi done . $ ./script.sh 100 0 1 . . 99 . | . while loop . Example-1: This program, Takes in a single integer as an argument | Returns the factorial of that number | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail counter=&quot;$1&quot; factorial=1 while [ $counter -gt 0 ] do factorial=$(( $factorial * $counter )) counter=$(( $counter - 1 )) done echo &quot;Factorial of $1 is $factorial&quot; . $ ./script.sh 5 Factorial of 5 is 120 . | Example-2: This program, Takes two integers as arguments | Returns the summation of the numbers | Sending -1 as an input quits the program | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail while : do read -p &quot;Enter two numbers ( - 1 to quit ) : &quot; &quot;a&quot; &quot;b&quot; if [ $a -eq -1 ] then break fi ans=$(( $a + $b )) echo $ans done . $ ./script.sh Enter two numbers (-1 to quit): 20 30 30 . | Example-3: This program, Takes in a text filepath as argument | Reads and prints out each line of the file | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail file=&quot;$1&quot; while read -r line do echo &quot;$line&quot; done &lt; &quot;$file&quot; . $ ./script.sh files/numbers.txt 5 55 . . 11 10 . | . functions . Functions are incredible tools when we need to reuse code. Creating functions are fairly straight forward in bash. . Example-1: This function, . Takes a directory as an input argument | Counts the number of files in that directory and prints that out | Note that this function ignores the dot files (The ls -1 flag ignores dot files) | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function file_count () { ls -1 &quot;$1&quot; | wc -l } # calling the function echo $( file_count $1 ) . $ ./script.sh ./files $ 2 . | Example-2: This function, Takes in a shortcode for any of the following languages (a) en for English (b) fr for French (c) bn for bangla . | Returns a welcome message in the selected language . | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function greetings () { language=&quot;$1&quot; if [ $language == &quot;en&quot; ] then echo &quot;Greetings Mortals!&quot; elif [ $language == &quot;fr&quot; ] then echo &quot;Salutations Mortels!&quot; elif [ $language == &quot;bn&quot; ] then echo &quot;নশ্বরকে শুভেচ্ছা!&quot; fi } # calling the function echo $( greetings $1 ) . $ ./script.sh en Greetings Mortals! . | Example-3: This function, Takes a directory as an argument | Loop through the files | Only returns the text files with full path | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function return_text () { dir=&quot;$1&quot; for file in $dir&quot;/*.txt&quot; do echo &quot;$( realpath $file )&quot; done } echo &quot;$( return_text $1 )&quot; . $ ./script.sh /home/redowan/code/bash/files/numbers.txt /home/redowan/code/bash/files/sorted_numbers.txt . | . Some Good Practices . Use a Bash Strict Mode . Your bash scripts will be more robust, reliable and easy to debug if it starts with: . #!/bin/bash set -euo pipefail . This can be regarded as an unofficial bash strict mode and often prevents many classes of sneaky bugs in your script. The above command can be synthesized into multiple commands. . set -euo pipefail is short for: . set -e set -u set -o pipefail . Let’s have a look at each of them separately. . set-e: This instruction forces the bash script to exit immediately if any command has a non zero exit status. If there’s an issue in any of the lines in your code, the subsequent lines simply won’t run. . | set-u: If your code has a reference to any variable that wasn’t defined previously, this will cause the program to exit. . | set -o pipefail: This setting prevents errors in a pipeline being masked. If any command in a pipeline fails, that return code will be used as the return code of the whole pipeline, not the last command’s return code. . | . For a more in depth explanation of the different settings and Bash Strict Mode in general, check out, AAron Maxwell’s blog on this topic. . Double Quote Your Variables . It is generally a good practice to double quote your variables, specially user input variables where spaces are involved. . External Resources . Here are some awesome sources where you can always look into if you get stuck: . Command Line Crash Course | Ryans Bash Tutorial | W3 School CLI Tutorial | .",
            "url": "https://rednafi.github.io/digressions/linux/2019/09/05/essential-bash.html",
            "relUrl": "/linux/2019/09/05/essential-bash.html",
            "date": " • Sep 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Building stuff at @helloShopUp. Writing &amp; talking about — Statistics, Machine Learning, System Arch, APIs, Redis, Docker, Python, Golang etc. . Contact . Gmail: redowan.nafi@gmail.com | Github: rednafi | Twitter: rednafi | LinkedIn: Redowan Delowar | .",
          "url": "https://rednafi.github.io/digressions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}