{
  
    
        "post0": {
            "title": "The Curious Case of Python's Context Manager",
            "content": "Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: . with open(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: . try: f = open(&quot;file.txt&quot;, &quot;wt&quot;) text = f.write(&quot;contents go here&quot;) finally: f.close() . Writing Custom Context Managers . To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above. . class CustomFileOpen: &quot;&quot;&quot;Custom context manager for opening files.&quot;&quot;&quot; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() . You can use the above class just like a regular context manager. . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . From Generators to Context Managers . Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: . @contextlib.contextmanager def generator_func(&lt;arguments&gt;): &lt;setup&gt; try: yield &lt;value&gt; finally: &lt;cleanup&gt; . Let’s implement the same CustomFileOpen context manager with contextmanager decorator. . from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): &quot;&quot;&quot;Custom context manager for opening a file.&quot;&quot;&quot; f = open(filename, method) try: yield f finally: f.close() . Now use it just like before: . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . Writing Context Managers as Decorators . You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let’s make a RunTime decorator that will be applied on a file-opening function. The decorator will: . Print a user provided description of the function | Print the time it takes to run the function | . from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): &quot;&quot;&quot;Timing decorator.&quot;&quot;&quot; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . You can use the decorator like this: . @RunTime(&quot;This function opens a file&quot;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) . Using the function like this should return: . print(custom_file_write(&quot;file.txt&quot;, &quot;wt&quot;, &quot;jello&quot;)) . This function opens a file The function took 0.0005390644073486328 seconds to run. None . You can also create the same decorator via contextlib.contextmanager decorator. . from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . Nesting Contexts . You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: . from contextlib import contextmanager @contextmanager def get_state(name): print(&quot;entering:&quot;, name) yield name print(&quot;exiting :&quot;, name) # multiple get_state can be nested like this with get_state(&quot;A&quot;) as A, get_state(&quot;B&quot;) as B, get_state(&quot;C&quot;) as C: print(&quot;inside with statement:&quot;, A, B, C) . entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A . Notice the order they were closed. Context managers are treated as a stack, and should be exited in reverse order in which they were entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. . Combining Multiple Context Managers . You can combine multiple context managers too. Let’s consider these two managers. . from contextlib import contextmanager @contextmanager def a(name): print(&quot;entering a:&quot;, name) yield name print(&quot;exiting a:&quot;, name) @contextmanager def b(name): print(&quot;entering b:&quot;, name) yield name print(&quot;exiting b:&quot;, name) . Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. . @contextmanager def ab(a, b): with a(&quot;A&quot;) as A, b(&quot;B&quot;) as B: yield (A, B) . This can be used as: . with ab(a, b) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: (&#39;A&#39;, &#39;B&#39;) exiting b: B exiting a: A . If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let’s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. . from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] . with ab((a, b), (&quot;A&quot;, &quot;B&quot;)) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: [&#39;A&#39;, &#39;B&#39;] exiting b: B exiting a: A . Using Context Managers to Create SQLAlchemy Session . If you are familiar with SQLALchemy, Python’s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: . from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(&quot;sqlite://&quot;) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() . The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of commiting and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: . with session_scope() as session: myobject = MyObject(&quot;foo&quot;, &quot;bar&quot;) session.add(myobject) . Abstract Away Exception Handling Monstrosity with Context Managers . This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let’s write a decorator that will handle ZeroDivisionError and TypeError simultaneously. . from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(&quot;This is a custom ZeroDivisionError message.&quot;) raise except TypeError: print(&quot;This is a custom TypeError message.&quot;) raise . Now use this in a function where these exceptions occur. . @errhandler() def div(a, b): return a // b . div(&quot;b&quot;, 0) . This is a custom TypeError message. TypeError Traceback (most recent call last) &lt;ipython-input-43-65497ed57253&gt; in &lt;module&gt; -&gt; 1 div(&#39;b&#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): &gt; 75 return func(*args, **kwds) 76 return inner 77 &lt;ipython-input-42-b7041bcaa9e6&gt; in div(a, b) 1 @errhandler() 2 def div(a, b): -&gt; 3 return a // b TypeError: unsupported operand type(s) for //: &#39;str&#39; and &#39;int&#39; . You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? . The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logics from the main method. . import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=&quot; n(asctime)s [%(levelname)s] %(message)s&quot;, handlers=[logging.FileHandler(&quot;./debug.log&quot;), logging.StreamHandler()], ) class Calculation: &quot;&quot;&quot;Dummy class for demonstrating exception decoupling with contextmanager.&quot;&quot;&quot; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f&quot;Custom handling of Zero Division Error! Printing &quot; &quot;only 2 levels of traceback..&quot; ) logging.exception(&quot;ZeroDivisionError&quot;) def main_func(self): &quot;&quot;&quot;Function that we want to save from nasty error handling logic.&quot;&quot;&quot; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) . This will return . (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 25, in errorhandler yield File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None . Remarks . All the code snippets are updated for python 3.8. To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. . Resources . Python Contextlib Documentation | Python with Context Manager - Jeff Knupp | SQLALchemy Session Creation | Scipy Lectures: Context Managers | Merging Context Managers |",
            "url": "https://rednafi.github.io/digressions/python/2020/03/26/python-contextmanager.html",
            "relUrl": "/python/2020/03/26/python-contextmanager.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Up and Running with MySQL in Docker",
            "content": ". Setting Up . Installation . This part describes the basic installation steps of setting up MySQL 5.7 server on Ubuntu Linux using docker. . Install docker on your Linux machine. See the instruction here. . | Install docker compose via following the instructions here. . | Create another folder on your project folder and make a docker-compose.yml file. Run the following instructions one by one: . mkdir mysql_dump cd mysql_dump touch docker-compose.yml . | Open the docker-compose.yml file and copy the following lines into it. . # docker-compose version version: &quot;3.3&quot; services: # images mysql-dev: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test_db ports: - &quot;3306:3306&quot; # making data persistent volumes: - db-data:/var/lib/mysql volumes: db-data: . | . Run MySQL Server . Run the docker-compose command. This will build and run the server in detached mode. . docker compose up -d . Connect Shell to Server . Check the name of the running container with docker ps command. In this case, the running container is called mysql_dumps_mysql-dev_1. Then run the following command to connect your shell to the running server. . # connect shell to server docker exec -it mysql_dumps_mysql-dev_1 mysql -uroot -p . Alter Root Password . If you want to change the root password, enter the following command in the MySQL shell. Replace MyNewPass with your new root password: . ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;; . You should see something like this in the command prompt: . Query OK, 0 rows affected (0.02 sec) . To make the change take effect, type the following command: . FLUSH PRIVILEGES; . View Users . MySQL stores the user information in its own database. The name of the database is mysql. If you want to see what users are set up in the MySQL user table, run the following command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like this: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | ++--+-+ 4 rows in set (0.00 sec) . Create a Database . According to the docker-compose.yml file, you already have created a database named test_db. You can create anotehr database named test_db_2 via the following command: . CREATE DATABASE test_db_2; . List your databases via the following command: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | mysql | | performance_schema | | sys | | test_db | | test_db_2 | +--+ 6 rows in set (0.01 sec) . To ensure the changes: . FLUSH PRIVILEGES; . Creating Dummy Table in the Database . -- create dummy table CREATE TABLE IF NOT EXISTS `student` ( `id` int(2) NOT NULL DEFAULT &#39;0&#39;, `name` varchar(50) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `class` varchar(10) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `mark` int(3) NOT NULL DEFAULT &#39;0&#39;, `sex` varchar(6) CHARACTER SET utf8 NOT NULL DEFAULT &#39;male&#39; ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- insert data into the dummy table INSERT INTO `student` (`id`, `name`, `class`, `mark`, `sex`) VALUES (1, &#39;John Deo&#39;, &#39;Four&#39;, 75, &#39;female&#39;), (2, &#39;Max Ruin&#39;, &#39;Three&#39;, 85, &#39;male&#39;), (3, &#39;Arnold&#39;, &#39;Three&#39;, 55, &#39;male&#39;), (4, &#39;Krish Star&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (5, &#39;John Mike&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (6, &#39;Alex John&#39;, &#39;Four&#39;, 55, &#39;male&#39;), (7, &#39;My John Rob&#39;, &#39;Fifth&#39;, 78, &#39;male&#39;), (8, &#39;Asruid&#39;, &#39;Five&#39;, 85, &#39;male&#39;), (9, &#39;Tes Qry&#39;, &#39;Six&#39;, 78, &#39;male&#39;), (10, &#39;Big John&#39;, &#39;Four&#39;, 55, &#39;female&#39;), (11, &#39;Ronald&#39;, &#39;Six&#39;, 89, &#39;female&#39;), (12, &#39;Recky&#39;, &#39;Six&#39;, 94, &#39;female&#39;), (13, &#39;Kty&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (14, &#39;Bigy&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (15, &#39;Tade Row&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (16, &#39;Gimmy&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (17, &#39;Tumyu&#39;, &#39;Six&#39;, 54, &#39;male&#39;), (18, &#39;Honny&#39;, &#39;Five&#39;, 75, &#39;male&#39;), (19, &#39;Tinny&#39;, &#39;Nine&#39;, 18, &#39;male&#39;), (20, &#39;Jackly&#39;, &#39;Nine&#39;, 65, &#39;female&#39;), (21, &#39;Babby John&#39;, &#39;Four&#39;, 69, &#39;female&#39;), (22, &#39;Reggid&#39;, &#39;Seven&#39;, 55, &#39;female&#39;), (23, &#39;Herod&#39;, &#39;Eight&#39;, 79, &#39;male&#39;), (24, &#39;Tiddy Now&#39;, &#39;Seven&#39;, 78, &#39;male&#39;), (25, &#39;Giff Tow&#39;, &#39;Seven&#39;, 88, &#39;male&#39;), (26, &#39;Crelea&#39;, &#39;Seven&#39;, 79, &#39;male&#39;), (27, &#39;Big Nose&#39;, &#39;Three&#39;, 81, &#39;female&#39;), (28, &#39;Rojj Base&#39;, &#39;Seven&#39;, 86, &#39;female&#39;), (29, &#39;Tess Played&#39;, &#39;Seven&#39;, 55, &#39;male&#39;), (30, &#39;Reppy Red&#39;, &#39;Six&#39;, 79, &#39;female&#39;), (31, &#39;Marry Toeey&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (32, &#39;Binn Rott&#39;, &#39;Seven&#39;, 90, &#39;female&#39;), (33, &#39;Kenn Rein&#39;, &#39;Six&#39;, 96, &#39;female&#39;), (34, &#39;Gain Toe&#39;, &#39;Seven&#39;, 69, &#39;male&#39;), (35, &#39;Rows Noump&#39;, &#39;Six&#39;, 88, &#39;female&#39;); . Show Tables . USE test_db; SHOW tables; . Delete a Database . To delete a database test_db run the following command: . DROP DATABASE test_db, FLUSH PRIVILEGES; . Add a Database User . To create a new user (here, we created a new user named redowan with the password password), run the following command in the MySQL shell: . CREATE USER &#39;redowan&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; FlUSH PRIVILEGES; . Ensure that the changes has been saved via running FLUSH PRIVILEGES;. Verify that a user has been successfully created via running the previous command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like below. Notice that a new user named redowan has been created: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | | redowan | localhost | *0756A562377EDF6ED3AC45A00B356AAE6D3C6BB6 | ++--+-+ . Delete a Database User . To delete a database user (here, I’m deleting the user-redowan) run: . DELETE FROM mysql.user WHERE user=&#39;&lt;redowan&gt;&#39; AND host = &#39;localhost&#39; FlUSH PRIVILEGES; . Grant Database User Permissions . Give the user full permissions for your new database by running the following command (Here, I provided full permission of test_db to the user redowan: . GRANT ALL PRIVILEGES ON test_db.table TO &#39;redowan&#39;@&#39;localhost&#39;; . If you want to give permission to all the databases, type: . GRANT ALL PRIVILEGES ON *.* TO &#39;redowan&#39;@&#39;localhost&#39;; FlUSH PRIVILEGES; . Loading Sample Database to Your Own MySQL Server . To load mysqlsampledatabase.sql to your own server (In this case the user is redowan. Provide database password in the prompt), first fireup the server and type the following commands: . mysql -u redowan -p test_db &lt; mysqlsampledatabase.sql; . Now run: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | classicmodels | | mysql | | performance_schema | | sys | | test_db | +--+ 6 rows in set (0.00 sec) . Stop the Server . The following command stops the server. . docker-compose down . Notice that a new database named classicmodels has been added to the list. . Connecting to a Third Party Client . We will be using DBeaver as a third party client. While you can use the mysql shell to work on your data, a third partly client that make the experience much better with auto formatting, earsier import features, syntax highlighting etc. . Installing DBeaver . You can install DBeaver installer from here. Installation is pretty straight forward. . Connecting MySQL Database to DBeaver . Fire up DBeaver and you should be presented with this screen. Select MySQL 8+ and go next. . . The dialogue box will ask for credentials to connect to a database. In this case, I will log into previously created local database test_db with the username redowan, corresponding password password and press test connection tab. A dialogue box might pop up, prompting you download necessary drivers. . . If everything is okay, you should see a success message. You can select the SQL Editor and start writing your MySQL scripts right away. . Connecting to MySQL Server via Python . PyMySQL and DBUtils can be used to connect to MySQL Server. . import pymysql import os from dotenv import load_dotenv from DBUtils.PooledDB import PooledDB load_dotenv(verbose=True) MYSQL_REPLICA_CONFIG = { &quot;host&quot;: os.environ.get(&quot;SQL_HOST&quot;), &quot;port&quot;: int(os.environ.get(&quot;SQL_PORT&quot;)), &quot;db&quot;: os.environ.get(&quot;SQL_DB&quot;), &quot;password&quot;: os.environ.get(&quot;SQL_PASSWORD&quot;), &quot;user&quot;: os.environ.get(&quot;SQL_USER&quot;), &quot;charset&quot;: os.environ.get(&quot;SQL_CHARSET&quot;), &quot;cursorclass&quot;: pymysql.cursors.DictCursor, } # class to create database connection pooling POOL = PooledDB(**configs.MYSQL_POOL_CONFIG, **configs.MYSQL_REPLICA_CONFIG) class SqlPooled: &quot;&quot;&quot;Sql connection with pooling.&quot;&quot;&quot; def __init__(self): self._connection = POOL.connection() self._cursor = self._connection.cursor() def fetch_one(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchone() return result def fetch_all(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchall() return result def __del__(self): self._connection.close() .",
            "url": "https://rednafi.github.io/digressions/database/2020/03/15/mysql-install.html",
            "relUrl": "/database/2020/03/15/mysql-install.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Reduce Boilerplate Code with Python's Dataclasses",
            "content": "Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: . class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; print(CartesianPoint(1, 2, 3)) . &gt;&gt; CartesianPoint(x = 1, y = 2, z = 3) . This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. . Dataclasses . Let’s see how dataclasses can help to improve this situation. Dataclasses were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) . &gt;&gt; CartesianPoint(x=1, y=2, z=3) . In the above code, the magic is done by the dataclass decorator. Dataclasses require you to use explicit type annotation and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via python’s help. . help(CartesianPoint) . Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -&gt; None | | __repr__(self) | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes defined here: | | __annotations__ = {&#39;x&#39;: &lt;class &#39;float&#39;&gt;, &#39;y&#39;: &lt;class &#39;float&#39;&gt;, &#39;z&#39;: &lt;c... | | __dataclass_fields__ = {&#39;x&#39;: Field(name=&#39;x&#39;,type=&lt;class &#39;float&#39;&gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None . Using Default Values . You can provide default values to the fields in the following way: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 . Using Arbitrary Field Type . If you don’t want to specify your field type during type hinting, you can use Any type from python’s typing module. . from dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any . Instance Ordering . You can check if two instances are equal without making any modification to the class. . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) . &gt;&gt; False . However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually. . from dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 &gt; point_2) . &gt;&gt; True . By default, while comparing intances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function. . Suppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way: . from dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create intance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 &lt; point_2) . &gt;&gt; True &gt;&gt; False . You can see the above code prints out True despite the intances have different y and z attributes. . Adding Methods . Methods can be added to dataclasses just like normal classes. Let’s add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin. . from dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) # create a new instance and use method `abs_val` point = CartesianPoint(5, 6, 7) norm = point.abs_val() print(norm) . &gt;&gt; 10.488088481701515 . Making Instances Immutable . By default, instances of dataclasses are immutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass. . from dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float . If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError. . point = CartesianPoint(2, 4, 6) point.x = 23 . FrozenInstanceError Traceback (most recent call last) &lt;ipython-input-34-b712968bd0eb&gt; in &lt;module&gt; 1 point = CartesianPoint(2, 4, 6) -&gt; 2 point.x = 23 &lt;string&gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;x&#39; . Making Instances Hashable . You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perfom set operation on them. However, if you are using unsafe_hash make sure that your dataclasses do not contain any mutable data structure in it. . from dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f&quot;{point}&quot;: &quot;origin&quot;}) . &gt;&gt; {&#39;CartesianPoint(x=0, y=0, z=0)&#39;: &#39;origin&#39;} . Converting Instances to Dicts . The asdict() function converts a dataclass instance to a dict of its fields. . from dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) . &gt;&gt; {&#39;x&#39;: 1, &#39;y&#39;: 5, &#39;z&#39;: 6} . Post-init Processing . When dataclass generates the __init__ method, internally it will call _post_init__ method. You can add additional processing in the __post_init__ method. Here, I have added another attribute tup that returns the cartesian point as a tuple. . from dataclasses import dataclass @dataclass class CartesianPoint: x : float y : float z : float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) . &gt;&gt; (4, 5, 6) . Refactoring the Entire Cartesian Point Class . The feature rich original CartesianPoint looks something like this: . import math class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): &quot;&quot;&quot;Print the instance neatly.&quot;&quot;&quot; return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; def __eq__(self, other): &quot;Checks if equal.&quot; return self.x == other.x def __nq__(self, other): &quot;&quot;&quot;Checks non equality.&quot;&quot;&quot; return self.x != other.x def __gt__(self, other): &quot;&quot;&quot;Checks if greater than.&quot;&quot;&quot; return self.x &gt; other.x def __ge__(self, other): &quot;&quot;&quot;Checks if greater than or equal.&quot;&quot;&quot; return self.x &gt;= other.x def __lt__(self, other): &quot;&quot;&quot;Checks if less than.&quot;&quot;&quot; return self.x &lt; other.x def __le__(self, other): &quot;&quot;&quot;Checks if less than or equal.&quot;&quot;&quot; return self.x &lt;= other.x def __hash__(self): &quot;&quot;&quot;Make the instances hashable.&quot;&quot;&quot; return hash(self) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Let’s see the class in action: . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x = 1, y = 2, z = 3) True True False True {&#39;CartesianPoint(x = 1, y = 2, z = 3)&#39;: 3.7416573867739413} . Below is the same class refactored using dataclass. . from dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Use this class like before. . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x=1, y=2, z=3) True True False True {&#39;CartesianPoint(x=1, y=2, z=3)&#39;: 3.7416573867739413} . References . Python Dataclasses: Official Doc | The Ultimate Guide to Data Classes in Python 3.7 | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/12/python-dataclasses.html",
            "relUrl": "/python/2020/03/12/python-dataclasses.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Python Virtual Environment Workflow for Sanity",
            "content": "There are multiple ways of installing Python, creating and switching between different virtual environments. Also, Python’s package manager hyperspace is a mess. So, things can quickly get out of hands while dealing with projects that require quick environment switching across multiple versions of Python. I use Debian linux in my primary development environment and this is how I keep the option explosion in check: . Installing Python . Run the following commands one by one: . # update the packages list and install the prerequisites sudo apt update sudo apt install software-properties-common # add deadsnakes ppa to your sources&#39; list (When prompted press Enter to continue) sudo add-apt-repository ppa:deadsnakes/ppa # install python3.7 sudo apt install python3.8 # verify python installation python3.8 --version . Creating Virtual Environment . There are multiple ways creating and switching between different environments can be done. I use venv for creating virtual environments. For demonstration, here I’m creating a virtual environment that uses python3.8. . Install python3-venv for creating virtual environment sudo apt install python3.8-venv . | Create virtual environment named venv in the project folder . python3.8 -m venv venv . | Activate venv . source venv/bin/activate . | Deactivate venv deactivate . | . Switching Between Different Environments . To create another environment with a different python version, you have to: . Install the desired version of python following the procedures stated above. | Install python3.7-venv specific for your python version, like if you are using python3.7, you should run: . sudo apt install python3.7-venv . | Create multiple environments with multiple versions and name them distinctively. i.e. venv3.7, venv3.8 etc. Follow the instructions above. | Activate and deactivate the desired virtual environment. | . Package Management . For local development, I use pip. | For production application and libraries poetry is preferred. | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/11/python-venv-workflow.html",
            "relUrl": "/python/2020/03/11/python-venv-workflow.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Polymorphic Dispatching with Python's Singledispatch",
            "content": "Recently, I was refactoring a portion of a Python function that somewhat looked like this: . def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 &amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 &amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 &amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... . This pattern gets tedious when the number of conditions and actionable functions starts to grow. I was looking for a functional approach to dynamically overload a function based on some specific conditions. Let’s see how Python’s singledispatch decorator can help to design a better solution. . Function Overloading . Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. . Singledispatch . Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. . A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. . As PEP-443 said, singledispatch only happens based on the first argument’s type. Let’s take a look at an example to see how this works! . Example-1: Singledispatch with built-in argument type . Let’s consider the following code: . # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; def process_float(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this code will return . &gt;&gt; Float 12.0 has been processed successfully! &gt;&gt; Integer 1 has been processed successfully! . The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let’s see how the same thing can be achieved with singledispatch. . # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(&quot;Implement process function.&quot;) @process.register(int) def sub_process(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; @process.register(float) def sub_process(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this will return the same result as before. . &gt;&gt; Float 12.0 has been processed successfully! &gt;&gt; Integer 1 has been processed successfully! . Example-2: Singledispatch with custom argument type . Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: . def process(data: dict): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return process_cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return process_dog(data) def process_cat(data: dict): # processing cat return &quot;Cat data has been processed successfully!&quot; def process_dog(data: dict): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} # using process print(process(cat_data)) print(process(dog_data)) . Running this snippet will print out: . &gt;&gt; Cat data has been processed successfully! &gt;&gt; Dog data has been processed successfully! . To refactor this with singledispatch, you can create two data types Cat and Dog. A class_factory function will determine data type based on condition and singledispatch will take care of dispatching the appropriate implementation of the process function. . from functools import singledispatch from dataclasses import dataclass @dataclass class Cat: data: dict @dataclass class Dog: data: dict def class_factory(data): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return Cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return Dog(data) @singledispatch def process(obj=None): raise NotImplementedError(&quot;Implement process for bucket&quot;) @process.register(Cat) def sub_process(obj): # processing cat return &quot;Cat data has been processed successfully!&quot; @process.register(Dog) def sub_process(obj): return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} cat_obj = class_factory(cat_data) dog_obj = class_factory(dog_data) print(process(cat_obj)) print(process(dog_obj)) . Running this will print out the same output as before: . &gt;&gt; Cat data has been processed successfully! &gt;&gt; Dog data has been processed successfully! .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/09/python-singledispatch.html",
            "relUrl": "/python/2020/03/09/python-singledispatch.html",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . blog, defined in ci.yaml. Like all Actions workflows, this is YAML file is located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://hamelsmu/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called hamelsmu/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://rednafi.github.io/digressions/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rednafi.github.io/digressions/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://rednafi.github.io/digressions/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "A Minimalistic Approach to ZSH",
            "content": ". Although I’m on Debian Linux, Apple’s recent announcement about replacing Bash with Zsh on MacOS made me take a look at Z-shell aka zsh. It’s a POSIX compliant Bash alternative that has been around for quite a long time. While Bash shell’s efficiency and ubiquity make it hard to think about changing the default shell of your primary development machine, I find its features as an interactive shell to be somewhat limited. So I did some digging around and soon found out that zsh’s lackluster default configurations and bloated ecosystem make it difficult for someone who just want to switch without any extra overhead. So, let’s make the process quicker. Here is what we are aiming for: . A working shell that can (almost always) take bash commands without complaining (looking at you fish) | History based autocomplete | Syntax highlighting | Git branch annotations | . Instructions were applied and tested on debian based linux (Ubuntu) . Install Z Shell . GNU/Linux . To install on a debian based linux, type: . $ apt install zsh . MacOS . Use homebrew to install zsh on MacOs. Run: . $ brew install zsh . Make Zsh as Your Default Shell . Run: . $ chsh -s $(which zsh) . Install Oh-My-Zsh Framework . Oh-My-Zsh is the tool that makes zsh so much fun and overly configurable at the same time. So we’ll tread here carefully. To install oh-my-zsh , type: . $ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; . Set Firacode As the Default Terminal Font . Your selected theme may not display all the glyphs if the default terminal font doesn’t support them. Installing a font with glyphs and ligature support can solve this. I recommend installing firacode and setting that as your default terminal font. Install Fira Code From here. . Set Syntax Highlighting . Using zsh-syntax-highlighting to achieve this. . Clone this repository in oh-my-zsh’s plugins directory . $ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . | Activate the plugin in ~/.zshrc . plugins=( [plugins...] zsh-syntax-highlighting) . | Source ~/.zshrc . | . Set Suggestions . Using zsh-autosuggestions to achieve this. . Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) . $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . | Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc ) . plugins=(zsh-autosuggestions) . | Source ~/.zshrc . | . Start a new terminal session to see the effects!!! You might need to log out and log in again for the changes to be effective. . Load .profile from .zprofile . Add the following lines to ~/.zprofile and source via the command: source ~/.zprofile. Make sure you are on zsh before running the source command. . [[ -e ~/.profile ]] &amp;&amp; emulate sh -c &#39;source ~/.profile&#39; . A Barebone ~/.zshrc . Instead of adding the plugins individually, you can just install the plugins and then add this barebone config to your ~/.zshrc . Don’t forget to replace YourUserName with your username. Source your zshrc once you are done. . # ===================== # MINIMALIST ZSHRC # AUTHOR: REDNAFI # ===================== # omz path export ZSH=&quot;$HOME/.oh-my-zsh&quot; # theme settings ZSH_THEME=&quot;juanghurtado&quot; # pluging settings plugins=(git zsh-syntax-highlighting zsh-autosuggestions) # autosuggestion highlight ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=4&quot; # source omz source $ZSH/oh-my-zsh.sh #History setup HISTFILE=$HOME/.zsh_history HISTSIZE=100000 SAVEHIST=$HISTSIZ zstyle &#39;:completion:*&#39; menu select zstyle &#39;:completion:*&#39; group-name &#39;&#39; # group results by category zstyle &#39;:completion:::::&#39; completer _expand _complete _ignored _approximate #enable approximate matches for completion #disable auto correct unsetopt correct_all . Set Terminal Color (Optional) . Optionally you customize your terminal color and in this case I’ve used Gogh to achieve this. . Pre Install | . $ sudo apt-get install dconf-cli uuid-runtime . Install on Linux | . $ bash -c &quot;$(wget -qO- https://git.io/vQgMr)&quot; . Install on MacOS $ bash -c &quot;$(curl -sLo- https://git.io/vQgMr)&quot; . | Put the code associated with your desired color scheme. | . Updating OMZ . $ upgrade_oh_my_zsh . Uninstall Zsh . $ sudo apt-get --purge remove zsh . Uninstall OMZ . $ uninstall_oh_my_zsh . Switch Back to Bash . $ chsh -s $(which bash) . Reference . Oh-My-Zsh | FiraCode | Gogh |",
            "url": "https://rednafi.github.io/digressions/linux/2019/10/29/minimal-zsh.html",
            "relUrl": "/linux/2019/10/29/minimal-zsh.html",
            "date": " • Oct 29, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Essential Bash Scripting",
            "content": ". Shell . Several layers of events take place whenever a Linux command is entered into the terminal. The top layer of that is known as shell. . A shell is any user interface to the UNIX operating system, i.e., any program that takes input from the user, translates it into instructions that the operating system can understand, and conveys the operating system’s output back to the user. . Let’s look at an example: . sort -n src/files/numbers.txt &gt; src/files/sorted_numbers.txt . This command will perform the following tasks: . Go to the src/files directory | Sort the numbers in the numbers.txt files in ascending order | Save the result in a new file called sorted_numbers.txt in the same directory | . History . The first major shell was the Bourne shell (named after its inventor, Steven Bourne); it was included in the first popular version of UNIX, Version 7, starting in 1979. The Bourne shell is known on the system as sh. Although UNIX has gone through many, many changes, the Bourne shell is still popular and essentially unchanged. Several UNIX utilities and administration features depend on it. . Variants of some popular shells: . C Shell or csh (The syntax has resemblance with C programming language) | Korn Shell or ksh (Similar to Bourne Shell with features from both Bourne and C Shell) | The Bourne Again Shell or BASH (Started with the GNU project in 1988.) | . BASH is going to be our primary focus here. . A Few Basic Commands . List of most frequently used commands. All of these commands can be run directly from a bash command prompt: . cd | ls | cat | cp | mv | mkdir | rm | grep | lp | . All of the following command summaries can be found via: . curl cheat.sh/&lt;prompt&gt; . cd . cd is used to change directory . #Go to the given directory cd path/to/directory #Go to home directory of current user cd #Go up to the parent of the current directory cd .. #Go to the previously chosen directory cd - . ls . ls lists all the files and folders in a user-specified directory . # Displays everything in the target directory ls path/to/the/target/directory # Displays everything including hidden files ls -a # Displays all files, along with the size (with unit suffixes) and timestamp ls -lh # Display files, sorted by size ls -S # Display directories only ls -d */ # Display directories only, include hidden ls -d .*/ */ . cat . cat shows the contents of a user-specified file . # Display the contents of a file cat /path/to/foo # Display contents with line numbers cat -n /path/to/foo # Display contents with line numbers (blank lines excluded) cat -b /path/to/foo . cp . cp copies files or folders from one directory to another . # Create a copy of a file cp ~/Desktop/foo.txt ~/Downloads/foo.txt # Create a copy of a directory cp -r ~/Desktop/cruise_pics/ ~/Pictures/ # Create a copy but ask to overwrite if the destination file already exists cp -i ~/Desktop/foo.txt ~/Documents/foo.txt # Create a backup file with date cp foo.txt{,.&quot;$(date +%Y%m%d-%H%M%S)&quot;} . mv . mv moves files or folders from one directory to another and can also be used to rename files or folders . # Move a file from one place to another mv ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another and automatically overwrite if the destination file exists # (This will override any previous -i or -n args) mv -f ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but ask before overwriting an existing file # (This will override any previous -f or -n args) mv -i ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but never overwrite anything # (This will override any previous -f or -i args) mv -n ~/Desktop/foo.txt ~/Documents/foo.txt # Move listed files to a directory mv -t ~/Desktop/ file1 file2 file3 . mkdir . mkdir is used to create a folder in a directory . # Create a directory and all its parents mkdir -p foo/bar/baz # Create foo/bar and foo/baz directories mkdir -p foo/{bar,baz} # Create the foo/bar, foo/baz, foo/baz/zip and foo/baz/zap directories mkdir -p foo/{bar,baz/{zip,zap}} . rm . rm is mainly used to delete files or folders . # Remove files and subdirs rm -rf path/to/the/target/ # Ignore non existent files rm -f path/to/the/target # Remove a file with his inode find /tmp/ -inum 6666 -exec rm -i &#39;{}&#39; ; . grep . grep can be used to search through the output of another command . # Search a file for a pattern grep pattern file # Case insensitive search (with line numbers) grep -in pattern file # Recursively grep for string &lt;pattern&gt; in folder: grep -R pattern folder # Read search patterns from a file (one per line) grep -f pattern_file file # Find lines NOT containing pattern grep -v pattern file # You can grep with regular expressions grep &quot;^00&quot; file #Match lines starting with 00 grep -E &quot;[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3}&quot; file #Find IP add # Find all files which match {pattern} in {directory} # This will show: &quot;file:line my research&quot; grep -rnw &#39;directory&#39; -e &quot;pattern&quot; # Exclude grep from your grepped output of ps. # Add [] to the first letter. Ex: sshd -&gt; [s]shd ps aux | grep &#39;[h]ttpd&#39; # Colour in red {bash} and keep all other lines ps aux | grep -E --color &#39;bash|$&#39; . lp . lp prints the specified output via an available printer . # lp # Print files. # Print the output of a command to the default printer (see `lpstat` command): echo &quot;test&quot; | lp # Print a file to the default printer: lp path/to/filename # Print a file to a named printer (see `lpstat` command): lp -d printer_name path/to/filename # Print N copies of a file to the default printer (replace N with the desired number of copies): lp -n N path/to/filename # Print only certain pages to the default printer (print pages 1, 3-5, and 16): lp -P 1,3-5,16 path/to/filename # Resume printing a job: lp -i job_id -H resume . clear . clear is used to clear the CLI window . # clear # Clears the screen of the terminal. # Clear the screen (equivalent to typing Control-L when using the bash shell): clear . exit . exit closes the CLI window . # exit # Quit the current CMD instance or the current batch file. # More information: #&lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/exit&gt;. # Quit the current CMD instance: exit # Quit the current batch script: exit /b # Quit using a specific exit code: exit exit_code . Basic Scripting Examples . When you need to execute multiple shell commands sequentially or want to do more complex stuffs, it’s better to enclose the commands in a bash script. . Running a Shell Script . Create a file with .sh extension. I have used Ubuntu’s built-in nano editor for that. . $ nano script.sh . | Put your code in the .sh file | Make sure you put the shebang #!/bin/bash at the beginning of each script, otherwise, the system wouldn’t know which interpreter to use. . | Give permission to run: . $ chmod +x script.sh . | Run the script via: . $ ./script . | If the script takes in one or multiple arguments, then place those with spaces in between. . $ ./script arg1 arg2 . | . conditionals (if-else) . Example-1: This program, Takes in two integers as arguments | Compares if one number is greater than the other or if they are equal | Returns the greater of the two numbers or if they are equal, returns equal | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number1=&quot;$1&quot; number2=&quot;$2&quot; if [ $number1 -eq $number2 ] then echo &quot;The numbers are equal&quot; elif [ $number1 -gt $number2 ] then echo &quot;The greater number is $number1&quot; elif [ $number2 -gt $number1 ] then echo &quot;The greater number is $number2&quot; fi . $ ./script.sh 12 13 The greater number is 13 . | Example-2: This program, Takes a single number as an argument | Checks whether the number is Odd or Even | Returns Odd or Even accordingly | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number=&quot;$1&quot; if [ $(( number%2 )) -eq 0 ] then echo &quot;Even&quot; else echo &quot;Odd&quot; fi . $ ./script.sh 20 Even . | Example-3: This program, Takes in two integers and an operation instruction | Returns the value according to the operation | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail echo &quot;Enter two numbers and the intended operation: * for addition, write add * for subtraction, write sub * for multiplication, write mul * for division, write div (write quit to quit the program)&quot; num1=&quot;$1&quot; num2=&quot;$2&quot; operation=&quot;$3&quot; if [ $num1 == &quot;quit&quot; ] then break elif [ $operation == &quot;add&quot; ] then ans=$(( $num1 + $num2 )) echo &quot;addition: $ans&quot; elif [ $operation == &quot;sub&quot; ] then ans=$(( $num1 - $num2 )) echo &quot;subtraction: $ans&quot; elif [ $operation == &quot;mul&quot; ] then ans=$(( $num1 * $num2 )) echo &quot;multiplication: $ans&quot; elif [ $operation == &quot;div&quot; ] then ans=$(( $num1 / $num2 )) echo &quot;division: $ans&quot; fi . $ ./script.sh 12 13 add 25 . | . for loop . Example-1: Looping through 0 to 9 with increment 3 and printing the numbers . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for var in {0..9..3} do echo $var done . $ ./script.sh 0 3 6 9 . | Example-2: Looping through files in a folder and printing them one by one . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for file in $(ls ./files) do echo $file done . $ ./script.sh numbers.txt sorted_numbers.txt . | Example-3: This program, Doesn’t take any argument | Returns the summation of all the integers, starting from 0, up to 100 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail sum=0 for num in $(seq 0 100) do sum=$(($sum + $num)) done echo &quot;Total sum is $sum&quot; . $ ./script.sh Total sum is 5050 . | Example-4: This program, Takes in an integer as an argument | Prints all the numbers up to that number, starting from 0 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail input_number=&quot;$1&quot; for num in $(seq 0 $input_number) do if [ $num -lt $input_number ] then echo $num fi done . $ ./script.sh 100 0 1 . . 99 . | . while loop . Example-1: This program, Takes in a single integer as an argument | Returns the factorial of that number | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail counter=&quot;$1&quot; factorial=1 while [ $counter -gt 0 ] do factorial=$(( $factorial * $counter )) counter=$(( $counter - 1 )) done echo &quot;Factorial of $1 is $factorial&quot; . $ ./script.sh 5 Factorial of 5 is 120 . | Example-2: This program, Takes two integers as arguments | Returns the summation of the numbers | Sending -1 as an input quits the program | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail while : do read -p &quot;Enter two numbers ( - 1 to quit ) : &quot; &quot;a&quot; &quot;b&quot; if [ $a -eq -1 ] then break fi ans=$(( $a + $b )) echo $ans done . $ ./script.sh Enter two numbers (-1 to quit): 20 30 30 . | Example-3: This program, Takes in a text filepath as argument | Reads and prints out each line of the file | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail file=&quot;$1&quot; while read -r line do echo &quot;$line&quot; done &lt; &quot;$file&quot; . $ ./script.sh files/numbers.txt 5 55 . . 11 10 . | . functions . Functions are incredible tools when we need to reuse code. Creating functions are fairly straight forward in bash. . Example-1: This function, . Takes a directory as an input argument | Counts the number of files in that directory and prints that out | Note that this function ignores the dot files (The ls -1 flag ignores dot files) | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function file_count () { ls -1 &quot;$1&quot; | wc -l } # calling the function echo $( file_count $1 ) . $ ./script.sh ./files $ 2 . | Example-2: This function, Takes in a shortcode for any of the following languages (a) en for English (b) fr for French (c) bn for bangla . | Returns a welcome message in the selected language . | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function greetings () { language=&quot;$1&quot; if [ $language == &quot;en&quot; ] then echo &quot;Greetings Mortals!&quot; elif [ $language == &quot;fr&quot; ] then echo &quot;Salutations Mortels!&quot; elif [ $language == &quot;bn&quot; ] then echo &quot;নশ্বরকে শুভেচ্ছা!&quot; fi } # calling the function echo $( greetings $1 ) . $ ./script.sh en Greetings Mortals! . | Example-3: This function, Takes a directory as an argument | Loop through the files | Only returns the text files with full path | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function return_text () { dir=&quot;$1&quot; for file in $dir&quot;/*.txt&quot; do echo &quot;$( realpath $file )&quot; done } echo &quot;$( return_text $1 )&quot; . $ ./script.sh /home/redowan/code/bash/files/numbers.txt /home/redowan/code/bash/files/sorted_numbers.txt . | . Some Good Practices . Use a Bash Strict Mode . Your bash scripts will be more robust, reliable and easy to debug if it starts with: . #!/bin/bash set -euo pipefail . This can be regarded as an unofficial bash strict mode and often prevents many classes of sneaky bugs in your script. The above command can be synthesized into multiple commands. . set -euo pipefail is short for: . set -e set -u set -o pipefail . Let’s have a look at each of them separately. . set-e: This instruction forces the bash script to exit immediately if any command has a non zero exit status. If there is an issue in any of the lines in your code, the subsequent lines simply won’t run. . | set-u: If your code has a reference to any variable that wasn’t defined previously, this will cause the program to exit. . | set -o pipefail: This setting prevents errors in a pipeline being masked. If any command in a pipeline fails, that return code will be used as the return code of the whole pipeline, not the last command’s return code. . | . For a more in depth explanation of the different settings and Bash Strict Mode in general, check out, AAron Maxwell’s blog on this topic. . Double Quote Your Variables . It is generally a good practice to double quote your variables, specially user input variables where spaces are involved. . External Resources . Here are some awesome sources where you can always look into if you get stuck: . Command Line Crash Course | Ryans Bash Tutorial | W3 School CLI Tutorial | .",
            "url": "https://rednafi.github.io/digressions/linux/2019/09/05/essential-bash.html",
            "relUrl": "/linux/2019/09/05/essential-bash.html",
            "date": " • Sep 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Redowan Delowar is currently working as a junior Data Scientist for ShopUp (~2 years). . Redowan primarily focuses on Computational Statistics, Representation Learning and Software development. He is an avid Machine Learning evangelist, researcher, practitioner and Open Source developer. He is also available for on-site teaching, presentations and certain types of short term contract works.  . Contact . Gmail: redowan.nafi@gmail.com | Github: rednafi | Twitter: rednafi | LinkedIn: Redowan Delowar | .",
          "url": "https://rednafi.github.io/digressions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}