{
  
    
        "post0": {
            "title": "Effortless API Request Caching with Python & Redis",
            "content": "Recently, I was working with MapBox‚Äôs Route Optimization API. Basically, it tries to solve the traveling salesman problem where you provide the API with coordinates of multiple places and it returns a duration-optimized route between those locations. This is a perfect usecase where Redis caching can come handy. Redis is an blazing fast, lightweight in-memory database with additional persistence options; making it a perfect candidate for the task at hand. Here, caching can save you from making redundant API requests and also, it can dramatically improve the response time as well. . I found that in my country, the optimized routes returned by the API do not change dramatically for at least for a couple of hours. So the workflow will look something like this: . Caching the API response in Redis using the key-value data structure. Here the requested coordinate-string will be the key and the response will be the corresponding value | Setting a timeout on the records | Serving new requests from cache if the records exist | Only send a new request to MapBox API if the response is not cached and then add that response to cache | . Setting Up Redis &amp; RedisInsight . To proceed with the above workflow, you‚Äôll need to install and setup Redis database on your system. For monitoring the database, I‚Äôll be using RedisInsight. The easiest way to setup Redis and RedisInsight is through docker and docker-compose. Here‚Äôs a docker-compose that you can use to setup everything with a single command. . # docker-compose.yml version: &quot;3.2&quot; services: redis: container_name: redis-cont image: &quot;redis:alpine&quot; command: redis-server --requirepass ubuntu environment: - REDIS_PASSWORD=ubuntu - REDIS_REPLICATION_MODE=master ports: - &quot;6379:6379&quot; volumes: # save redisearch data to your current working directory - ./redis-data:/data command: # Save if 100 keys are added in every 10 seconds - &quot;--save 10 100&quot; # Set password - &quot;--requirepass ubuntu&quot; redisinsight: # redis db visualization dashboard container_name: redisinsight-cont image: redislabs/redisinsight ports: - 8001:8001 volumes: - redisinsight:/db volumes: redis-data: redisinsight: . The above docker-compose file has two services, redis and redisinsight. I‚Äôve set up the database with a dummy password ubuntu and made it persistent using a folder named redis-data in the current working directory. The database listens in localhost‚Äôs port 6379. You can monitor the database using redisinsight in port 8000. To spin up Redis and RedisInsight containers, run: . docker-compose up -d . This command will start the database and monitor accordingly. You can go to this localhost:8000 link using your browser and connect redisinsight to your database. After connecting your database, you should see a dashboard like this in your redisinsight panel: . . Preparing Python Environment . For local development, you can set up your python environment and install the dependencies using pip. Here, I‚Äôm on a Linux machine and using virtual environment for isolation. The following commands will work if you‚Äôre on a *nix based system and have python 3.8 installed on your system. This will install the necessary dependencies in a virtual environment: . python3.8 -m venv venv source venv/bin/activate pip install redis httpx . Workflow . Connecting Python Client to Redis . Assuming the database server is running and you‚Äôve installed the dependencies, the following snippet connects redis-py client to the database. . import redis import sys def redis_connect() -&gt; redis.client.Redis: try: client = redis.Redis( host=&quot;localhost&quot;, port=6379, password=&quot;ubuntu&quot;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(&quot;AuthenticationError&quot;) sys.exit(1) client = redis_connect() . The above excerpt tries to connect to the Redis database server using the port 6379. Notice, how I‚Äôm providing the password ubuntu via the password argument. Here, client.ping() helps you determine if a connection has been established successfully. It returns True if a successful connection can be established or raises specific errors in case of failures. The above function handles AuthenticationError and prints out an error message if the error occurs. If everything goes well, running the redis_connect() function will return an instance of the redis.client.Redis class. This instance will be used later to set and retrieve data to and from the redis database. . Getting Route Data From MapBox API . The following function strikes the MapBox Route Optimization API and collects route data. . import httpx def get_routes_from_api(coordinates: str) -&gt; dict: &quot;&quot;&quot;Data from mapbox api.&quot;&quot;&quot; with httpx.Client() as client: base_url = &quot;https://api.mapbox.com/optimized-trips/v1/mapbox/driving&quot; geometries = &quot;geojson&quot; access_token = &quot;Your-MapBox-API-token&quot; url = f&quot;{base_url}/{coordinates}?geometries={geometries}&amp;access_token={access_token}&quot; response = client.get(url) return response.json() . The above code uses Python‚Äôs httpx library to make the get request. Httpx is almost a drop-in replacement for the ubiquitous Requests library but way faster and has async support. Here, I‚Äôve used context manager httpx.Client() for better resource management while making the get request. You can read more about context managers and how to use them for hassle free resource management here. . The base_url is the base url of the route optimization API and the you‚Äôll need to provide your own access token in the access_token field. Notice, how the url variable builds up the final request url. The coordinates are provided using the lat0,lon0;lat1,lon1;lat2,lon2... format. Rest of the function sends the http requests and converts the response into a native dictionary object using the response.json() method. . Setting &amp; Retrieving Data to &amp; from Redis Database . The following two functions retrieves data from and sets data to redis database respectively. . from datetime import timedelta def get_routes_from_cache(key: str) -&gt; str: &quot;&quot;&quot;Get data from redis.&quot;&quot;&quot; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -&gt; bool: &quot;&quot;&quot;Set data to redis.&quot;&quot;&quot; state = client.setex(key, timedelta(seconds=3600), value=value, ) return state . Here, both the keys and the values are strings. In the second function, set_routes_to_cache, the client.setex() method sets a timeout of 1 hour on the key. After that the key and its associated value get deleted automatically. . The Central Orchestration . The route_optima function is the primary agent that orchestrates and executes the caching and returning of responses against requests. It roughly follows the execution flow shown below. . . When a new request arrives, the function first checks if the return-value exists in the Redis cache. If the value exists, it shows the cached value, otherwise, it sends a new request to the MapBox API, cache that value and then shows the result. . def route_optima(coordinates: str) -&gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[&quot;cache&quot;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data[&quot;code&quot;] == &quot;Ok&quot;: data[&quot;cache&quot;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data . Exposing As an API . This part of the code wraps the original Route Optimization API and exposes that as a new endpoint. I‚Äôve used fastAPI to build the wrapper API. Doing this also hides the underlying details of authentication and the actual endpoint of the MapBox API. . from fastapi import FastAPI app = FastAPI() @app.get(&quot;/route-optima/{coordinates}&quot;) def view(coordinates): &quot;&quot;&quot;This will wrap our original route optimization API and incorporate Redis Caching. You&#39;ll only expose this API to the end user. &quot;&quot;&quot; # coordinates = &quot;90.3866,23.7182;90.3742,23.7461&quot; return route_optima(coordinates) . Putting It All Together . # app.py import json import sys from datetime import timedelta import httpx import redis from fastapi import FastAPI def redis_connect() -&gt; redis.client.Redis: try: client = redis.Redis( host=&quot;localhost&quot;, port=6379, password=&quot;ubuntu&quot;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(&quot;AuthenticationError&quot;) sys.exit(1) client = redis_connect() def get_routes_from_api(coordinates: str) -&gt; dict: &quot;&quot;&quot;Data from mapbox api.&quot;&quot;&quot; with httpx.Client() as client: base_url = &quot;https://api.mapbox.com/optimized-trips/v1/mapbox/driving&quot; geometries = &quot;geojson&quot; access_token = &quot;Your-MapBox-API-token&quot; url = f&quot;{base_url}/{coordinates}?geometries={geometries}&amp;access_token={access_token}&quot; response = client.get(url) return response.json() def get_routes_from_cache(key: str) -&gt; str: &quot;&quot;&quot;Data from redis.&quot;&quot;&quot; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -&gt; bool: &quot;&quot;&quot;Data to redis.&quot;&quot;&quot; state = client.setex(key, timedelta(seconds=3600), value=value,) return state def route_optima(coordinates: str) -&gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[&quot;cache&quot;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data[&quot;code&quot;] == &quot;Ok&quot;: data[&quot;cache&quot;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data app = FastAPI() @app.get(&quot;/route-optima/{coordinates}&quot;) def view(coordinates: str) -&gt; dict: &quot;&quot;&quot;This will wrap our original route optimization API and incorporate Redis Caching. You&#39;ll only expose this API to the end user. &quot;&quot;&quot; # coordinates = &quot;90.3866,23.7182;90.3742,23.7461&quot; return route_optima(coordinates) . You can copy the complete code to a file named app.py and run the app using the command below (assuming redis, redisinsight is running and you‚Äôve installed the dependencies beforehand): . uvicorn app.app:app --host 0.0.0.0 --port 5000 --reload . This will run a local server where you can send new request with coordinates. . Go to your browser and hit the endpoint with a set of new coordinates. For example: . http://localhost:5000/route-optima/90.3866,23.7182;90.3742,23.7461 . This should return a response with the coordinates of the optimized route. . { &quot;code&quot;: &quot;Ok&quot;, &quot;waypoints&quot;: [ { &quot;distance&quot;: 26.041809241776583, &quot;name&quot;: &quot;&quot;, &quot;location&quot;: [ 90.386855, 23.718213 ], &quot;waypoint_index&quot;: 0, &quot;trips_index&quot;: 0 }, { &quot;distance&quot;: 6.286653078791968, &quot;name&quot;: &quot;&quot;, &quot;location&quot;: [ 90.374253, 23.746129 ], &quot;waypoint_index&quot;: 1, &quot;trips_index&quot;: 0 } ], &quot;trips&quot;: [ { &quot;geometry&quot;: { &quot;coordinates&quot;: [ [ 90.386855, 23.718213 ], ... ... ], &quot;type&quot;: &quot;LineString&quot; }, &quot;legs&quot;: [ { &quot;summary&quot;: &quot;&quot;, &quot;weight&quot;: 3303.1, &quot;duration&quot;: 2842.8, &quot;steps&quot;: [], &quot;distance&quot;: 5250.2 }, { &quot;summary&quot;: &quot;&quot;, &quot;weight&quot;: 2536.5, &quot;duration&quot;: 2297, &quot;steps&quot;: [], &quot;distance&quot;: 4554.8 } ], &quot;weight_name&quot;: &quot;routability&quot;, &quot;weight&quot;: 5839.6, &quot;duration&quot;: 5139.8, &quot;distance&quot;: 9805 } ], &quot;cache&quot;: false } . If you‚Äôve hit the above URL for the first time, the cache attribute of the json response should show false. This means that the response is being served from the original MapBox API. However, hitting the same URL with the same coordinates again will show the cached response and this time the cache attribute should show true. . Inspection . Once you‚Äôve got everything up and running you can inspect the cache via redis insight. To do so, go to the link below while your app server is running: . http://localhost:8000/ . Select the Browser panel from the left menu and click on a key of your cached data. It should show something like this: . . Also you can play around with the API in the swagger UI. To do so, go to the following link: . http://localhost:5000/docs . This will take you to the swagger dashboard. Here you can make requests using the interactive UI. Go ahead and inspect how the caching works for new coordinates. . . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. You can find the complete source code of the app here. . Disclaimer . This app has been made for demonstration purpose only. So it might not reflect the best practices of production ready applications. Using APIs without authentication like this is not recommended. . Resources . Http Request Caching with Redis | Httpx | Redis | RedisInsight | FastAPI | .",
            "url": "https://rednafi.github.io/digressions/python/2020/05/25/python-redis-cache.html",
            "relUrl": "/python/2020/05/25/python-redis-cache.html",
            "date": " ‚Ä¢ May 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Untangling Python Decorators",
            "content": "When I first learned about Python decorators, using them felt like doing voodoo magic. Decorators can give you the ability to add new functionalities to any callable without actually touching or changing the code inside it. This can typically yield better encapsulation and help you write cleaner and more understandable code. However, decorator is considered as a fairly advanced topic in Python since understanding and writing it requires you to have command over multiple additional concepts like first class objects, higher order functions, closures etc. First, I‚Äôll try to introduce these concepts as necessary and then unravel the core concept of decorator layer by layer. So let‚Äôs dive in. . First Class Objects . In Python, basically everything is an object and functions are regarded as first-class objects. It means that functions can be passed around and used as arguments, just like any other object (string, int, float, list, and so on). You can assign functions to variables and treat them like any other objects. Consider this example: . def func_a(): return &quot;I was angry with my friend.&quot; def func_b(): return &quot;I told my wrath, my wrath did end&quot; def func_c(*funcs): for func in funcs: print(func()) main_func = func_c main_func(func_a, func_b) . &gt;&gt;&gt; I was angry with my friend. &gt;&gt;&gt; I told my wrath, my wrath did end . The above example demonstrates how Python treats functions as first class citizens. First, I defined two functions, func_a and func_b and then func_c takes them as parameters. func_c runs the functions taken as parameters and prints the results. Then we assign func_c to variable main_func. Finally, we run main_func and it behaves just like func_c. . Higher Order Functions . Python also allows you to use functions as return values. You can take in another function and return that function or you can define a function within another function and return the inner function. . def higher(func): &quot;&quot;&quot;This is a higher order function. It returns another function. &quot;&quot;&quot; return func def lower(): return &quot;I&#39;m hunting high and low&quot; higher(lower) . &gt;&gt;&gt; &lt;function __main__.lower()&gt; . Now you can assign the result of higher to another variable and execute the output function. . h = higher(lower) h() . &gt;&gt;&gt; &quot;I&#39;m hunting high and low&quot; . Let‚Äôs look into another example where you can define a nested function within a function and return the nested function instead of its result. . def outer(): &quot;&quot;&quot;Define and return a nested function from another function.&quot;&quot;&quot; def inner(): return &quot;Hello from the inner func&quot; return inner inn = outer() inn() . &gt;&gt;&gt; &#39;Hello from the inner func&#39; . Notice how the nested function inner was defined inside the outer function and then the return statement of the outer function returned the nested function. After definition, to get to the nested function, first we called the outer function and received the result as another function. Then executing the result of the outer function prints out the message from the inner function. . Closures . You saw examples of inner functions at work in the previous section. Nested functions can access variables of the enclosing scope. In Python, these non-local variables are read only by default and we must declare them explicitly as non-local (using nonlocal keyword) in order to modify them. Following is an example of a nested function accessing a non-local variable. . def burger(name): def ingredients(): if name == &quot;deli&quot;: return (&quot;steak&quot;, &quot;pastrami&quot;, &quot;emmental&quot;) elif name == &quot;smashed&quot;: return (&quot;chicken&quot;, &quot;nacho cheese&quot;, &quot;jalapeno&quot;) else: return None return ingredients . Now run the function, . ingr = burger(&quot;deli&quot;) ingr() . &gt;&gt;&gt; (&#39;steak&#39;, &#39;pastrami&#39;, &#39;emmental&#39;) . Well, that‚Äôs unusual. . The burger function was called with the string deli and the returned function was bound to the name ingr. On calling ingr(), the message was still remembered and used to derive the outcome although the outer function burger had already finished its execution. . This technique by which some data (‚Äúdeli‚Äù) gets attached to the code is called closure in Python. The value in the enclosing scope is remembered even when the variable goes out of scope or the function itself is removed from the current namespace. Decorators uses the idea of non-local variables multiple times and soon you‚Äôll see how. . Writing a Basic Decorator . With these prerequisites out of the way, let‚Äôs go ahead and create your first simple decorator. . def deco(func): def wrapper(): print(&quot;This will get printed before the function is called.&quot;) func() print(&quot;This will get printed after the function is called.&quot;) return wrapper . Before using the decorator, let‚Äôs define a simple function without any parameters. . def ans(): print(42) . Treating the functions as first-class objects, you can use your decorator like this: . ans = deco(ans) ans() . &gt;&gt;&gt; This will get printed before the function is called. 42 This will get printed after the function is called. . In the above two lines, you can see a very simple decorator in action. Our deco function takes in a target function, manipulates the target function inside a wrapper function and then returns the wrapper function. Running the function returned by the decorator, you will get your modified result. To put it simply, decorators wraps a function and modifies its behavior. . The decorator function runs at the time the decorated function is imported/defined, not when it is called. . Before moving onto the next section, let‚Äôs see how we can get the return value of target function instead of just printing it. . def deco(func): &quot;&quot;&quot;This modified decorator also returns the result of func.&quot;&quot;&quot; def wrapper(): print(&quot;This will get printed before the function is called.&quot;) val = func() print(&quot;This will get printed after the function is called.&quot;) return val return wrapper def ans(): return 42 . In the above example, the wrapper function returns the result of the target function and the wrapper itself. This makes it possible to get the result of the modified function. . ans = deco(ans) print(ans()) . &gt;&gt;&gt; This will get printed before the function is called. This will get printed after the function is called. 42 . Can you guess why the return value of the decorated function appeared in the last line instead of in the middle like before? . The @ Syntactic Sugar . The way you‚Äôve used decorator in the last section might feel a little clunky. First, you have to type the name ans three times to call and use the decorator. Also, it becomes harder to tell apart where the decorator is actually working. So Python allows you to use decorator with the special syntax @. You can apply your decorators while defining your functions, like this: . @deco def func(): ... # Now call your decorated function just like a normal one func() . Sometimes the above syntax is called the pie syntax and it‚Äôs just a syntactic sugar for func = deco(func). . Decorating Functions with Arguments . The naive decorator that we‚Äôve implemented above will only work for functions that take no arguments. It‚Äôll fail and raise TypeError if your try to decorate a function having arguments with deco. Now let‚Äôs create another decorator called yell which will take in a function that returns a string value and transform that string value to uppercase. . def yell(func): def wrapper(*args, **kwargs): val = func(*args, **kwargs) val = val.upper() + &quot;!&quot; return val return wrapper . Create the target function that returns string value. . @yell def hello(name): return f&quot;Hello {name}&quot; . hello(&quot;redowan&quot;) . &gt;&gt;&gt; &#39;HELLO REDOWAN!&#39; . Function hello takes a name:string as parameter and returns a message as string. Look how the yell decorator is modifying the original return string, transforming that to uppercase and adding an extra ! sign without directly changing any code in the hello function. . Solving Identity Crisis . In Python, you can introspect any object and its properties via the interactive shell. A function knows its identity, docstring etc. For instance, you can inspect the built in print function in the following ways: . print . &gt;&gt;&gt; &lt;function print&gt; . print.__name__ . &gt;&gt;&gt; &#39;print&#39; . print.__doc__ . &gt;&gt;&gt; &quot;print(value, ..., sep=&#39; &#39;, end=&#39; n&#39;, file=sys.stdout, flush=False) n nPrints the values to a stream, or to sys.stdout by default. nOptional keyword arguments: nfile: a file-like object (stream); defaults to the current sys.stdout. nsep: string inserted between values, default a space. nend: string appended after the last value, default a newline. nflush: whether to forcibly flush the stream.&quot; . help(print) . &gt;&gt;&gt; Help on built-in function print in module builtins: print(...) print(value, ..., sep=&#39; &#39;, end=&#39; n&#39;, file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. . This introspection works similarly for functions that you defined yourself. I‚Äôll be using the previously defined hello function. . hello.__name__ . &gt;&gt;&gt; &#39;wrapper&#39; . help(hello) . &gt;&gt;&gt; Help on function wrapper in module __main__: wrapper(*args, **kwargs) . Now what‚Äôs going on there. The decorator yell has made the function hello confused about its own identity. Instead of reporting its own name, it takes the identity of the inner function wrapper. This can be confusing while doing debugging. You can fix this using builtin functools.wraps decorator. This will make sure that the original identity of the decorated function stays preserved. . import functools def yell(func): @functools.wraps(func) def wrapper(*args, **kwargs): val = func(*args, **kwargs) val = val.upper() + &quot;!&quot; return val return wrapper @yell def hello(name): &quot;Hello from the other side.&quot; return f&quot;Hello {name}&quot; . hello(&quot;Galaxy&quot;) . &gt;&gt;&gt; &#39;HELLO GALAXY!&#39; . Introspecting the hello function decorated with modified decorator will give you the desired result. . hello.__name__ . &gt;&gt;&gt; &#39;hello&#39; . help(hello) . &gt;&gt;&gt; Help on function hello in module __main__: hello(name) Hello from the other side. . Decorators in the Wild . Before moving on to the next section let‚Äôs see a few real world examples of decorators. To define all the decorators, we‚Äôll be using the following template that we‚Äôve perfected so far. . import functools def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): # Do something before val = func(*args, **kwargs) # Do something after return val return wrapper . Timer . Timer decorator will help you time your callables in a non-intrusive way. It can help you while debugging and profiling your functions. . import time import functools def timer(func): &quot;&quot;&quot;This decorator prints out the execution time of a callable.&quot;&quot;&quot; @functools.wraps(func) def wrapper(*args, **kwargs): start_time = time.time() val = func(*args, **kwargs) end_time = time.time() run_time = end_time - start_time print(f&quot;Finished running {func.__name__} in {run_time:.4f} seconds.&quot;) return val return wrapper @timer def dothings(n_times): for _ in range(n_times): return sum((i ** 3 for i in range(100_000))) . In the above way, we can introspect the time it requires for function dothings to complete its execution. . dothings(100_000) . &gt;&gt;&gt; Finished running dothings in 0.0231 seconds. 24999500002500000000 . Exception Logger . Just like the timer decorator, we can define a logger decorator that will log the state of a callable. For this demonstration, I‚Äôll be defining a exception logger that will show additional information like timestamp, argument names when an exception occurs inside of the decorated callable. . import functools from datetime import datetime def logexc(func): @functools.wraps(func) def wrapper(*args, **kwargs): # Stringify the arguments args_rep = [repr(arg) for arg in args] kwargs_rep = [f&quot;{k}={v!r}&quot; for k, v in kwargs.items()] sig = &quot;, &quot;.join(args_rep + kwargs_rep) # Try running the function try: return func(*args, **kwargs) except Exception as e: print(&quot;Time: &quot;, datetime.now().strftime(&quot;%Y-%m-%d [%H:%M:%S]&quot;)) print(&quot;Arguments: &quot;, sig) print(&quot;Error: n&quot;) raise return wrapper @logexc def divint(a, b): return a / b . Let‚Äôs invoke ZeroDivisionError to see the logger in action. . divint(1, 0) . &gt;&gt;&gt; Time: 2020-05-12 [12:03:31] Arguments: 1, 0 Error: ZeroDivisionError Traceback (most recent call last) .... . The decorator first prints a few info regarding the function and then raises the original error. . Validation &amp; Runtime Checks . Python‚Äôs type system is strongly typed, but very dynamic. For all its benefits, this means some bugs can try to creep in, which more statically typed languages (like Java) would catch at compile time. Looking beyond even that, you may want to enforce more sophisticated, custom checks on data going in or out. Decorators can let you easily handle all of this, and apply it to many functions at once. . Imagine this: you have a set of functions, each returning a dictionary, which (among other fields) includes a field called ‚Äúsummary.‚Äù The value of this summary must not be more than 30 characters long; if violated, that‚Äôs an error. Here is a decorator that raises a ValueError if that happens: . import functools def validate_summary(func): @functools.wraps(func) def wrapper(*args, **kwargs): data = func(*args, **kwargs) if len(data[&quot;summary&quot;]) &gt; 30: raise ValueError(&quot;Summary exceeds 30 character limit.&quot;) return data return wrapper @validate_summary def short_summary(): return {&quot;summary&quot;: &quot;This is a short summary&quot;} @validate_summary def long_summary(): return {&quot;summary&quot;: &quot;This is a long summary that exceeds character limit.&quot;} print(short_summary()) print(long_summary()) . &gt;&gt;&gt; {&#39;summary&#39;: &#39;This is a short summary&#39;} - ValueError Traceback (most recent call last) &lt;ipython-input-178-7375d8e2a623&gt; in &lt;module&gt; 19 20 print(short_summary()) &gt; 21 print(long_summary()) ... . Retry . Imagine a situation where your defined callable fails due to some I/O related issues and you‚Äôd like to retry that again. Decorator can help you to achieve that in a reusable manner. Let‚Äôs define a retry decorator that will rerun the decorated function multiple times if an http error occurs. . import functools import requests def retry(func): &quot;&quot;&quot;This will rerun the decorated callable 3 times if the callable encounters http 500/404 error.&quot;&quot;&quot; @functools.wraps(func) def wrapper(*args, **kwargs): n_tries = 3 tries = 0 while True: resp = func(*args, **kwargs) if resp.status_code == 500 or resp.status_code == 404 and tries &lt; n_tries: print(f&quot;retrying... ({tries})&quot;) tries += 1 continue break return resp return wrapper @retry def getdata(url): resp = requests.get(url) return resp resp = getdata(&quot;https://httpbin.org/get/1&quot;) resp.text . &gt;&gt;&gt; retrying... (0) retrying... (1) retrying... (2) &#39;&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt; n&lt;title&gt;404 Not Found&lt;/title&gt; n&lt;h1&gt;Not Found&lt;/h1&gt; n&lt;p&gt;The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.&lt;/p&gt; n&#39; . Applying Multiple Decorators . You can apply multiple decorators to a function by stacking them on top of each other. Let‚Äôs define two simple decorators and use them both on a function. . import functools def greet(func): &quot;&quot;&quot;Greet in English.&quot;&quot;&quot; @functools.wraps(func) def wrapper(*args, **kwargs): val = func(*args, **kwargs) return &quot;Hello &quot; + val + &quot;!&quot; return wrapper def flare(func): &quot;&quot;&quot;Add flares to the string.&quot;&quot;&quot; @functools.wraps(func) def wrapper(*args, **kwargs): val = func(*args, **kwargs) return &quot;üéâ &quot; + val + &quot; üéâ&quot; return wrapper @flare @greet def getname(name): return name getname(&quot;Nafi&quot;) . &gt;&gt;&gt; &#39;üéâ Hello Nafi! üéâ&#39; . The decorators are called in a bottom up order. First, the decorator greet gets applied on the result of getname function and then the result of greet gets passed to the flare decorator. The decorator stack above can be written as flare(greet(getname(name))). Change the order of the decorators and see what happens! . Decorators with Arguments . While defining the retry decorator in the previous section, you may have noticed that I‚Äôve hard coded the number of times I‚Äôd like the function to retry if an error occurs. It‚Äôd be handy if you could inject the number of tries as a parameter into the decorator and make it work accordingly. This is not a trivial task and you‚Äôll need three levels of nested functions to achieve that. . Before doing that let‚Äôs cook up a trivial example of how you can define decorators with parameters. . import functools def joinby(delimiter=&quot; &quot;): &quot;&quot;&quot;This decorator splits the string output of the decorated function by a single space and then joins them using a user specified delimiter.&quot;&quot;&quot; def outer_wrapper(func): @functools.wraps(func) def inner_wrapper(*args, **kwargs): val = func(*args, **kwargs) val = val.split(&quot; &quot;) val = delimiter.join(val) return val return inner_wrapper return outer_wrapper @joinby(delimiter=&quot;,&quot;) def hello(name): return f&quot;Hello {name}!&quot; @joinby(delimiter=&quot;&gt;&quot;) def greet(name): return f&quot;Greetings {name}!&quot; @joinby() def goodbye(name): return f&quot;Goodbye {name}!&quot; print(hello(&quot;Nafi&quot;)) print(greet(&quot;Redowan&quot;)) print(goodbye(&quot;Delowar&quot;)) . &gt;&gt;&gt; Hello,Nafi! Greetings&gt;Redowan! Goodbye Delowar! . The decorator joinby takes a single parameter called delimiter. It splits the string output of the decorated function by a single space and then joins them using the user defined delimiter specified in the delimiter argument. The three layer nested definition looks scary but we‚Äôll get to that in a moment. Notice how you can use the decorator with different parameters. In the above example, I‚Äôve defined three different functions to demonstrate the usage of joinby. It‚Äôs important to note that in case of a decorator that takes parameters, you‚Äôll always need to pass something to it and even if you don‚Äôt want to pass any parameter (run with the default), you‚Äôll still need to decorate your function with deco() instead of deco. Try changing the decorator on the goodbye function from joinby() to joinby and see what happens. . Typically, a decorator creates and returns an inner wrapper function but here in the repeat decorator, there is an inner function within another inner function. This almost looks like a dream within a dream from the movie Inception. . There are a few subtle things happening in the joinby() function: . Defining outer_wrapper() as an inner function means that repeat() will refer to a function object outer_wrapper. . | The delimiter argument is seemingly not used in joinby() itself. But by passing delimiter a closure is created where the value of delimiter is stored until it will be used later by inner_wrapper() . | . Decorators with &amp; without Arguments . You saw earlier that a decorator specifically designed to take parameters can‚Äôt be used without parameters; you need to at least apply parenthesis after the decorator deco() to use it without explicitly providing the arguments. But what if you want to design one that can used both with and without arguments. Let‚Äôs redefine the joinby decorator so that you can use it with parameters or just like an ordinary parameter-less decorator that we‚Äôve seen before. . import functools def joinby(_func=None, *, delimiter=&quot; &quot;): &quot;&quot;&quot;This decorator splits the string output of a function by a single space and then joins that using a user specified delimiter.&quot;&quot;&quot; def outer_wrapper(func): @functools.wraps(func) def inner_wrapper(*args, **kwargs): val = func(*args, **kwargs) val = val.split(&quot; &quot;) val = delimiter.join(val) return val return inner_wrapper # This part enables you to use the decorator with/without arguments if _func is None: return outer_wrapper else: return outer_wrapper(_func) @joinby(delimiter=&quot;,&quot;) def hello(name): return f&quot;Hello {name}!&quot; @joinby def greet(name): return f&quot;Greetings {name}!&quot; print(hello(&quot;Nafi&quot;)) print(greet(&quot;Redowan&quot;)) . &gt;&gt;&gt; Hello,Nafi! Greetings Redowan! . Here, the _func argument acts as a marker, noting whether the decorator has been called with arguments or not: . If joinby has been called without arguments, the decorated function will be passed in as _func. If it has been called with arguments, then _func will be None. The * in the argument list means that the remaining arguments can‚Äôt be called as positional arguments. This time you can use joinby with or without arguments and function hello and greet above demonstrate that. . A Generic Pattern . Personally, I find it cumbersome how you need three layers of nested functions to define a generalized decorator that can be used with or without arguments. David Beazly in his book Python Cookbook shows an excellent way to define generalized decorators without writing three levels of nested functions. It uses the built in functools.partial function to achieve that. The following is a pattern you can use to define generalized decorators in a more elegant way: . import functools def decorator(func=None, foo=&quot;spam&quot;): if func is None: return functools.partial(decorator, foo=foo) @functools.wraps(func) def wrapper(*args, **kwargs): # Do something with `func` and `foo`, if you&#39;re so inclined pass return wrapper # Applying decorator without any parameter @decorator def f(*args, **kwargs): pass # Applying decorator with extra parameter @decorator(foo=&quot;buzz&quot;) def f(*args, **kwargs): pass . Let‚Äôs redefine our retry decorator using this pattern. . import functools def retry(func=None, n_tries=4): if func is None: return functools.partial(retry, n_tries=n_tries) @functools.wraps(func) def wrapper(*args, **kwargs): tries = 0 while True: resp = func(*args, **kwargs) if resp.status_code == 500 or resp.status_code == 404 and tries &lt; n_tries: print(f&quot;retrying... ({tries})&quot;) tries += 1 continue break return resp return wrapper @retry def getdata(url): resp = requests.get(url) return resp @retry(n_tries=2) def getdata_(url): resp = requests.get(url) return resp resp1 = getdata(&quot;https://httpbin.org/get/1&quot;) print(&quot;--&quot;) resp2 = getdata_(&quot;https://httpbin.org/get/1&quot;) . &gt;&gt;&gt; retrying... (0) retrying... (1) retrying... (2) retrying... (3) -- retrying... (0) retrying... (1) . In this case, you do not have to write three level nested functions and the functools.partial takes care of that. Partials can be used to make new derived functions that have some input parameters pre-assigned.Roughly partial does the following: . def partial(func, *part_args): def wrapper(*extra_args): args = list(part_args) args.extend(extra_args) return func(*args) return wrapper . This eliminates the need to write multiple layers of nested factory function get a generalized decorator. . Defining Decorators with Classes . This time, I‚Äôll be using a class to compose a decorator. Classes can be handy to avoid nested architecture while writing decorators. Also, it can be helpful to use a class while writing stateful decorators. You can follow the pattern below to compose decorators with classes. . class ClassDeco: def __init__(self, func): functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): # You can add some code before the function call val = self.func(*args, **kwargs) # You can also add some code after the function call return val . Let‚Äôs use the above template to write a decorator named Emphasis that will add bold tags &lt;b&gt;&lt;/b&gt;to the string output of a function. . import functools class Emphasis: def __init__(self, func): functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): val = self.func(*args, **kwargs) return &quot;&lt;b&gt;&quot; + val + &quot;&lt;/b&gt;&quot; @Emphasis def hello(name): return f&quot;Hello {name}&quot; print(hello(&quot;Nafi&quot;)) print(hello(&quot;Redowan&quot;)) . &gt;&gt;&gt; &lt;b&gt;Hello Nafi&lt;/b&gt; &lt;b&gt;Hello Redowan&lt;/b&gt; . The init() method stores a reference to the function num_calls and can do other necessary initialization. The call() method will be called instead of the decorated function. It does essentially the same thing as the wrapper() function in our earlier examples. Note that you need to use the functools.update_wrapper() function instead of @functools.wraps. . Before moving on, let‚Äôs write a stateful decorator using classes. Stateful decorators can remember the state of their previous run. Here‚Äôs a stateful decorator called Tally that will keep track of the number of times decorated functions are called in a dictionary. The keys of the dictionary will hold the names of the functions and the corresponding values will hold the call count. . import functools class Tally: def __init__(self, func): functools.update_wrapper(self, func) self.func = func self.tally = {} self.n_calls = 0 def __call__(self, *args, **kwargs): self.n_calls += 1 self.tally[self.func.__name__] = self.n_calls print(&quot;Callable Tally:&quot;, self.tally) return self.func(*args, **kwargs) @Tally def hello(name): return f&quot;Hello {name}!&quot; print(hello(&quot;Redowan&quot;)) print(hello(&quot;Nafi&quot;)) . &gt;&gt;&gt; Callable Tally: {&#39;hello&#39;: 1} Hello Redowan! Callable Tally: {&#39;hello&#39;: 2} Hello Nafi! . A Few More Examples . Caching Return Values . Decorators can provide an elegant way of memoizing function return values. Imagine you have an expensive API and you‚Äôd like call that as few times as possible. The idea is to save and cache values returned by the API for particular arguments, so that if those arguments appear again, you can serve the results from the cache instead of calling the API again. This can dramatically improve your applications‚Äô performance. Here I‚Äôve simulated an expensive API call and provided caching with a decorator. . import functools import time def api(a): &quot;&quot;&quot;API takes an integer and returns the square value of it. To simulate a time consuming process, I&#39;ve added some time delay to it.&quot;&quot;&quot; print(&quot;The API has been called...&quot;) # This will delay 3 seconds time.sleep(3) return a * a api(3) . &gt;&gt;&gt; The API has been called... 9 . You‚Äôll see that running this function takes roughly 3 seconds. To cache the result , we can use Python‚Äôs built in functools.lru_cache to save the result against an argument in a dictionary and serve that when it encounters the same argument again. The only drawback here is, all the arguments need to be hashable. . import functools @functools.lru_cache(maxsize=32) def api(a): &quot;&quot;&quot;API takes an integer and returns the square value of it. To simulate a time consuming process, I&#39;ve added some time delay to it.&quot;&quot;&quot; print(&quot;The API has been called...&quot;) # This will delay 3 seconds time.sleep(3) return a * a api(3) . &gt;&gt;&gt; 9 . Least Recently Used (LRU) Cache organizes items in order of use, allowing you to quickly identify which item hasn‚Äôt been used for the longest amount of time. In the above case, the parameter max_size refers to the maximum numbers of responses to be saved up before it starts deleting the earliest ones. While you run the decorated function, you‚Äôll see first time it‚Äôll take roughly 3 seconds to return the result. But if you rerun the function again with the same parameter it‚Äôll spit the result from the cache almost instantly. . Unit Conversion . The following decorator converts length from SI units to multiple other units without polluting your target function with conversion logics. . import functools def convert(func=None, convert_to=None): &quot;&quot;&quot;This converts value from meter to others.&quot;&quot;&quot; if func is None: return functools.partial(convert, convert_to=convert_to) @functools.wraps(func) def wrapper(*args, **kwargs): print(f&quot;Conversion unit: {convert_to}&quot;) val = func(*args, **kwargs) # Adding conversion rules if convert_to is None: return val elif convert_to == &quot;km&quot;: return val / 1000 elif convert_to == &quot;mile&quot;: return val * 0.000621371 elif convert_to == &quot;cm&quot;: return val * 100 elif convert_to == &quot;mm&quot;: return val * 1000 else: raise ValueError(&quot;Conversion unit is not supported.&quot;) return wrapper . Let‚Äôs use that on a function that returns the area of a rectangle. . @convert(convert_to=&quot;mile&quot;) def area(a, b): return a * b area(1, 2) . &gt;&gt;&gt; Conversion unit: mile 0.001242742 . Using the convert decorator on the area function shows how it prints out the transformation unit before returning the desired result. Experiment with other conversion units and see what happens. . Function Registration . The following is an example of registering logger function in Flask framework. The decorator register_logger doesn‚Äôt make any change to the decorated logger function. Rather it takes the function and registers it in a list called logger_list every time it‚Äôs invoked. . from flask import Flask, request app = Flask(__name__) logger_list = [] def register_logger(func): logger_list.append(func) return func def run_loggers(request): for logger in logger_list: logger(request) @register_logger def logger(request): print(request.method, request.path) @app.route(&quot;/&quot;) def index(): run_loggers(request) return &quot;Hello World!&quot; if __name__ == &quot;__main__&quot;: app.run(host=&quot;localhost&quot;, port=&quot;5000&quot;) . If you run the server and hit the http://localhost:5000/ url, it‚Äôll greet you with a Hello World! message. Also you‚Äôll able to see the printed method and path of your http request on the terminal. Moreover, if you inspect the logger_list, you‚Äôll find the registered logger there. You‚Äôll find a lot more real life usage of decorators in the Flask framework. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. . References . Primer on Python Decorator - Real Python | Decorators in Python - DataCamp | 5 Reasons You Need to Write Python Decorators | .",
            "url": "https://rednafi.github.io/digressions/python/2020/05/13/python-decorators.html",
            "relUrl": "/python/2020/05/13/python-decorators.html",
            "date": " ‚Ä¢ May 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Migrating From Medium & Other Maladies",
            "content": "I picked up Medium primarily for dumping my thoughts and learnings immediately after discovering it back in 2016. I have published several pieces of writings on software engineering and garnered a healthy amount of audience there. Up until very recently, it has served me quite well. Medium aims to democratize online content writing by lowering the access barrier for the non-technical writers and does a fairly good job in doing so. It‚Äôs extremely easy to fire up Medium‚Äôs built-in editor and just start writing. There‚Äôs nothing to install or configure as everything is embedded into their website. The formatting tools are not fancy but adequate and the process of publishing involves merely adding a few tags and hitting the designated button. This reduction of friction usually means less focus on formatting, deployment, hosting and more focus on the actual writing. May be that‚Äôs why people outside of the tech sphere have embraced the platform wholeheartedly. Also, the platform has fantastic SEO and no matter what you are writing about, it‚Äôs almost guaranteed to gather a few eyeballs around it. There are a number of publications dedicated to different topics and they make the process of building up an audience even easier. However, the good stuffs probably end there. . Although Medium has been adopted by numerous technical writers and publications, it wasn‚Äôt necessarily built targeting this group and doesn‚Äôt consider them as first class citizens of the platform. I primarily write about Python, data science, software development and infosec in general. There‚Äôre multiple pain points that eventually forced me to steer away from Medium and look for better alternatives. First, It doesn‚Äôt support markdown syntax and the bare-bone formatting tools can get in the way of writing contents that have code snippets or require custom formatting. Secondly, there‚Äôs no built-in support for code syntax highlighting and you have to embed your code snippets as github gists. Managing and maintaining all these random gists can be a lot of work if you have many sizeable blogs that contain code snippets. Also, there‚Äôs no proper support for Latex syntax to render mathematical equations. That‚Äôs a huge deal breaker for me since many of my Data Science blogs use mathematical equations to explain disparate concepts. Usually, you get around this by converting the equations to images and embedding them in the blogs. But it completely borks mobile device readability. Another thing is that you don‚Äôt control your contents in the platform, Medium hosts and manages them for you. It can be both a blessing and a curse. Blessing because you don‚Äôt have to worry about deploying, hosting or managing your writings but at the same time you lose control over them too. Medium can censor you without you even knowing and as of writing this rant, you can only export your content to html, not to markdown format. So, if you ever think of leaving the platform, migrating can be big pain in the rear. . For me, the final nail in the coffin was their introduction of the premium tier. Until then reading and writing on Medium was, although problematic but quite manageable. Then they started blasting these premium subscription banners right at your face. This incessant pestering for subscribing turned into an unavoidable nuisance pretty quickly. Now, I don‚Äôt know about you, but most of the contents Medium displays in my feed are premium contents. Often I open an article just to arrive at a dead end that hides the rest of the content behind a paywall. Then I started paying 5$ per month to get rid of these, only to be disappointed by a plethora of low quality articles that were previously hidden behind the paywall. I‚Äôm all in for a subscription based business model but it seems like Medium tries its best to make you feel almost sorry for yourself if you‚Äôre not paying. . So, I finally took the step and wanted to get out of the platform even at the cost of losing a few visitors in the process and started looking for other options. I had a few almost non-negotiable requirements in my mind. . First, I needed absolute control over all of my contents, which means writing them using my favorite text editor (VSCode). Secondly, complete access to the contents in their raw format is mandatory. Markdown had to be the format of choice since it‚Äôs fast and easy to write without losing focus while tinkering with bolts and knobs of different custom menus of the in-built editor that usually these blogging platform offers. Also, Latex support and image rendering was crucial for me as several of my blogs contained charts and mathematical equations in them. It also had to be mobile friendly. Finally, the deployment and publication procedures should be almost as easy as Medium, means, I wouldn‚Äôt have to deal with the complexities that might arise while hosting and deployment when I just wanted to get the job done. . So, I started exploring multiple options. Since I primarily work with Python, I was looking for a Python based blogging framework and discovered Pelican. However, to my disappointment, almost all of the themes pelican offered were either ugly, didn‚Äôt work on phones, unmaintained or several years old. So, I shifted my focus on some of the newer frameworks like Hugo and Gatsby. Hugo is written in Go and the ecosystem is pretty darn cool. It has a huge collection of official and community built themes that work almost out of the box. Gatsby is even better if you are comfortable with Javascript and React. I picked up Hugo and started a building a brand new blog. Unfortunately, I quickly became obsessed with the process of building a new blog and began tinkering and exploring the framework a little too much. Changing the themes multiple times, customizing them with CSS, changing colors and stuff etc. I focused more on the building process than the writings itself. Then there was this messy process of deployment. Since I wanted full control over my contents, I decided to deploy the blog using Github pages. Hugo supports deployments using gh-pages but I couldn‚Äôt find any straight-forward CI formula that worked out of the box. So, I wrote something myself to automate the deployment but wasn‚Äôt very happy with the overall speed of the entire process. Dealing with deployment issues weren‚Äôt exactly the best experience when I just wanted to push my darn contents. . While I was again in search of a simple tool to make my own blog, I found this tweet where Hamel Hussain and Jeremy P Howard announced fastpages. It has pretty much everything that I want in a tool to architect my blog. The UI is super simple, it supports blogging in multiple formats, I can write and preserve my blogs as markdown files, jupyter notebooks or even as microsoft word documents. The UI also looks great on mobile devices and the it hosts the blog in Github. I don‚Äôt have to get out of my text editor to write a blog and the CI works out of the box. This makes the entire hosting and deployment completely automatic and hassle free. Now I just write my contents in markdown or jupyter notebook format and push it to the master branch. The CI takes care of the rest of the things. . Fastpages is very opinionated. It exposes a few loose strands for you to customize and the philosophy here is to bootstrap you with a minimum set of toolkit required to perform the tasks at hand. Also, it‚Äôs the perfect blogging tool if you work in the data science paradigm since it has the ability to directly turn your jupyter notebooks into beautifully formatted static blogs. However, the process of migrating my previous blogs was a chore. I haven‚Äôt yet fully migrated all of the writings as Medium doesn‚Äôt export the contents to markdown format. I tried a few CLIs to convert the html documents to markdown but the results weren‚Äôt satisfactory. So copy paste is my only friend for now. However, I‚Äôm quite enjoying the process of writing new posts and publishing them with a single git push origin master. Also, my Google Analytics is already showing a pretty encouraging amount of traffic here. Not regretting the journey and definitely not going back. . I still read Medium blogs from time to time, especially when the contents come from someone I already know. However, my days of writing anything there is over. Adios, Medium, It‚Äôs been a pleasure! .",
            "url": "https://rednafi.github.io/digressions/meta/2020/04/24/leaving-medium.html",
            "relUrl": "/meta/2020/04/24/leaving-medium.html",
            "date": " ‚Ä¢ Apr 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Effortless Concurrency with Python's concurrent.futures",
            "content": "Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all these icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect: . In Python, if the task at hand is I/O bound, you can use use standard library‚Äôs threading module or if the task is CPU bound then multiprocessing module can be your friend. These threading and multiprocessing APIs give you a lot of control and flexibility but they come at the cost of having to write relatively low-level verbose code that adds extra layers of complexity on top of your core logic. Sometimes when the target task is complicated, it‚Äôs often impossible to avoid complexity while adding concurrency. However, a lot of simpler tasks can be made concurrent without adding too much extra overhead. . Python standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for providing the developers a high-level interface to launch asynchronous tasks. It‚Äôs a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It‚Äôs the perfect tool when you just want to run a piece of eligible code concurrently and don‚Äôt need the added modularity that the threading and multiprocessing APIs expose. . Anatomy of concurrent.futures . From the official docs, . The concurrent.futures module provides a high-level interface for asynchronously executing callables. . What it means is you can run your subroutines asynchronously using either threads or processes through a common high-level interface. Basically, the module provides an abstract class called Executor. You can‚Äôt instantiate it directly, rather you need to use one of two subclasses that it provides to run your tasks. . Executor (Abstract Base Class) ‚îÇ ‚îú‚îÄ‚îÄ ThreadPoolExecutor ‚îÇ ‚îÇ ‚îÇA concrete subclass of the Executor class to ‚îÇ ‚îÇmanage I/O bound tasks with threading underneath ‚îÇ ‚îú‚îÄ‚îÄ ProcessPoolExecutor ‚îÇ ‚îÇ ‚îÇA concrete subclass of the Executor class to ‚îÇ ‚îÇmanage CPU bound tasks with multiprocessing underneath . Internally, these two classes interact with the pools and manage the workers. Futures are used for managing results computed by the workers. To use a pool of workers, an application creates an instance of the appropriate executor class and then submits them for it to run. When each task is started, a Future instance is returned. When the result of the task is needed, an application can use the Future object to block until the result is available. Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects do not need to be managed directly. . Executor Objects . Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I‚Äôll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim. . submit(fn, args, *kwargs) . Schedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable. . with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(pow, 323, 1235) print(future.result()) . map(func, *iterables, timeout=None, chunksize=1) . Similar to map(func, *iterables) except: . the iterables are collected immediately rather than lazily; | func is executed asynchronously and several calls to func may be made concurrently. . The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn‚Äôt available after timeout seconds from the original call to Executor.map(). Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time. . If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator. . When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . | . Generic Workflows for Running Tasks Concurrently . A lot of my scripts contains some variants of the following: . for task in get_tasks(): perform(task) . Here, get_tasks returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. Tasks are usually blocking callables and they run one after another, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming. . A general rule of thumb is using ThreadPoolExecutor when the tasks are primarily I/O bound like - sending multiple http requests to many urls, saving a large number of files to disk etc. ProcessPoolExecutor should be used in tasks that are primarily CPU bound like - running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc. . Running Tasks with Executor.submit . When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task) for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): print(f&quot;The outcome is {fut.result()}&quot;) . Here you start by creating an Executor, which manages all the tasks that are running ‚Äì either in separate processes or threads. Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling the executor.shutdown() method implicitly when you‚Äôre done. . In real code, you‚Äôd would need to replace the Executor with ThreadPoolExecutor or a ProcessPoolExecutor depending on the nature of the callables. Then a set comprehension has been used here to start all the tasks. The executor.submit() method schedules each task. This creates a Future object, which represents the task to be done. Once all the tasks have been scheduled, the method concurrent.futures_as_completed() is called, which yields the futures as they‚Äôre done ‚Äì that is, as each task completes. The fut.result() method gives you the return value of perform(task), or throws an exception in case of failure. . The executor.submit() method schedules the tasks asynchronously and doesn‚Äôt hold any contexts regarding the original tasks. So if you want to map the results with the original tasks, you need to track those yourself. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task): task for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): original_task = futures[fut] print(f&quot;The result of {original_task} is {fut.result()}&quot;) . Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary. . Running Tasks with Executor.map . Another way the results can be collected in the same order they‚Äôre scheduled is via using executor.map() method. . import concurrent.futures with concurrent.futures.Executor() as executor: for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())): print(f&quot;The result of {arg} is {res}&quot;) . Notice how the map function takes the entire iterable at once. It spits out the results immediately rather than lazily and in the same order they‚Äôre scheduled. If any unhandled exception occurs during the operation, it‚Äôll also be raised immediately and the execution won‚Äôt go any further. . In Python 3.5+, executor.map() receives an optional argument: chunksize. While using ProcessPoolExecutor, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . A Few Real World Examples . Before proceeding with the examples, let‚Äôs write a small decorator that‚Äôll be helpful to measure and compare the execution time between concurrent and sequential code. . import time from functools import wraps def timeit(method): @wraps(method) def wrapper(*args, **kwargs): start_time = time.time() result = method(*args, **kwargs) end_time = time.time() print(f&quot;{method.__name__} =&gt; {(end_time-start_time)*1000} ms&quot;) return result return wrapper . The decorator can be used like this: . @timeit def func(n): return list(range(n)) . This will print out the name of the method and how long it took to execute it. . Download &amp; Save Files from URLs with Multi-threading . First, let‚Äôs download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we‚Äôll be using the ThreadPoolExecutor class to carry out the operation. But before that, let‚Äôs do this sequentially first. . from pathlib import Path import urllib.request def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): return [download_one(url) for url in urls] if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 22850.6863117218 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk. The second function download_all just iterates through a sequence of URLs and applies the download_one function on each of them. The sequential code takes about 22.8 seconds to run. Now let‚Äôs see how our threaded version of the same code performs. . from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor, as_completed def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): &quot;&quot;&quot; Create a thread pool and download specified urls &quot;&quot;&quot; with ThreadPoolExecutor(max_workers=13) as executor: return executor.map(download_one, urls, timeout=60) if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 5042.651653289795 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . The concurrent version of the code takes only about 1/4 th the time of it‚Äôs sequential counterpart. Notice in this concurrent version, the download_one function is the same as before but in the download_all function, a ThreadPoolExecutor context manager wraps the executor.map() method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using 2 * multiprocessing.cpu_count() + 1. My machine has 6 physical cores with 12 threads. So 13 is the value I chose. . Note: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task. . There is one small problem with the example above. The executor.map() method returns a generator which allows to iterate through the results once ready. That means if any error occurs inside map, it‚Äôs not possible to handle that and resume the generator after the exception occurs. From PEP255: . If an unhandled exception‚Äì including, but not limited to, StopIteration ‚Äìis raised by, or passes through, a generator function, then the exception is passed on to the caller in the usual way, and subsequent attempts to resume the generator function raise StopIteration. In other words, an unhandled exception terminates a generator‚Äôs useful life. . To get around that, you can use the executor.submit() method to create futures, accumulated the futures in a list, iterate through the futures and handle the exceptions manually. See the following example: . def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): &quot;&quot;&quot; Create a thread pool and download specified urls &quot;&quot;&quot; futures_list = [] results = [] with ThreadPoolExecutor(max_workers=13) as executor: for url in urls: futures = executor.submit(download_one, url) futures_list.append(futures) for future in futures_list: try: result = future.result(timeout=60) results.append(result) except Exception: results.append(None) return results if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . The above snippet should print out similar messages as before. . Running Multiple CPU Bound Subroutines with Multi-processing . The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. Let‚Äôs run the function sequentially first. . import hashlib def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; for i in range(n): hsh = hash_one(n) return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 18317.330598831177 ms . If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. The above code takes roughly 18 seconds to run in sequential mode. Now let‚Äôs run it parallelly using ProcessPoolExecutor. . import hashlib from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; with ProcessPoolExecutor(max_workers=10) as executor: for arg, res in zip(range(n), executor.map(hash_one, range(n), chunksize=2)): pass return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 1673.842430114746 ms . If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance. As you can see the concurrent version of the above CPU intensive operation is about 11 times faster than its sequential counterpart. . Avoiding Concurrency Pitfalls . Since the concurrent.futures provides such a simple API, you might be tempted to apply concurrency to every simple tasks at hand. However, that‚Äôs not a good idea. First, the simplicity has its fair share of constraints. In this way, you can apply concurrency only to the simplest of the tasks, usually mapping a function to an iterable or running a few subroutines simultaneously. If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules. . Another pitfall of using concurrency is deadlock situations that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let‚Äôs see a slightly modified example from the official docs. . import time from concurrent.futures import ThreadPoolExecutor def wait_on_b(): time.sleep(5) print(b.result()) # b will never complete because it is waiting on a. return 5 def wait_on_a(): time.sleep(5) print(a.result()) # a will never complete because it is waiting on b. return 6 with ThreadPoolExecutor(max_workers=2) as executor: # here, the future from a depends on the future from b # and vice versa # so this is never going to be completed a = executor.submit(wait_on_b) b = executor.submit(wait_on_a) print(&quot;Result from wait_on_b&quot;, a.result()) print(&quot;Result from wait_on_a&quot;, b.result()) . In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function‚Äôs result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. Let‚Äôs explain another deadlock situation from the official docs. . from concurrent.futures import ThreadPoolExecutor def wait_on_future(): f = executor.submit(pow, 5, 2) # This will never complete because there is only one worker thread and # it is executing this function. print(f.result()) with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(wait_on_future) print(future.result()) . The above situation usually happens when a subroutine produces nested Future object and runs on a single thread. In the function wait_on_future, the executor.submit(pow, 5, 2) creates another Future object. Since I‚Äôm running the entire thing using a single thread, the internal future object is blocking the thread and the external executor.submit() method inside the context manager can not use any threads. This situation can be avoided using multiple threads but in general, this is a bad design itself. . Then there‚Äôre situations when you might be getting lower performance with concurrent code than its sequential counterpart. This could happen for multiple reasons. . Threads were used to perform CPU bound tasks | Multiprocessing were used to perform I/O bound tasks | The tasks were too trivial to justify using either threads or multiple processes | Spawning and squashing multiple threads or processes bring extra overheads. Usually threads are much faster than processes to spawn and squash. However, using the wrong type of concurrency can actually slow down your code rather than making it any performant. Below is a trivial example where both ThreadPoolExecutor and ProcessPoolExecutor perform worse than their sequential counterpart. . import math PRIMES = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): for number in PRIMES: print(f&quot;{number} is prime: {is_prime(number)}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 67.65174865722656 ms . The above examples verifies whether a number in a list is prime or not. We ran the function on 1000 numbers to determine if they‚Äôre prime or not. The sequential version took roughly 67ms to do that. However, look below where the threaded version of the same code takes more than double the time (140ms) to so the same task. . from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ThreadPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 140.17250061035156 ms . The multiprocessing version of the same code is even slower. The tasks doesn‚Äôt justify opening so many processes. . from concurrent.futures import ProcessPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ProcessPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 311.3126754760742 ms . Although intuitively, it may seem like the task of checking prime numbers should be a CPU bound operation, it‚Äôs also important to determine if the task itself is computationally heavy enough to justify spawning multiple threads or processes. Otherwise you might end up with complicated code that performs worse than the simple solutions. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. . References . concurrent.futures- the official documentation | Easy Concurrency in Python | Adventures in Python with concurrent.futures |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/21/python-concurrent-futures.html",
            "relUrl": "/python/2020/04/21/python-concurrent-futures.html",
            "date": " ‚Ä¢ Apr 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "No Really, Python's Pathlib is Great",
            "content": "When I first encountered Python‚Äôs pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when this ticket confirmed that Django was replacing os.path with pathlib, I got curious. . The os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I‚Äôve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top. . Problem with Python‚Äôs Path Handling . Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome . However, paths are not actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src. . from glob import glob import os import shutil for fname in glob(&quot;*.py&quot;): new_path = os.path.join(&quot;src&quot;, fname) shutil.copy(fname, new_path) . The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let‚Äôs have a look at a few more examples of performing the same tasks using os.path and pathlib modules. . Joining &amp; Creating New Paths . Say you want to achieve the following goals: . There is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory . | Then you want to save the absolute path of file_another.txt in a new variable. . | . Let‚Äôs see how you‚Äôd usually do this via the os module. . from os.path import abspath, dirname, join file_path = abspath(&quot;./file.txt&quot;) base_dir = dirname(file_path) file_another_path = join(base_dir, &quot;file_another.txt&quot;) . The variables file_path, base_dir, file_another_path look like this on my machine: . print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . You can use the usual string methods to transform the paths but generally, that‚Äôs not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems do not define paths in the same way. Windows uses &quot; &quot; while Mac and *nix based OSes use &quot;/&quot; as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses &quot;/&quot; operator overloading and make this a little less painful. . from pathlib import Path file_path = Path(&quot;file.txt&quot;).resolve() base_dir = file_path.parent file_another_path = base_dir / &quot;another_file.txt&quot; print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly. . Making Directories &amp; Renaming Files . Here‚Äôs a piece of code that: . Tries to make a src/stuff/ directory when it already exists | Renames a file in the src directory called .config to .stuffconfig: | . import os import os.path os.makedirs(os.path.join(&quot;src&quot;, &quot;stuff&quot;), exist_ok=True) os.rename(&quot;src/.config&quot;, &quot;src/.stuffconfig&quot;) . Here is the same thing done using the pathlib module: . from pathlib import Path Path(&quot;src/stuff&quot;).mkdir(parents=True, exist_ok=True) Path(&quot;src/.config&quot;).rename(&quot;src/.stuffconfig&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/.stuffconfig&#39;) . Notice the output where the renamed file path is printed. It‚Äôs not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably. . Listing Specific Types of Files in a Directory . Let‚Äôs say you want to recursively visit nested directories and list .py files in a directroy called source. The directory looks like this: . src/ ‚îú‚îÄ‚îÄ stuff ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îî‚îÄ‚îÄ submodule.py ‚îú‚îÄ‚îÄ .stuffconfig ‚îú‚îÄ‚îÄ somefiles.tar.gz ‚îî‚îÄ‚îÄ module.py . Usually, glob module is used to resolve this kind of situation: . from glob import glob top_level_py_files = glob(&quot;src/*.py&quot;) all_py_files = glob(&quot;src/**/*.py&quot;, recursive=True) print(top_level_py_files) print(all_py_files) . &gt;&gt;&gt; [&#39;src/module.py&#39;] &gt;&gt;&gt; [&#39;src/module.py&#39;, &#39;src/stuff/__init__.py&#39;, &#39;src/stuff/submodule.py&#39;] . The above approach works perfectly. However, if you don‚Äôt want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way: . from pathlib import Path top_level_py_files = Path(&quot;src&quot;).glob(&quot;*.py&quot;) all_py_files = Path(&quot;src&quot;).rglob(&quot;*.py&quot;) print(list(top_level_py_files)) print(list(all_py_files)) . This will also print the same as before: . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;)] &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh? . Opening Multiple Files &amp; Reading their Contents . Now let‚Äôs open the .py files and read their contents that you recursively discovered in the previous example. . from glob import glob contents = [] for fname in glob(&quot;src/**/*.py&quot;, recursive=True): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib ...&#39;] . The pathlib implementation is almost identical as above. . from pathlib import Path contents = [] for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . You can also cook up a more robust implementation with generator comprehension and context manager. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . Anatomy of the Pathlib Module . Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are: . PurePath (Useful when you want to work with windows path on a Linux machine) | PurePosixPath (Subclass of PurePath) | PureWindowsPath (Subclass of PurePath) | Path (Concrete path object, most of the time, you‚Äôll be dealing with this one) | PosixPath (Concrete posix path, subclass of Path) | WindowsPath (Concrete windows path, subclass of Path) | This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes. . . Unless you are doing cross platform path manipulation, most of the time you‚Äôll be working with the concrete Path object. So I‚Äôll focus on the methods and properties of Path class only. . Operators . Instead of using os.path.join you can use / operator to create child paths. . from pathlib import Path base_dir = Path(&quot;src&quot;) child_dir = base_dir / &quot;stuff&quot; file_path = child_dir / &quot;__init__.py&quot; print(file_path) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Attributes &amp; Methods . The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We‚Äôll linearly traverse through the tree and provide necessary examples to grasp their usage. . Path ‚îÇ ‚îú‚îÄ‚îÄ Attributes ‚îÇ ‚îú‚îÄ‚îÄ parts ‚îÇ ‚îú‚îÄ‚îÄ parent &amp; parents ‚îÇ ‚îú‚îÄ‚îÄ name ‚îÇ ‚îú‚îÄ‚îÄ suffix &amp; suffixes ‚îÇ ‚îî‚îÄ‚îÄ stem ‚îÇ ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Methods ¬†¬† ‚îú‚îÄ‚îÄ joinpath(*other) ‚îú‚îÄ‚îÄ cwd() ‚îú‚îÄ‚îÄ home() ‚îú‚îÄ‚îÄ exists() ‚îú‚îÄ‚îÄ expanduser() ‚îú‚îÄ‚îÄ glob() ‚îú‚îÄ‚îÄ rglob(pattern) ‚îú‚îÄ‚îÄ is_dir() ‚îú‚îÄ‚îÄ is_file() ‚îú‚îÄ‚îÄ is_absolute() ‚îú‚îÄ‚îÄ iterdir() ‚îú‚îÄ‚îÄ mkdir(mode=0o777, parents=False, exist_ok=False) ‚îú‚îÄ‚îÄ open(mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None) ‚îú‚îÄ‚îÄ rename(target) ‚îú‚îÄ‚îÄ replace(target) ‚îú‚îÄ‚îÄ resolve(strict=False) ‚îî‚îÄ‚îÄ rmdir() . Let‚Äôs dive into their usage one by one. For all the examples, We‚Äôll be using the previously seen directory structure. . src/ ‚îú‚îÄ‚îÄ stuff ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îî‚îÄ‚îÄ submodule.py ‚îú‚îÄ‚îÄ .stuffconfig ‚îú‚îÄ‚îÄ somefile.tar.gz ‚îî‚îÄ‚îÄ module.py . Path.parts . Returns a tuple containing individual components of a path. . from pathlib import Path file_path = Path(&quot;src/stuff/__init__.py&quot;) file_path.parts . &gt;&gt;&gt; (&#39;src&#39;, &#39;stuff&#39;, &#39;__init__.py&#39;) . Path.parents &amp; Path.parent . Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path. . file_path = Path(&quot;src/stuff/__init__.py&quot;) for parent in file_path.parents: print(parent) . &gt;&gt;&gt; src/stuff ... src ... . . file_path.parent . &gt;&gt;&gt; PosixPath(&#39;src/stuff&#39;) . Path.name . Returns the last component of a path as string. Usually used to extract file name from a path. . from pathlib import Path file_path = Path(&quot;src/module.py&quot;) file_path.name . &gt;&gt;&gt; &#39;module.py&#39; . Path.suffixes &amp; Path.suffix . Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.suffixes . &gt;&gt;&gt; [&#39;.tar&#39;, &#39;.gz&#39;] . file_path.suffix . &gt;&gt;&gt;&#39;.gz&#39; . Path.stem . Returns the final path component without the suffix. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.stem . &gt;&gt;&gt; &#39;somefile.tar&#39; . Path.is_absolute . Checks if a path is absolute or not. Return boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.is_absolute() . &gt;&gt;&gt; False . Path.joinpath(*other) . This method is used to combine multiple components into a complete path. This can be used as an alternative to &quot;/&quot; operator for joining path components. . from pathlib import Path file_path = Path(&quot;src&quot;).joinpath(&quot;stuff&quot;).joinpath(&quot;__init__.py&quot;) file_path . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Path.cwd() . Returns the current working directory. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.cwd() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo&#39;) . Path.home() . Returns home directory. . from pathlib import Path Path.home() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi&#39;) . Path.exists() . Checks if a path exists or not. Returns boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/thisisabsent.py&quot;) file_path.exists() . &gt;&gt;&gt; False . Path.expanduser() . Returns a new path with expanded ~ symbol. . from pathlib import Path file_path = Path(&quot;~/code/demo/src/stuff/somefile.tar.gz&quot;) file_path.expanduser() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/somefile.tar.gz&#39;) . Path.glob() . Globs and yields all file paths matching a specific pattern. Let‚Äôs discover all the files in src/stuff/ directory that have .py extension. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) file_paths = dir_path.glob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.rglob(pattern) . This is like Path.glob method but matches the file pattern recursively. . from pathlib import Path dir_path = Path(&quot;src&quot;) file_paths = dir_path.rglob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.is_dir() . Checks if a path points to a directory or not. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_dir() . &gt;&gt;&gt; True . Path.is_file() . Checks if a path points to a file. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_file() . &gt;&gt;&gt; False . Path.is_absolute() . Checks if a path is absolute or relative. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_absolute() . &gt;&gt;&gt; False . Path.iterdir() . When the path points to a directory, this yields the content path objects. . from pathlib import Path base_path = Path(&quot;src&quot;) contents = [content for content in base_path.iterdir()] print(contents) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff&#39;), PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/.stuffconfig&#39;)] . Path.mkdir(mode=0o777, parents=False, exist_ok=False) . Creates a new directory at this given path. . Parameters: . mode:(str) Posix permissions (mimicking the POSIX mkdir -p command) . | parents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised. . | exist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored. . | . from pathlib import Path dir_path = Path(&quot;src/other/side&quot;) dir_path.mkdir(parents=True) . Path.open(mode=‚Äôr‚Äô, buffering=-1, encoding=None, errors=None, newline=None) . This is same as the built in open function. . from pathlib import Path with Path(&quot;src/module.py&quot;) as f: contents = open(f, &quot;r&quot;) for line in contents: print(line) . &gt;&gt;&gt; from contextlib import contextmanager ... from time import time ... ..... . Path.rename(target) . Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found. . from pathlib import Path file_path = Path(&quot;src/stuff/submodule.py&quot;) file_path.rename(file_path.parent / &quot;anothermodule.py&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/anothermodule.py&#39;) . Path.replace(target) . Replaces a file or directory to the given target. Returns the new path instance. . from pathlib import Path file_path = Path(&quot;src/stuff/anothermodule.py&quot;) file_path.replace(file_path.parent / &quot;Dockerfile&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/Dockerfile&#39;) . Path.resolve(strict=False) . Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn‚Äôt exist, FileNotFoundError will be raised. . from pathlib import Path file_path = Path(&quot;src/./stuff/Dockerfile&quot;) file_path.resolve() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/Dockerfile&#39;) . Path.rmdir() . Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised. . from pathlib import Path file_path = Path(&quot;src/stuff&quot;) file_path.rmdir() . So, Should You Use It? . Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems. . The ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. . References . pathlib ‚Äî Object-oriented filesystem paths | Python 3‚Äôs pathlib Module: Taming the File System | Why you should be using pathlib |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/13/python-pathlib.html",
            "relUrl": "/python/2020/04/13/python-pathlib.html",
            "date": " ‚Ä¢ Apr 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Running Python Linters with Pre-commit Hooks",
            "content": "Pre-commit hooks can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let‚Äôs see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase. . To keep my sanity, I only use three linters in all of my python projects: . Isort: Isort is a Python utility to sort imports alphabetically, and automatically separate them by sections and type. . It parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import: . Future | Python Standard Library | Third Party | Current Python Project | Explicitly Local (. before import, as in: from . import x) | Custom Separate Sections (Defined by forced_separate list in the configuration file) | Custom Sections (Defined by sections list in configuration file) | . Inside each section, the imports are sorted alphabetically. This also automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79). . | Black: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you‚Äôre reading. . | Flake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder‚Äôs McCabe script. The combination of these three linters makes sure that your code is compliant with PEP 8 and free of some obvious code smells. . | . Installing Pre-commit . Install using pip: . pip install pre-commit . | Install via curl: . curl https://pre-commit.com/install-local.py | python - . | . Defining the Pre-commit Config File . Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m &quot;Commit message&quot;. The following example shows how black and a few other linters can be added as hooks to the config: . # .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black . Installing the Git Hook scripts . Run . pre-commit install . This will set up the git hook scripts and should show the following output in your terminal: . pre-commit installed at .git/hooks/pre-commit . Now you‚Äôll be able to implicitly or explicitly run the hooks before each commit. . Running the Hooks Against All the Files . By default, the hooks will run every time you say: . git commit -m &quot;Commit message&quot; . However, if you wish to run the hooks manually on every file, you can do so via: . pre-commit run --all-files . Running the Linters as Pre-commit Hooks . To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there‚Äôre a few minor issues that need to be taken care of. . The default line length of black formatter is 88 (you should embrace that) but flake8 caps the line at 79 characters. This raises conflict and can cause failures. . | Flake8 can be overly strict at times. You‚Äôll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors. . | . The following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts. . # .pre-commit-config.yaml # isort - repo: https://github.com/asottile/seed-isort-config rev: v1.9.3 hooks: - id: seed-isort-config - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include=&#39; .pyi?$&#39; # these folders wont be formatted by black - --exclude=&quot;&quot;&quot; .git | .__pycache__| .hg| .mypy_cache| .tox| .venv| _build| buck-out| build| dist&quot;&quot;&quot; language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - &quot;--max-line-length=88&quot; - &quot;--max-complexity=18&quot; - &quot;--select=B,C,E,F,W,T4,B9&quot; # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - &quot;--ignore=E203,E266,E501,W503,F403,F401,E402&quot; . You can add the above lines to your configuration and run . pre-commit run --all-files . This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters. .",
            "url": "https://rednafi.github.io/digressions/python/2020/04/06/python-precommit.html",
            "relUrl": "/python/2020/04/06/python-precommit.html",
            "date": " ‚Ä¢ Apr 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Generic Functions with Python's Singledispatch",
            "content": "Recently, I was refactoring a portion of a Python function that somewhat looked like this: . def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 &amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 &amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 &amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... . This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism comes into play. The idea is, you have to define a single function that will be dynamically overloaded with alternative implementations based on the type of the function arguments. . Function Overloading &amp; Generic Functions . Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. When function overloading happens based on its argument types, the resulting function is known as generic function. Let‚Äôs see how Python‚Äôs singledispatch decorator can help to design generic functions and refactor the icky code above. . Singledispatch . Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. . A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. . As PEP-443 said, singledispatch only happens based on the first argument‚Äôs type. Let‚Äôs take a look at an example to see how this works! . Example-1: Singledispatch with built-in argument type . Let‚Äôs consider the following code: . # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; def process_float(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this code will return . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let‚Äôs see how the same thing can be achieved with singledispatch. . # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(&quot;Implement process function.&quot;) @process.register(int) def sub_process(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; @process.register(float) def sub_process(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this will return the same result as before. . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . Example-2: Singledispatch with custom argument type . Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: . def process(data: dict): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return process_cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return process_dog(data) def process_cat(data: dict): # processing cat return &quot;Cat data has been processed successfully!&quot; def process_dog(data: dict): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} # using process print(process(cat_data)) print(process(dog_data)) . Running this snippet will print out: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . To refactor this with singledispatch, you can create two data types Cat and Dog.When you make Cat and Dog objects from the classes and pass them through the process function, singledispatch will take care of dispatching the appropriate implementation of that function. . from functools import singledispatch from dataclasses import dataclass @dataclass class Cat: genus: str species: str @dataclass class Dog: genus: str species: str @singledispatch def process(obj=None): raise NotImplementedError(&quot;Implement process for bucket&quot;) @process.register(Cat) def sub_process(obj): # processing cat return &quot;Cat data has been processed successfully!&quot; @process.register(Dog) def sub_process(obj): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_obj = Cat(genus=&quot;Felis&quot;, species=&quot;catus&quot;) dog_obj = Dog(genus=&quot;Canis&quot;, species=&quot;familiaris&quot;) print(process(cat_obj)) print(process(dog_obj)) . Running this will print out the same output as before: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . References . Transform a function into a single dispatch generic function | Function overloading | Parametric polymorphism |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/05/python-singledispatch.html",
            "relUrl": "/python/2020/04/05/python-singledispatch.html",
            "date": " ‚Ä¢ Apr 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "The Curious Case of Python's Context Manager",
            "content": "Python‚Äôs context managers are great for resource management and stopping the propagation of leaked abstractions. You‚Äôve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: . with open(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: . try: f = open(&quot;file.txt&quot;, &quot;wt&quot;) text = f.write(&quot;contents go here&quot;) finally: f.close() . Writing Custom Context Managers . To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let‚Äôs recreate a custom context manager that will execute the same workflow as above. . class CustomFileOpen: &quot;&quot;&quot;Custom context manager for opening files.&quot;&quot;&quot; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() . You can use the above class just like a regular context manager. . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . From Generators to Context Managers . Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: . @contextmanager def some_generator(&lt;arguments&gt;): &lt;setup&gt; try: yield &lt;value&gt; finally: &lt;cleanup&gt; . When you use the context manager with the with statement: . with some_generator(&lt;arguments&gt;) as &lt;variable&gt;: &lt;body&gt; . It roughly translates to: . &lt;setup&gt; try: &lt;variable&gt; = &lt;value&gt; &lt;body&gt; finally: &lt;cleanup&gt; . The setup code goes before the try..finally block. Notice the point where the generator yields. This is where the code block nested in the with statement gets executed. After the completion of the code block, the generator is then resumed. If an unhandled exception occurs in the block, it‚Äôs re-raised inside the generator at the point where the yield occurred and then the finally block is executed. If no unhandled exception occurs, the code gracefully proceeds to the finally block where you run your cleanup code. . Let‚Äôs implement the same CustomFileOpen context manager with contextmanager decorator. . from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): &quot;&quot;&quot;Custom context manager for opening a file.&quot;&quot;&quot; f = open(filename, method) try: yield f finally: f.close() . Now use it just like before: . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . Writing Context Managers as Decorators . You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let‚Äôs make a RunTime decorator that will be applied on a file-opening function. The decorator will: . Print a user provided description of the function | Print the time it takes to run the function | . from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): &quot;&quot;&quot;Timing decorator.&quot;&quot;&quot; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . You can use the decorator like this: . @RunTime(&quot;This function opens a file&quot;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) . Using the function like this should return: . print(custom_file_write(&quot;file.txt&quot;, &quot;wt&quot;, &quot;jello&quot;)) . This function opens a file The function took 0.0005390644073486328 seconds to run. None . You can also create the same decorator via contextlib.contextmanager decorator. . from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . Nesting Contexts . You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: . from contextlib import contextmanager @contextmanager def get_state(name): print(&quot;entering:&quot;, name) yield name print(&quot;exiting :&quot;, name) # multiple get_state can be nested like this with get_state(&quot;A&quot;) as A, get_state(&quot;B&quot;) as B, get_state(&quot;C&quot;) as C: print(&quot;inside with statement:&quot;, A, B, C) . entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A . Notice the order they‚Äôre closed. Context managers are treated as a stack, and should be exited in reverse order in which they‚Äôre entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. . Combining Multiple Context Managers . You can combine multiple context managers too. Let‚Äôs consider these two managers. . from contextlib import contextmanager @contextmanager def a(name): print(&quot;entering a:&quot;, name) yield name print(&quot;exiting a:&quot;, name) @contextmanager def b(name): print(&quot;entering b:&quot;, name) yield name print(&quot;exiting b:&quot;, name) . Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. . @contextmanager def ab(a, b): with a(&quot;A&quot;) as A, b(&quot;B&quot;) as B: yield (A, B) . This can be used as: . with ab(a, b) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: (&#39;A&#39;, &#39;B&#39;) exiting b: B exiting a: A . If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let‚Äôs rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. . from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] . with ab((a, b), (&quot;A&quot;, &quot;B&quot;)) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: [&#39;A&#39;, &#39;B&#39;] exiting b: B exiting a: A . ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let‚Äôs see, how you can do so while avoiding accidental memory leakage with robust resource management. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] . Using Context Managers to Create SQLAlchemy Session . If you are familiar with SQLALchemy, Python‚Äôs SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: . from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(&quot;sqlite://&quot;) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() . The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of committing and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: . with session_scope() as session: myobject = MyObject(&quot;foo&quot;, &quot;bar&quot;) session.add(myobject) . Abstract Away Exception Handling Monstrosity with Context Managers . This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let‚Äôs write a decorator that will handle ZeroDivisionError and TypeError simultaneously. . from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(&quot;This is a custom ZeroDivisionError message.&quot;) raise except TypeError: print(&quot;This is a custom TypeError message.&quot;) raise . Now use this in a function where these exceptions occur. . @errhandler() def div(a, b): return a // b . div(&quot;b&quot;, 0) . This is a custom TypeError message. TypeError Traceback (most recent call last) &lt;ipython-input-43-65497ed57253&gt; in &lt;module&gt; -&gt; 1 div(&#39;b&#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): &gt; 75 return func(*args, **kwds) 76 return inner 77 &lt;ipython-input-42-b7041bcaa9e6&gt; in div(a, b) 1 @errhandler() 2 def div(a, b): -&gt; 3 return a // b TypeError: unsupported operand type(s) for //: &#39;str&#39; and &#39;int&#39; . You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? . The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logics from the main method. . import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=&quot; n(asctime)s [%(levelname)s] %(message)s&quot;, handlers=[logging.FileHandler(&quot;./debug.log&quot;), logging.StreamHandler()], ) class Calculation: &quot;&quot;&quot;Dummy class for demonstrating exception decoupling with contextmanager.&quot;&quot;&quot; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f&quot;Custom handling of Zero Division Error! Printing &quot; &quot;only 2 levels of traceback..&quot; ) logging.exception(&quot;ZeroDivisionError&quot;) def main_func(self): &quot;&quot;&quot;Function that we want to save from nasty error handling logic.&quot;&quot;&quot; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) . This will return . (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 25, in errorhandler yield File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None . Persistent Parameters Across Http Requests with Context Managers . Another great use case for context managers is making parameters persistent across multiple http requests. Python‚Äôs requests library has a Session object that will let you easily achieve this. So, if you‚Äôre making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. The following example is taken directly from requests‚Äô official docs. Let‚Äôs persist some cookies across requests. . with requests.Session() as session: session.get(&quot;http://httpbin.org/cookies/set/sessioncookie/123456789&quot;) response = session.get(&quot;http://httpbin.org/cookies&quot;) print(response.text) . This should show: . { &quot;cookies&quot;: { &quot;sessioncookie&quot;: &quot;123456789&quot; } } . Remarks . All the code snippets are updated for python 3.8. To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. . Resources . Python Contextlib Documentation | Python with Context Manager - Jeff Knupp | SQLALchemy Session Creation | Scipy Lectures: Context Managers | Merging Context Managers |",
            "url": "https://rednafi.github.io/digressions/python/2020/03/26/python-contextmanager.html",
            "relUrl": "/python/2020/03/26/python-contextmanager.html",
            "date": " ‚Ä¢ Mar 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Up and Running with MySQL in Docker",
            "content": ". Setting Up . Installation . This part describes the basic installation steps of setting up MySQL 5.7 server on Ubuntu Linux using docker. . Install docker on your Linux machine. See the instruction here. . | Install docker compose via following the instructions here. . | Create another folder on your project folder and make a docker-compose.yml file. Run the following instructions one by one: . mkdir mysql_dump cd mysql_dump touch docker-compose.yml . | Open the docker-compose.yml file and copy the following lines into it. . # docker-compose version version: &quot;3.3&quot; services: # images mysql-dev: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test_db ports: - &quot;3306:3306&quot; # making data persistent volumes: - db-data:/var/lib/mysql volumes: db-data: . | . Run MySQL Server . Run the docker-compose command. This will build and run the server in detached mode. . docker compose up -d . Connect Shell to Server . Check the name of the running container with docker ps command. In this case, the running container is called mysql_dumps_mysql-dev_1. Then run the following command to connect your shell to the running server. . # connect shell to server docker exec -it mysql_dumps_mysql-dev_1 mysql -uroot -p . Alter Root Password . If you want to change the root password, enter the following command in the MySQL shell. Replace MyNewPass with your new root password: . ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;; . You should see something like this in the command prompt: . Query OK, 0 rows affected (0.02 sec) . To make the change take effect, type the following command: . FLUSH PRIVILEGES; . View Users . MySQL stores the user information in its own database. The name of the database is mysql. If you want to see what users are set up in the MySQL user table, run the following command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like this: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | ++--+-+ 4 rows in set (0.00 sec) . Create a Database . According to the docker-compose.yml file, you already have created a database named test_db. You can create anotehr database named test_db_2 via the following command: . CREATE DATABASE test_db_2; . List your databases via the following command: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | mysql | | performance_schema | | sys | | test_db | | test_db_2 | +--+ 6 rows in set (0.01 sec) . To ensure the changes: . FLUSH PRIVILEGES; . Creating Dummy Table in the Database . -- create dummy table CREATE TABLE IF NOT EXISTS `student` ( `id` int(2) NOT NULL DEFAULT &#39;0&#39;, `name` varchar(50) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `class` varchar(10) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `mark` int(3) NOT NULL DEFAULT &#39;0&#39;, `sex` varchar(6) CHARACTER SET utf8 NOT NULL DEFAULT &#39;male&#39; ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- insert data into the dummy table INSERT INTO `student` (`id`, `name`, `class`, `mark`, `sex`) VALUES (1, &#39;John Deo&#39;, &#39;Four&#39;, 75, &#39;female&#39;), (2, &#39;Max Ruin&#39;, &#39;Three&#39;, 85, &#39;male&#39;), (3, &#39;Arnold&#39;, &#39;Three&#39;, 55, &#39;male&#39;), (4, &#39;Krish Star&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (5, &#39;John Mike&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (6, &#39;Alex John&#39;, &#39;Four&#39;, 55, &#39;male&#39;), (7, &#39;My John Rob&#39;, &#39;Fifth&#39;, 78, &#39;male&#39;), (8, &#39;Asruid&#39;, &#39;Five&#39;, 85, &#39;male&#39;), (9, &#39;Tes Qry&#39;, &#39;Six&#39;, 78, &#39;male&#39;), (10, &#39;Big John&#39;, &#39;Four&#39;, 55, &#39;female&#39;), (11, &#39;Ronald&#39;, &#39;Six&#39;, 89, &#39;female&#39;), (12, &#39;Recky&#39;, &#39;Six&#39;, 94, &#39;female&#39;), (13, &#39;Kty&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (14, &#39;Bigy&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (15, &#39;Tade Row&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (16, &#39;Gimmy&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (17, &#39;Tumyu&#39;, &#39;Six&#39;, 54, &#39;male&#39;), (18, &#39;Honny&#39;, &#39;Five&#39;, 75, &#39;male&#39;), (19, &#39;Tinny&#39;, &#39;Nine&#39;, 18, &#39;male&#39;), (20, &#39;Jackly&#39;, &#39;Nine&#39;, 65, &#39;female&#39;), (21, &#39;Babby John&#39;, &#39;Four&#39;, 69, &#39;female&#39;), (22, &#39;Reggid&#39;, &#39;Seven&#39;, 55, &#39;female&#39;), (23, &#39;Herod&#39;, &#39;Eight&#39;, 79, &#39;male&#39;), (24, &#39;Tiddy Now&#39;, &#39;Seven&#39;, 78, &#39;male&#39;), (25, &#39;Giff Tow&#39;, &#39;Seven&#39;, 88, &#39;male&#39;), (26, &#39;Crelea&#39;, &#39;Seven&#39;, 79, &#39;male&#39;), (27, &#39;Big Nose&#39;, &#39;Three&#39;, 81, &#39;female&#39;), (28, &#39;Rojj Base&#39;, &#39;Seven&#39;, 86, &#39;female&#39;), (29, &#39;Tess Played&#39;, &#39;Seven&#39;, 55, &#39;male&#39;), (30, &#39;Reppy Red&#39;, &#39;Six&#39;, 79, &#39;female&#39;), (31, &#39;Marry Toeey&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (32, &#39;Binn Rott&#39;, &#39;Seven&#39;, 90, &#39;female&#39;), (33, &#39;Kenn Rein&#39;, &#39;Six&#39;, 96, &#39;female&#39;), (34, &#39;Gain Toe&#39;, &#39;Seven&#39;, 69, &#39;male&#39;), (35, &#39;Rows Noump&#39;, &#39;Six&#39;, 88, &#39;female&#39;); . Show Tables . USE test_db; SHOW tables; . Delete a Database . To delete a database test_db run the following command: . DROP DATABASE test_db, FLUSH PRIVILEGES; . Add a Database User . To create a new user (here, we created a new user named redowan with the password password), run the following command in the MySQL shell: . CREATE USER &#39;redowan&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; FlUSH PRIVILEGES; . Ensure that the changes has been saved via running FLUSH PRIVILEGES;. Verify that a user has been successfully created via running the previous command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like below. Notice that a new user named redowan has been created: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | | redowan | localhost | *0756A562377EDF6ED3AC45A00B356AAE6D3C6BB6 | ++--+-+ . Delete a Database User . To delete a database user (here, I‚Äôm deleting the user-redowan) run: . DELETE FROM mysql.user WHERE user=&#39;&lt;redowan&gt;&#39; AND host = &#39;localhost&#39; FlUSH PRIVILEGES; . Grant Database User Permissions . Give the user full permissions for your new database by running the following command (Here, I provided full permission of test_db to the user redowan: . GRANT ALL PRIVILEGES ON test_db.table TO &#39;redowan&#39;@&#39;localhost&#39;; . If you want to give permission to all the databases, type: . GRANT ALL PRIVILEGES ON *.* TO &#39;redowan&#39;@&#39;localhost&#39;; FlUSH PRIVILEGES; . Loading Sample Database to Your Own MySQL Server . To load mysqlsampledatabase.sql to your own server (In this case the user is redowan. Provide database password in the prompt), first fireup the server and type the following commands: . mysql -u redowan -p test_db &lt; mysqlsampledatabase.sql; . Now run: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | classicmodels | | mysql | | performance_schema | | sys | | test_db | +--+ 6 rows in set (0.00 sec) . Stop the Server . The following command stops the server. . docker-compose down . Notice that a new database named classicmodels has been added to the list. . Connecting to a Third Party Client . We will be using DBeaver as a third party client. While you can use the mysql shell to work on your data, a third partly client that make the experience much better with auto formatting, earsier import features, syntax highlighting etc. . Installing DBeaver . You can install DBeaver installer from here. Installation is pretty straight forward. . Connecting MySQL Database to DBeaver . Fire up DBeaver and you should be presented with this screen. Select MySQL 8+ and go next. . . The dialogue box will ask for credentials to connect to a database. In this case, I will log into previously created local database test_db with the username redowan, corresponding password password and press test connection tab. A dialogue box might pop up, prompting you download necessary drivers. . . If everything is okay, you should see a success message. You can select the SQL Editor and start writing your MySQL scripts right away. . Connecting to MySQL Server via Python . PyMySQL and DBUtils can be used to connect to MySQL Server. . import pymysql import os from dotenv import load_dotenv from DBUtils.PooledDB import PooledDB load_dotenv(verbose=True) MYSQL_REPLICA_CONFIG = { &quot;host&quot;: os.environ.get(&quot;SQL_HOST&quot;), &quot;port&quot;: int(os.environ.get(&quot;SQL_PORT&quot;)), &quot;db&quot;: os.environ.get(&quot;SQL_DB&quot;), &quot;password&quot;: os.environ.get(&quot;SQL_PASSWORD&quot;), &quot;user&quot;: os.environ.get(&quot;SQL_USER&quot;), &quot;charset&quot;: os.environ.get(&quot;SQL_CHARSET&quot;), &quot;cursorclass&quot;: pymysql.cursors.DictCursor, } # class to create database connection pooling POOL = PooledDB(**configs.MYSQL_POOL_CONFIG, **configs.MYSQL_REPLICA_CONFIG) class SqlPooled: &quot;&quot;&quot;Sql connection with pooling.&quot;&quot;&quot; def __init__(self): self._connection = POOL.connection() self._cursor = self._connection.cursor() def fetch_one(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchone() return result def fetch_all(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchall() return result def __del__(self): self._connection.close() .",
            "url": "https://rednafi.github.io/digressions/database/2020/03/15/mysql-install.html",
            "relUrl": "/database/2020/03/15/mysql-install.html",
            "date": " ‚Ä¢ Mar 15, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Reduce Boilerplate Code with Python's Dataclasses",
            "content": "Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: . class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; print(CartesianPoint(1, 2, 3)) . &gt;&gt;&gt; CartesianPoint(x = 1, y = 2, z = 3) . This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. . Dataclasses . Let‚Äôs see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) . &gt;&gt;&gt; CartesianPoint(x=1, y=2, z=3) . In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotation and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via python‚Äôs help. . help(CartesianPoint) . Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -&gt; None | | __repr__(self) | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes defined here: | | __annotations__ = {&#39;x&#39;: &lt;class &#39;float&#39;&gt;, &#39;y&#39;: &lt;class &#39;float&#39;&gt;, &#39;z&#39;: &lt;c... | | __dataclass_fields__ = {&#39;x&#39;: Field(name=&#39;x&#39;,type=&lt;class &#39;float&#39;&gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None . Using Default Values . You can provide default values to the fields in the following way: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 . Using Arbitrary Field Type . If you don‚Äôt want to specify your field type during type hinting, you can use Any type from python‚Äôs typing module. . from dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any . Instance Ordering . You can check if two instances are equal without making any modification to the class. . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) . &gt;&gt;&gt; False . However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually. . from dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 &gt; point_2) . &gt;&gt;&gt; True . By default, while comparing instances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function. . Suppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way: . from dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create intance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 &lt; point_2) . &gt;&gt;&gt; True &gt;&gt;&gt; False . You can see the above code prints out True despite the instances have different y and z attributes. . Adding Methods . Methods can be added to dataclasses just like normal classes. Let‚Äôs add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin. . from dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) # create a new instance and use method `abs_val` point = CartesianPoint(5, 6, 7) norm = point.abs_val() print(norm) . &gt;&gt;&gt; 10.488088481701515 . Making Instances Immutable . By default, instances of dataclasses are immutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass. . from dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float . If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError. . point = CartesianPoint(2, 4, 6) point.x = 23 . FrozenInstanceError Traceback (most recent call last) &lt;ipython-input-34-b712968bd0eb&gt; in &lt;module&gt; 1 point = CartesianPoint(2, 4, 6) -&gt; 2 point.x = 23 &lt;string&gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;x&#39; . Making Instances Hashable . You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perform set operation on them. However, if you are using unsafe_hash make sure that your dataclasses do not contain any mutable data structure in it. . from dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f&quot;{point}&quot;: &quot;origin&quot;}) . &gt;&gt;&gt; {&#39;CartesianPoint(x=0, y=0, z=0)&#39;: &#39;origin&#39;} . Converting Instances to Dicts . The asdict() function converts a dataclass instance to a dict of its fields. . from dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) . &gt;&gt;&gt; {&#39;x&#39;: 1, &#39;y&#39;: 5, &#39;z&#39;: 6} . Post-init Processing . When dataclass generates the __init__ method, internally it‚Äôll call _post_init__ method. You can add additional processing in the __post_init__ method. Here, I have added another attribute tup that returns the cartesian point as a tuple. . from dataclasses import dataclass @dataclass class CartesianPoint: x : float y : float z : float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) . &gt;&gt;&gt; (4, 5, 6) . Refactoring the Entire Cartesian Point Class . The feature rich original CartesianPoint looks something like this: . import math class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): &quot;&quot;&quot;Print the instance neatly.&quot;&quot;&quot; return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; def __eq__(self, other): &quot;Checks if equal.&quot; return self.x == other.x def __nq__(self, other): &quot;&quot;&quot;Checks non equality.&quot;&quot;&quot; return self.x != other.x def __gt__(self, other): &quot;&quot;&quot;Checks if greater than.&quot;&quot;&quot; return self.x &gt; other.x def __ge__(self, other): &quot;&quot;&quot;Checks if greater than or equal.&quot;&quot;&quot; return self.x &gt;= other.x def __lt__(self, other): &quot;&quot;&quot;Checks if less than.&quot;&quot;&quot; return self.x &lt; other.x def __le__(self, other): &quot;&quot;&quot;Checks if less than or equal.&quot;&quot;&quot; return self.x &lt;= other.x def __hash__(self): &quot;&quot;&quot;Make the instances hashable.&quot;&quot;&quot; return hash(self) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Let‚Äôs see the class in action: . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x = 1, y = 2, z = 3) True True False True {&#39;CartesianPoint(x = 1, y = 2, z = 3)&#39;: 3.7416573867739413} . Below is the same class refactored using dataclass. . from dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Use this class like before. . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x=1, y=2, z=3) True True False True {&#39;CartesianPoint(x=1, y=2, z=3)&#39;: 3.7416573867739413} . References . Python Dataclasses: Official Doc | The Ultimate Guide to Data Classes in Python 3.7 | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/12/python-dataclasses.html",
            "relUrl": "/python/2020/03/12/python-dataclasses.html",
            "date": " ‚Ä¢ Mar 12, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Python Virtual Environment Workflow for Sanity",
            "content": "There are multiple ways of installing Python, creating and switching between different virtual environments. Also, Python‚Äôs package manager hyperspace is a mess. So, things can quickly get out of hands while dealing with projects that require quick environment switching across multiple versions of Python. I use Debian linux in my primary development environment and this is how I keep the option explosion in check: . Installing Python . Run the following commands one by one: . # update the packages list and install the prerequisites sudo apt update sudo apt install software-properties-common # add deadsnakes ppa to your sources&#39; list (When prompted press Enter to continue) sudo add-apt-repository ppa:deadsnakes/ppa # install python3.7 sudo apt install python3.8 # verify python installation python3.8 --version . Creating Virtual Environment . There are multiple ways creating and switching between different environments can be done. I use venv for creating virtual environments. For demonstration, here I‚Äôm creating a virtual environment that uses python3.8. . Install python3-venv for creating virtual environment sudo apt install python3.8-venv . | Create virtual environment named venv in the project folder . python3.8 -m venv venv . | Activate venv . source venv/bin/activate . | Deactivate venv deactivate . | . Switching Between Different Environments . To create another environment with a different python version, you have to: . Install the desired version of python following the procedures stated above. | Install python3.7-venv specific for your python version, like if you are using python3.7, you should run: . sudo apt install python3.7-venv . | Create multiple environments with multiple versions and name them distinctively. i.e. venv3.7, venv3.8 etc. Follow the instructions above. | Activate and deactivate the desired virtual environment. | . Package Management . For local development, I use pip. | For production application and libraries poetry is preferred. | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/11/python-venv-workflow.html",
            "relUrl": "/python/2020/03/11/python-venv-workflow.html",
            "date": " ‚Ä¢ Mar 11, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "A Minimalistic Approach to ZSH",
            "content": ". Although I‚Äôm on Debian Linux, Apple‚Äôs recent announcement about replacing Bash with Zsh on MacOS made me take a look at Z-shell aka zsh. It‚Äôs a POSIX compliant Bash alternative that has been around for quite a long time. While Bash shell‚Äôs efficiency and ubiquity make it hard to think about changing the default shell of your primary development machine, I find its features as an interactive shell to be somewhat limited. So I did some digging around and soon found out that zsh‚Äôs lackluster default configurations and bloated ecosystem make it difficult for someone who just want to switch without any extra overhead. So, let‚Äôs make the process quicker. Here is what we are aiming for: . A working shell that can (almost always) take bash commands without complaining (looking at you fish) | History based autocomplete | Syntax highlighting | Git branch annotations | . Instructions were applied and tested on debian based linux (Ubuntu) . Install Z Shell . GNU/Linux . To install on a debian based linux, type: . $ apt install zsh . MacOS . Use homebrew to install zsh on MacOs. Run: . $ brew install zsh . Make Zsh as Your Default Shell . Run: . $ chsh -s $(which zsh) . Install Oh-My-Zsh Framework . Oh-My-Zsh is the tool that makes zsh so much fun and overly configurable at the same time. So we‚Äôll tread here carefully. To install oh-my-zsh , type: . $ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; . Set Firacode As the Default Terminal Font . Your selected theme may not display all the glyphs if the default terminal font doesn‚Äôt support them. Installing a font with glyphs and ligature support can solve this. I recommend installing firacode and setting that as your default terminal font. Install Fira Code From here. . Set Syntax Highlighting . Using zsh-syntax-highlighting to achieve this. . Clone this repository in oh-my-zsh‚Äôs plugins directory . $ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . | Activate the plugin in ~/.zshrc . plugins=( [plugins...] zsh-syntax-highlighting) . | Source ~/.zshrc . | . Set Suggestions . Using zsh-autosuggestions to achieve this. . Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) . $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . | Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc ) . plugins=(zsh-autosuggestions) . | Source ~/.zshrc . | . Start a new terminal session to see the effects!!! You might need to log out and log in again for the changes to be effective. . Load .profile from .zprofile . Add the following lines to ~/.zprofile and source via the command: source ~/.zprofile. Make sure you are on zsh before running the source command. . [[ -e ~/.profile ]] &amp;&amp; emulate sh -c &#39;source ~/.profile&#39; . A Barebone ~/.zshrc . Instead of adding the plugins individually, you can just install the plugins and then add this barebone config to your ~/.zshrc . Don‚Äôt forget to replace YourUserName with your username. Source your zshrc once you are done. . # ===================== # MINIMALIST ZSHRC # AUTHOR: REDNAFI # ===================== # omz path export ZSH=&quot;$HOME/.oh-my-zsh&quot; # theme settings ZSH_THEME=&quot;juanghurtado&quot; # pluging settings plugins=(git zsh-syntax-highlighting zsh-autosuggestions) # autosuggestion highlight ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=4&quot; # source omz source $ZSH/oh-my-zsh.sh #History setup HISTFILE=$HOME/.zsh_history HISTSIZE=100000 SAVEHIST=$HISTSIZ zstyle &#39;:completion:*&#39; menu select zstyle &#39;:completion:*&#39; group-name &#39;&#39; # group results by category zstyle &#39;:completion:::::&#39; completer _expand _complete _ignored _approximate #enable approximate matches for completion #disable auto correct unsetopt correct_all . Set Terminal Color (Optional) . Optionally you customize your terminal color and in this case I‚Äôve used Gogh to achieve this. . Pre Install | . $ sudo apt-get install dconf-cli uuid-runtime . Install on Linux | . $ bash -c &quot;$(wget -qO- https://git.io/vQgMr)&quot; . Install on MacOS $ bash -c &quot;$(curl -sLo- https://git.io/vQgMr)&quot; . | Put the code associated with your desired color scheme. | . Updating OMZ . $ upgrade_oh_my_zsh . Uninstall Zsh . $ sudo apt-get --purge remove zsh . Uninstall OMZ . $ uninstall_oh_my_zsh . Switch Back to Bash . $ chsh -s $(which bash) . Reference . Oh-My-Zsh | FiraCode | Gogh |",
            "url": "https://rednafi.github.io/digressions/linux/2019/10/29/minimal-zsh.html",
            "relUrl": "/linux/2019/10/29/minimal-zsh.html",
            "date": " ‚Ä¢ Oct 29, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Essential Bash Scripting",
            "content": ". Shell . Several layers of events take place whenever a Linux command is entered into the terminal. The top layer of that is known as shell. . A shell is any user interface to the UNIX operating system, i.e., any program that takes input from the user, translates it into instructions that the operating system can understand, and conveys the operating system‚Äôs output back to the user. . Let‚Äôs look at an example: . sort -n src/files/numbers.txt &gt; src/files/sorted_numbers.txt . This command will perform the following tasks: . Go to the src/files directory | Sort the numbers in the numbers.txt files in ascending order | Save the result in a new file called sorted_numbers.txt in the same directory | . History . The first major shell was the Bourne shell (named after its inventor, Steven Bourne); it was included in the first popular version of UNIX, Version 7, starting in 1979. The Bourne shell is known on the system as sh. Although UNIX has gone through many, many changes, the Bourne shell is still popular and essentially unchanged. Several UNIX utilities and administration features depend on it. . Variants of some popular shells: . C Shell or csh (The syntax has resemblance with C programming language) | Korn Shell or ksh (Similar to Bourne Shell with features from both Bourne and C Shell) | The Bourne Again Shell or BASH (Started with the GNU project in 1988.) | . BASH is going to be our primary focus here. . A Few Basic Commands . List of most frequently used commands. All of these commands can be run directly from a bash command prompt: . cd | ls | cat | cp | mv | mkdir | rm | grep | lp | . All of the following command summaries can be found via: . curl cheat.sh/&lt;prompt&gt; . cd . cd is used to change directory . #Go to the given directory cd path/to/directory #Go to home directory of current user cd #Go up to the parent of the current directory cd .. #Go to the previously chosen directory cd - . ls . ls lists all the files and folders in a user-specified directory . # Displays everything in the target directory ls path/to/the/target/directory # Displays everything including hidden files ls -a # Displays all files, along with the size (with unit suffixes) and timestamp ls -lh # Display files, sorted by size ls -S # Display directories only ls -d */ # Display directories only, include hidden ls -d .*/ */ . cat . cat shows the contents of a user-specified file . # Display the contents of a file cat /path/to/foo # Display contents with line numbers cat -n /path/to/foo # Display contents with line numbers (blank lines excluded) cat -b /path/to/foo . cp . cp copies files or folders from one directory to another . # Create a copy of a file cp ~/Desktop/foo.txt ~/Downloads/foo.txt # Create a copy of a directory cp -r ~/Desktop/cruise_pics/ ~/Pictures/ # Create a copy but ask to overwrite if the destination file already exists cp -i ~/Desktop/foo.txt ~/Documents/foo.txt # Create a backup file with date cp foo.txt{,.&quot;$(date +%Y%m%d-%H%M%S)&quot;} . mv . mv moves files or folders from one directory to another and can also be used to rename files or folders . # Move a file from one place to another mv ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another and automatically overwrite if the destination file exists # (This will override any previous -i or -n args) mv -f ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but ask before overwriting an existing file # (This will override any previous -f or -n args) mv -i ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but never overwrite anything # (This will override any previous -f or -i args) mv -n ~/Desktop/foo.txt ~/Documents/foo.txt # Move listed files to a directory mv -t ~/Desktop/ file1 file2 file3 . mkdir . mkdir is used to create a folder in a directory . # Create a directory and all its parents mkdir -p foo/bar/baz # Create foo/bar and foo/baz directories mkdir -p foo/{bar,baz} # Create the foo/bar, foo/baz, foo/baz/zip and foo/baz/zap directories mkdir -p foo/{bar,baz/{zip,zap}} . rm . rm is mainly used to delete files or folders . # Remove files and subdirs rm -rf path/to/the/target/ # Ignore non existent files rm -f path/to/the/target # Remove a file with his inode find /tmp/ -inum 6666 -exec rm -i &#39;{}&#39; ; . grep . grep can be used to search through the output of another command . # Search a file for a pattern grep pattern file # Case insensitive search (with line numbers) grep -in pattern file # Recursively grep for string &lt;pattern&gt; in folder: grep -R pattern folder # Read search patterns from a file (one per line) grep -f pattern_file file # Find lines NOT containing pattern grep -v pattern file # You can grep with regular expressions grep &quot;^00&quot; file #Match lines starting with 00 grep -E &quot;[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3}&quot; file #Find IP add # Find all files which match {pattern} in {directory} # This will show: &quot;file:line my research&quot; grep -rnw &#39;directory&#39; -e &quot;pattern&quot; # Exclude grep from your grepped output of ps. # Add [] to the first letter. Ex: sshd -&gt; [s]shd ps aux | grep &#39;[h]ttpd&#39; # Colour in red {bash} and keep all other lines ps aux | grep -E --color &#39;bash|$&#39; . lp . lp prints the specified output via an available printer . # lp # Print files. # Print the output of a command to the default printer (see `lpstat` command): echo &quot;test&quot; | lp # Print a file to the default printer: lp path/to/filename # Print a file to a named printer (see `lpstat` command): lp -d printer_name path/to/filename # Print N copies of a file to the default printer (replace N with the desired number of copies): lp -n N path/to/filename # Print only certain pages to the default printer (print pages 1, 3-5, and 16): lp -P 1,3-5,16 path/to/filename # Resume printing a job: lp -i job_id -H resume . clear . clear is used to clear the CLI window . # clear # Clears the screen of the terminal. # Clear the screen (equivalent to typing Control-L when using the bash shell): clear . exit . exit closes the CLI window . # exit # Quit the current CMD instance or the current batch file. # More information: #&lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/exit&gt;. # Quit the current CMD instance: exit # Quit the current batch script: exit /b # Quit using a specific exit code: exit exit_code . Basic Scripting Examples . When you need to execute multiple shell commands sequentially or want to do more complex stuffs, it‚Äôs better to enclose the commands in a bash script. . Running a Shell Script . Create a file with .sh extension. I have used Ubuntu‚Äôs built-in nano editor for that. . $ nano script.sh . | Put your code in the .sh file | Make sure you put the shebang #!/bin/bash at the beginning of each script, otherwise, the system wouldn‚Äôt know which interpreter to use. . | Give permission to run: . $ chmod +x script.sh . | Run the script via: . $ ./script . | If the script takes in one or multiple arguments, then place those with spaces in between. . $ ./script arg1 arg2 . | . conditionals (if-else) . Example-1: This program, Takes in two integers as arguments | Compares if one number is greater than the other or if they are equal | Returns the greater of the two numbers or if they are equal, returns equal | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number1=&quot;$1&quot; number2=&quot;$2&quot; if [ $number1 -eq $number2 ] then echo &quot;The numbers are equal&quot; elif [ $number1 -gt $number2 ] then echo &quot;The greater number is $number1&quot; elif [ $number2 -gt $number1 ] then echo &quot;The greater number is $number2&quot; fi . $ ./script.sh 12 13 The greater number is 13 . | Example-2: This program, Takes a single number as an argument | Checks whether the number is Odd or Even | Returns Odd or Even accordingly | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number=&quot;$1&quot; if [ $(( number%2 )) -eq 0 ] then echo &quot;Even&quot; else echo &quot;Odd&quot; fi . $ ./script.sh 20 Even . | Example-3: This program, Takes in two integers and an operation instruction | Returns the value according to the operation | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail echo &quot;Enter two numbers and the intended operation: * for addition, write add * for subtraction, write sub * for multiplication, write mul * for division, write div (write quit to quit the program)&quot; num1=&quot;$1&quot; num2=&quot;$2&quot; operation=&quot;$3&quot; if [ $num1 == &quot;quit&quot; ] then break elif [ $operation == &quot;add&quot; ] then ans=$(( $num1 + $num2 )) echo &quot;addition: $ans&quot; elif [ $operation == &quot;sub&quot; ] then ans=$(( $num1 - $num2 )) echo &quot;subtraction: $ans&quot; elif [ $operation == &quot;mul&quot; ] then ans=$(( $num1 * $num2 )) echo &quot;multiplication: $ans&quot; elif [ $operation == &quot;div&quot; ] then ans=$(( $num1 / $num2 )) echo &quot;division: $ans&quot; fi . $ ./script.sh 12 13 add 25 . | . for loop . Example-1: Looping through 0 to 9 with increment 3 and printing the numbers . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for var in {0..9..3} do echo $var done . $ ./script.sh 0 3 6 9 . | Example-2: Looping through files in a folder and printing them one by one . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for file in $(ls ./files) do echo $file done . $ ./script.sh numbers.txt sorted_numbers.txt . | Example-3: This program, Doesn‚Äôt take any argument | Returns the summation of all the integers, starting from 0, up to 100 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail sum=0 for num in $(seq 0 100) do sum=$(($sum + $num)) done echo &quot;Total sum is $sum&quot; . $ ./script.sh Total sum is 5050 . | Example-4: This program, Takes in an integer as an argument | Prints all the numbers up to that number, starting from 0 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail input_number=&quot;$1&quot; for num in $(seq 0 $input_number) do if [ $num -lt $input_number ] then echo $num fi done . $ ./script.sh 100 0 1 . . 99 . | . while loop . Example-1: This program, Takes in a single integer as an argument | Returns the factorial of that number | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail counter=&quot;$1&quot; factorial=1 while [ $counter -gt 0 ] do factorial=$(( $factorial * $counter )) counter=$(( $counter - 1 )) done echo &quot;Factorial of $1 is $factorial&quot; . $ ./script.sh 5 Factorial of 5 is 120 . | Example-2: This program, Takes two integers as arguments | Returns the summation of the numbers | Sending -1 as an input quits the program | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail while : do read -p &quot;Enter two numbers ( - 1 to quit ) : &quot; &quot;a&quot; &quot;b&quot; if [ $a -eq -1 ] then break fi ans=$(( $a + $b )) echo $ans done . $ ./script.sh Enter two numbers (-1 to quit): 20 30 30 . | Example-3: This program, Takes in a text filepath as argument | Reads and prints out each line of the file | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail file=&quot;$1&quot; while read -r line do echo &quot;$line&quot; done &lt; &quot;$file&quot; . $ ./script.sh files/numbers.txt 5 55 . . 11 10 . | . functions . Functions are incredible tools when we need to reuse code. Creating functions are fairly straight forward in bash. . Example-1: This function, . Takes a directory as an input argument | Counts the number of files in that directory and prints that out | Note that this function ignores the dot files (The ls -1 flag ignores dot files) | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function file_count () { ls -1 &quot;$1&quot; | wc -l } # calling the function echo $( file_count $1 ) . $ ./script.sh ./files $ 2 . | Example-2: This function, Takes in a shortcode for any of the following languages (a) en for English (b) fr for French (c) bn for bangla . | Returns a welcome message in the selected language . | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function greetings () { language=&quot;$1&quot; if [ $language == &quot;en&quot; ] then echo &quot;Greetings Mortals!&quot; elif [ $language == &quot;fr&quot; ] then echo &quot;Salutations Mortels!&quot; elif [ $language == &quot;bn&quot; ] then echo &quot;‡¶®‡¶∂‡ßç‡¶¨‡¶∞‡¶ï‡ßá ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ!&quot; fi } # calling the function echo $( greetings $1 ) . $ ./script.sh en Greetings Mortals! . | Example-3: This function, Takes a directory as an argument | Loop through the files | Only returns the text files with full path | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function return_text () { dir=&quot;$1&quot; for file in $dir&quot;/*.txt&quot; do echo &quot;$( realpath $file )&quot; done } echo &quot;$( return_text $1 )&quot; . $ ./script.sh /home/redowan/code/bash/files/numbers.txt /home/redowan/code/bash/files/sorted_numbers.txt . | . Some Good Practices . Use a Bash Strict Mode . Your bash scripts will be more robust, reliable and easy to debug if it starts with: . #!/bin/bash set -euo pipefail . This can be regarded as an unofficial bash strict mode and often prevents many classes of sneaky bugs in your script. The above command can be synthesized into multiple commands. . set -euo pipefail is short for: . set -e set -u set -o pipefail . Let‚Äôs have a look at each of them separately. . set-e: This instruction forces the bash script to exit immediately if any command has a non zero exit status. If there‚Äôs an issue in any of the lines in your code, the subsequent lines simply won‚Äôt run. . | set-u: If your code has a reference to any variable that wasn‚Äôt defined previously, this will cause the program to exit. . | set -o pipefail: This setting prevents errors in a pipeline being masked. If any command in a pipeline fails, that return code will be used as the return code of the whole pipeline, not the last command‚Äôs return code. . | . For a more in depth explanation of the different settings and Bash Strict Mode in general, check out, AAron Maxwell‚Äôs blog on this topic. . Double Quote Your Variables . It is generally a good practice to double quote your variables, specially user input variables where spaces are involved. . External Resources . Here are some awesome sources where you can always look into if you get stuck: . Command Line Crash Course | Ryans Bash Tutorial | W3 School CLI Tutorial | .",
            "url": "https://rednafi.github.io/digressions/linux/2019/09/05/essential-bash.html",
            "relUrl": "/linux/2019/09/05/essential-bash.html",
            "date": " ‚Ä¢ Sep 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Redowan Delowar is currently working as a junior Data Scientist for ShopUp (~2 years). . Redowan primarily focuses on Computational Statistics, Representation Learning and Software development. He is an avid Machine Learning evangelist, researcher, practitioner and Open Source developer. He is also available for on-site teaching, presentations and certain types of short term contract works.¬† . Contact . Gmail: redowan.nafi@gmail.com | Github: rednafi | Twitter: rednafi | LinkedIn: Redowan Delowar | .",
          "url": "https://rednafi.github.io/digressions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}