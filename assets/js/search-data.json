{
  
    
        "post0": {
            "title": "No Really, Python's Pathlib is Great",
            "content": "When I first encountered Python’s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when this ticket confirmed that Django was replacing os.path with pathlib, I got curious. . The os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I’ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top. . Problem with Python’s Path Handling . Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome . However, paths are not actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src. . from glob import glob import os import shutil for fname in glob(&quot;*.py&quot;): new_path = os.path.join(&quot;src&quot;, fname) shutil.copy(fname, new_path) . The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let’s have a look at a few more examples of performing the same tasks using os.path and pathlib modules. . Joining &amp; Creating New Paths . Say you want to achieve the following goals: . There is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory . | Then you want to save the absolute path of file_another.txt in a new variable. . | . Let’s see how you’d usually do this via the os module. . from os.path import abspath, dirname, join file_path = abspath(&quot;./file.txt&quot;) base_dir = dirname(file_path) file_another_path = join(base_dir, &quot;file_another.txt&quot;) . The variables file_path, base_dir, file_another_path look like this on my machine: . print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . You can use the usual string methods to transform the paths but generally, that’s not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems do not define paths in the same way. Windows uses &quot; &quot; while Mac and *nix based OSes use &quot;/&quot; as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses &quot;/&quot; operator overloading and make this a little less painful. . from pathlib import Path file_path = Path(&quot;file.txt&quot;).resolve() base_dir = file_path.parent file_another_path = base_dir / &quot;another_file.txt&quot; print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly. . Making Directories &amp; Renaming Files . Here’s a piece of code that: . Tries to make a src/stuff/ directory when it already exists | Renames a file in the src directory called .config to .stuffconfig: | . import os import os.path os.makedirs(os.path.join(&quot;src&quot;, &quot;stuff&quot;), exist_ok=True) os.rename(&quot;src/.config&quot;, &quot;src/.stuffconfig&quot;) . Here is the same thing done using the pathlib module: . from pathlib import Path Path(&quot;src/stuff&quot;).mkdir(parents=True, exist_ok=True) Path(&quot;src/.config&quot;).rename(&quot;src/.stuffconfig&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/.stuffconfig&#39;) . Notice the output where the renamed file path is printed. It’s not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably. . Listing Specific Types of Files in a Directory . Let’s say you want to recursively visit nested directories and list .py files in a directroy called source. The directory looks like this: . src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py . Usually, glob module is used to resolve this kind of situation: . from glob import glob top_level_py_files = glob(&quot;src/*.py&quot;) all_py_files = glob(&quot;src/**/*.py&quot;, recursive=True) print(top_level_py_files) print(all_py_files) . &gt;&gt;&gt; [&#39;src/module.py&#39;] &gt;&gt;&gt; [&#39;src/module.py&#39;, &#39;src/stuff/__init__.py&#39;, &#39;src/stuff/submodule.py&#39;] . The above approach works perfectly. However, if you don’t want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way: . from pathlib import Path top_level_py_files = Path(&quot;src&quot;).glob(&quot;*.py&quot;) all_py_files = Path(&quot;src&quot;).rglob(&quot;*.py&quot;) print(list(top_level_py_files)) print(list(all_py_files)) . This will also print the same as before: . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;)] &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh? . Opening Multiple Files &amp; Reading their Contents . Now let’s open the .py files and read their contents that you recursively discovered in the previous example. . from glob import glob contents = [] for fname in glob(&quot;src/**/*.py&quot;, recursive=True): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib ...&#39;] . The pathlib implementation is almost identical as above. . from pathlib import Path contents = [] for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . You can also cook up a more robust implementation with generator comprehension and context manager. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . Anatomy of the Pathlib Module . Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are: . PurePath (Useful when you want to work with windows path on a Linux machine) | PurePosixPath (Subclass of PurePath) | PureWindowsPath (Subclass of PurePath) | Path (Concrete path object, most of the time, you’ll be dealing with this one) | PosixPath (Concrete posix path, subclass of Path) | WindowsPath (Concrete windows path, subclass of Path) | This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes. . . Unless you are doing cross platform path manipulation, most of the time you’ll be working with the concrete Path object. So we’ll focus on the methods and properties of Path class only. . Operators . Instead of using os.path.join you can use / operator to create child paths. . from pathlib import Path base_dir = Path(&quot;src&quot;) child_dir = base_dir / &quot;stuff&quot; file_path = child_dir / &quot;__init__.py&quot; print(file_path) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Attributes &amp; Methods . The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We’ll linearly traverse through the tree and provide necessary examples to grasp their usage. . Path │ ├── Attributes │ ├── parts │ ├── parent &amp; parents │ ├── name │ ├── suffix &amp; suffixes │ └── stem │ │   └── Methods    ├── joinpath(*other) ├── cwd() ├── home() ├── exists() ├── expanduser() ├── glob() ├── rglob(pattern) ├── is_dir() ├── is_file() ├── is_absolute() ├── iterdir() ├── mkdir(mode=0o777, parents=False, exist_ok=False) ├── open(mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None) ├── rename(target) ├── replace(target) ├── resolve(strict=False) └── rmdir() . Let’s dive into their usage one by one. For all the examples, We’ll be using the previously seen directory structure. . src/ ├── stuff │ ├── __init__.py │ └── module.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py . Path.parts . Returns a tuple containing individual components of a path. . from pathlib import Path file_path = Path(&quot;src/stuff/__init__.py&quot;) file_path.parts . &gt;&gt;&gt; (&#39;src&#39;, &#39;stuff&#39;, &#39;__init__.py&#39;) . Path.parents &amp; Path.parent . Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path. . file_path = Path(&quot;src/stuff/__init__.py&quot;) for parent in file_path.parents: print(parent) . &gt;&gt;&gt; src/stuff ... src ... . . file_path.parent . &gt;&gt;&gt; PosixPath(&#39;src/stuff&#39;) . Path.name . Returns the last component of a path as string. Usually used to extract file name from a path. . from pathlib import Path file_path = Path(&quot;src/things.py&quot;) file_path.name . &gt;&gt;&gt; &#39;things.py&#39; . Path.suffixes &amp; Path.suffix . Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.suffixes . &gt;&gt;&gt; [&#39;.tar&#39;, &#39;.gz&#39;] . file_path.suffix . &gt;&gt;&gt;&#39;.gz&#39; . Path.stem . Returns the final path component without the suffix. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.stem . &gt;&gt;&gt; &#39;somefile.tar&#39; . Path.is_absolute . Checks if a path is absolute or not. Return boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.is_absolute() . &gt;&gt;&gt; False . Path.joinpath(*other) . This method is used to combine multiple components into a complete path. This can be used as an alternative to &quot;/&quot; operator for joining path components. . from pathlib import Path file_path = Path(&quot;src&quot;).joinpath(&quot;stuff&quot;).joinpath(&quot;__init__.py&quot;) file_path . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Path.cwd() . Returns the current working directory. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.cwd() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo&#39;) . Path.home() . Returns home directory. . from pathlib import Path Path.home() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi&#39;) . Path.exists() . Checks if a path exists or not. Returns boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/thisisabsent.py&quot;) file_path.exists() . &gt;&gt;&gt; False . Path.expanduser() . Returns a new path with expanded ~ symbol. . from pathlib import Path file_path = Path(&quot;~/code/demo/src/stuff/somefile.tar.gz&quot;) file_path.expanduser() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/somefile.tar.gz&#39;) . Path.glob() . Globs and yields all file paths matching a specific pattern. Let’s discover all the files in src/stuff/ directory that have .py extension. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) file_paths = dir_path.glob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.rglob(pattern) . This is like Path.glob method but matches the file pattern recursively. . from pathlib import Path dir_path = Path(&quot;src&quot;) file_paths = dir_path.rglob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.is_dir() . Checks if a path points to a directory or not. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_dir() . &gt;&gt;&gt; True . Path.is_file() . Checks if a path points to a file. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_file() . &gt;&gt;&gt; False . Path.is_absolute() . Checks if a path is absolute or relative. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_absolute() . &gt;&gt;&gt; False . Path.iterdir() . When the path points to a directory, this yields the content path objects. . from pathlib import Path base_path = Path(&quot;src&quot;) contents = [content for content in base_path.iterdir()] print(contents) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff&#39;), PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/.stuffconfig&#39;)] . Path.mkdir(mode=0o777, parents=False, exist_ok=False) . Creates a new directory at this given path. . Parameters: . mode:(str) Posix permissions (mimicking the POSIX mkdir -p command) . | parents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised. . | exist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored. . | . from pathlib import Path dir_path = Path(&quot;src/other/side&quot;) dir_path.mkdir(parents=True) . Path.open(mode=’r’, buffering=-1, encoding=None, errors=None, newline=None) . This is same as the built in open function. . from pathlib import Path with Path(&quot;src/module.py&quot;) as f: contents = open(f, &quot;r&quot;) for line in contents: print(line) . &gt;&gt;&gt; from contextlib import contextmanager ... from time import time ... ..... . Path.rename(target) . Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found. . from pathlib import Path file_path = Path(&quot;src/stuff/submodule.py&quot;) file_path.rename(file_path.parent / &quot;anothermodule.py&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/anothermodule.py&#39;) . Path.replace(target) . Replaces a file or directory to the given target. Returns the new path instance. . from pathlib import Path file_path = Path(&quot;src/stuff/anothermodule.py&quot;) file_path.replace(file_path.parent / &quot;Dockerfile&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/Dockerfile&#39;) . Path.resolve(strict=False) . Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn’t exist, FileNotFoundError will be raised. . from pathlib import Path file_path = Path(&quot;src/./stuff/Dockerfile&quot;) file_path.resolve() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/Dockerfile&#39;) . Path.rmdir() . Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised. . from pathlib import Path file_path = Path(&quot;src/stuff&quot;) file_path.rmdir() . So, Should You Use It? . Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems. . The ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful. . References . pathlib — Object-oriented filesystem paths | Python 3’s pathlib Module: Taming the File System | Why you should be using pathlib | Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. .",
            "url": "https://rednafi.github.io/digressions/python/2020/04/13/python-pathlib.html",
            "relUrl": "/python/2020/04/13/python-pathlib.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Running Python Linters with Pre-commit Hooks",
            "content": "Pre-commit hooks can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let’s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase. . To keep my sanity, I only use three linters in all of my python projects: . Black: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you’re reading. . | Isort: Isort is a Python utility to sort imports alphabetically, and automatically separated into sections and by type. . Isort parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import: . Future | Python Standard Library | Third Party | Current Python Project | Explicitly Local (. before import, as in: from . import x) | Custom Separate Sections (Defined by forced_separate list in the configuration file) | Custom Sections (Defined by sections list in configuration file) | . Inside each section, the imports are sorted alphabetically. Isort automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79). . | Flake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder’s McCabe script. The combination of these three linters makes sure that your code is compliant with PEP 8 and free of some obvious code smells. . | . Installing Pre-commit . Install using pip: . pip install pre-commit . | Install via curl: . curl https://pre-commit.com/install-local.py | python - . | . Defining the Pre-commit Config File . Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m &quot;Commit message&quot;. The following example shows how black and a few other linters can be added as hooks to the config: . # .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black . Installing the Git Hook scripts . Run . pre-commit install . This will set up the git hook scripts and should show the following output in your terminal: . pre-commit installed at .git/hooks/pre-commit . Now you will be able to implicitly or explicitly run the hooks before each commit. . Running the Hooks Against All the Files . By default, the hooks will run every time you say: . git commit -m &quot;Commit message&quot; . However, if you wish to run the hooks manually on every file, you can do so via: . pre-commit run --all-files . Running the Linters as Pre-commit Hooks . To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there are a few minor issues that need to be taken care of. . The default line length of black formatter is 88 (you should embrace that) but both flake8 and isort cap the line at 79 characters. This raises conflict and can cause failures. . | Black and isort format the imports differently. . | Flake8 can be overly strict at times. You’ll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors. . | . The following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts. . # .pre-commit-config.yaml # isort - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort args: # arguments to configure isort # making isort line length compatible with black - --line_length 88 - --use_parentheses True - --include_trailing_comma True - --multi_line_output 3 # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include=&#39; .pyi?$&#39; # these folders wont be formatted by black - --exclude=&quot;&quot;&quot; .git | .__pycache__| .hg| .mypy_cache| .tox| .venv| _build| buck-out| build| dist&quot;&quot;&quot; language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - &quot;--max-line-length=88&quot; - &quot;--max-complexity=18&quot; - &quot;--select=B,C,E,F,W,T4,B9&quot; # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - &quot;--ignore=E203,E266,E501,W503,F403,F401,E402&quot; . You can add the above lines to your configuration and run . pre-commit run --all-files . This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters. .",
            "url": "https://rednafi.github.io/digressions/python/2020/04/06/python-precommit.html",
            "relUrl": "/python/2020/04/06/python-precommit.html",
            "date": " • Apr 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Generic Functions with Python's Singledispatch",
            "content": "Recently, I was refactoring a portion of a Python function that somewhat looked like this: . def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 &amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 &amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 &amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... . This pattern gets tedious when the number of conditions and actionable functions start to grow. I I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism comes into play. The idea is, you have to define a single function that will be dynamically overloaded with alternative implementations based on the type of the function arguments. . Function Overloading &amp; Generic Functions . Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. When function overloading happens based on its argument types, the resulting function is known as generic function. Let’s see how Python’s singledispatch decorator can help to design generic functions and refactor the icky code above. . Singledispatch . Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. . A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. . As PEP-443 said, singledispatch only happens based on the first argument’s type. Let’s take a look at an example to see how this works! . Example-1: Singledispatch with built-in argument type . Let’s consider the following code: . # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; def process_float(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this code will return . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let’s see how the same thing can be achieved with singledispatch. . # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(&quot;Implement process function.&quot;) @process.register(int) def sub_process(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; @process.register(float) def sub_process(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this will return the same result as before. . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . Example-2: Singledispatch with custom argument type . Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: . def process(data: dict): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return process_cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return process_dog(data) def process_cat(data: dict): # processing cat return &quot;Cat data has been processed successfully!&quot; def process_dog(data: dict): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} # using process print(process(cat_data)) print(process(dog_data)) . Running this snippet will print out: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . To refactor this with singledispatch, you can create two data types Cat and Dog. A class_factory function will determine data type based on condition and singledispatch will take care of dispatching the appropriate implementation of the process function. . from functools import singledispatch from dataclasses import dataclass @dataclass class Cat: data: dict @dataclass class Dog: data: dict def class_factory(data): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return Cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return Dog(data) @singledispatch def process(obj=None): raise NotImplementedError(&quot;Implement process for bucket&quot;) @process.register(Cat) def sub_process(obj): # processing cat return &quot;Cat data has been processed successfully!&quot; @process.register(Dog) def sub_process(obj): return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} cat_obj = class_factory(cat_data) dog_obj = class_factory(dog_data) print(process(cat_obj)) print(process(dog_obj)) . Running this will print out the same output as before: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . References . Transform a function into a single dispatch generic function | Function overloading | Parametric polymorphism |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/05/python-singledispatch.html",
            "relUrl": "/python/2020/04/05/python-singledispatch.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The Curious Case of Python's Context Manager",
            "content": "Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: . with open(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: . try: f = open(&quot;file.txt&quot;, &quot;wt&quot;) text = f.write(&quot;contents go here&quot;) finally: f.close() . Writing Custom Context Managers . To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above. . class CustomFileOpen: &quot;&quot;&quot;Custom context manager for opening files.&quot;&quot;&quot; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() . You can use the above class just like a regular context manager. . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . From Generators to Context Managers . Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: . @contextlib.contextmanager def generator_func(&lt;arguments&gt;): &lt;setup&gt; try: yield &lt;value&gt; finally: &lt;cleanup&gt; . Let’s implement the same CustomFileOpen context manager with contextmanager decorator. . from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): &quot;&quot;&quot;Custom context manager for opening a file.&quot;&quot;&quot; f = open(filename, method) try: yield f finally: f.close() . Now use it just like before: . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . Writing Context Managers as Decorators . You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let’s make a RunTime decorator that will be applied on a file-opening function. The decorator will: . Print a user provided description of the function | Print the time it takes to run the function | . from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): &quot;&quot;&quot;Timing decorator.&quot;&quot;&quot; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . You can use the decorator like this: . @RunTime(&quot;This function opens a file&quot;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) . Using the function like this should return: . print(custom_file_write(&quot;file.txt&quot;, &quot;wt&quot;, &quot;jello&quot;)) . This function opens a file The function took 0.0005390644073486328 seconds to run. None . You can also create the same decorator via contextlib.contextmanager decorator. . from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . Nesting Contexts . You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: . from contextlib import contextmanager @contextmanager def get_state(name): print(&quot;entering:&quot;, name) yield name print(&quot;exiting :&quot;, name) # multiple get_state can be nested like this with get_state(&quot;A&quot;) as A, get_state(&quot;B&quot;) as B, get_state(&quot;C&quot;) as C: print(&quot;inside with statement:&quot;, A, B, C) . entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A . Notice the order they were closed. Context managers are treated as a stack, and should be exited in reverse order in which they were entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. . Combining Multiple Context Managers . You can combine multiple context managers too. Let’s consider these two managers. . from contextlib import contextmanager @contextmanager def a(name): print(&quot;entering a:&quot;, name) yield name print(&quot;exiting a:&quot;, name) @contextmanager def b(name): print(&quot;entering b:&quot;, name) yield name print(&quot;exiting b:&quot;, name) . Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. . @contextmanager def ab(a, b): with a(&quot;A&quot;) as A, b(&quot;B&quot;) as B: yield (A, B) . This can be used as: . with ab(a, b) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: (&#39;A&#39;, &#39;B&#39;) exiting b: B exiting a: A . If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let’s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. . from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] . with ab((a, b), (&quot;A&quot;, &quot;B&quot;)) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: [&#39;A&#39;, &#39;B&#39;] exiting b: B exiting a: A . ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let’s see, how you can do so while avoiding accidental memory leakage with robust resource management. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] . Using Context Managers to Create SQLAlchemy Session . If you are familiar with SQLALchemy, Python’s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: . from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(&quot;sqlite://&quot;) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() . The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of commiting and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: . with session_scope() as session: myobject = MyObject(&quot;foo&quot;, &quot;bar&quot;) session.add(myobject) . Abstract Away Exception Handling Monstrosity with Context Managers . This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let’s write a decorator that will handle ZeroDivisionError and TypeError simultaneously. . from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(&quot;This is a custom ZeroDivisionError message.&quot;) raise except TypeError: print(&quot;This is a custom TypeError message.&quot;) raise . Now use this in a function where these exceptions occur. . @errhandler() def div(a, b): return a // b . div(&quot;b&quot;, 0) . This is a custom TypeError message. TypeError Traceback (most recent call last) &lt;ipython-input-43-65497ed57253&gt; in &lt;module&gt; -&gt; 1 div(&#39;b&#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): &gt; 75 return func(*args, **kwds) 76 return inner 77 &lt;ipython-input-42-b7041bcaa9e6&gt; in div(a, b) 1 @errhandler() 2 def div(a, b): -&gt; 3 return a // b TypeError: unsupported operand type(s) for //: &#39;str&#39; and &#39;int&#39; . You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? . The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logics from the main method. . import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=&quot; n(asctime)s [%(levelname)s] %(message)s&quot;, handlers=[logging.FileHandler(&quot;./debug.log&quot;), logging.StreamHandler()], ) class Calculation: &quot;&quot;&quot;Dummy class for demonstrating exception decoupling with contextmanager.&quot;&quot;&quot; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f&quot;Custom handling of Zero Division Error! Printing &quot; &quot;only 2 levels of traceback..&quot; ) logging.exception(&quot;ZeroDivisionError&quot;) def main_func(self): &quot;&quot;&quot;Function that we want to save from nasty error handling logic.&quot;&quot;&quot; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) . This will return . (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 25, in errorhandler yield File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None . Remarks . All the code snippets are updated for python 3.8. To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. . Resources . Python Contextlib Documentation | Python with Context Manager - Jeff Knupp | SQLALchemy Session Creation | Scipy Lectures: Context Managers | Merging Context Managers |",
            "url": "https://rednafi.github.io/digressions/python/2020/03/26/python-contextmanager.html",
            "relUrl": "/python/2020/03/26/python-contextmanager.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Up and Running with MySQL in Docker",
            "content": ". Setting Up . Installation . This part describes the basic installation steps of setting up MySQL 5.7 server on Ubuntu Linux using docker. . Install docker on your Linux machine. See the instruction here. . | Install docker compose via following the instructions here. . | Create another folder on your project folder and make a docker-compose.yml file. Run the following instructions one by one: . mkdir mysql_dump cd mysql_dump touch docker-compose.yml . | Open the docker-compose.yml file and copy the following lines into it. . # docker-compose version version: &quot;3.3&quot; services: # images mysql-dev: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test_db ports: - &quot;3306:3306&quot; # making data persistent volumes: - db-data:/var/lib/mysql volumes: db-data: . | . Run MySQL Server . Run the docker-compose command. This will build and run the server in detached mode. . docker compose up -d . Connect Shell to Server . Check the name of the running container with docker ps command. In this case, the running container is called mysql_dumps_mysql-dev_1. Then run the following command to connect your shell to the running server. . # connect shell to server docker exec -it mysql_dumps_mysql-dev_1 mysql -uroot -p . Alter Root Password . If you want to change the root password, enter the following command in the MySQL shell. Replace MyNewPass with your new root password: . ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;; . You should see something like this in the command prompt: . Query OK, 0 rows affected (0.02 sec) . To make the change take effect, type the following command: . FLUSH PRIVILEGES; . View Users . MySQL stores the user information in its own database. The name of the database is mysql. If you want to see what users are set up in the MySQL user table, run the following command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like this: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | ++--+-+ 4 rows in set (0.00 sec) . Create a Database . According to the docker-compose.yml file, you already have created a database named test_db. You can create anotehr database named test_db_2 via the following command: . CREATE DATABASE test_db_2; . List your databases via the following command: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | mysql | | performance_schema | | sys | | test_db | | test_db_2 | +--+ 6 rows in set (0.01 sec) . To ensure the changes: . FLUSH PRIVILEGES; . Creating Dummy Table in the Database . -- create dummy table CREATE TABLE IF NOT EXISTS `student` ( `id` int(2) NOT NULL DEFAULT &#39;0&#39;, `name` varchar(50) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `class` varchar(10) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `mark` int(3) NOT NULL DEFAULT &#39;0&#39;, `sex` varchar(6) CHARACTER SET utf8 NOT NULL DEFAULT &#39;male&#39; ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- insert data into the dummy table INSERT INTO `student` (`id`, `name`, `class`, `mark`, `sex`) VALUES (1, &#39;John Deo&#39;, &#39;Four&#39;, 75, &#39;female&#39;), (2, &#39;Max Ruin&#39;, &#39;Three&#39;, 85, &#39;male&#39;), (3, &#39;Arnold&#39;, &#39;Three&#39;, 55, &#39;male&#39;), (4, &#39;Krish Star&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (5, &#39;John Mike&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (6, &#39;Alex John&#39;, &#39;Four&#39;, 55, &#39;male&#39;), (7, &#39;My John Rob&#39;, &#39;Fifth&#39;, 78, &#39;male&#39;), (8, &#39;Asruid&#39;, &#39;Five&#39;, 85, &#39;male&#39;), (9, &#39;Tes Qry&#39;, &#39;Six&#39;, 78, &#39;male&#39;), (10, &#39;Big John&#39;, &#39;Four&#39;, 55, &#39;female&#39;), (11, &#39;Ronald&#39;, &#39;Six&#39;, 89, &#39;female&#39;), (12, &#39;Recky&#39;, &#39;Six&#39;, 94, &#39;female&#39;), (13, &#39;Kty&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (14, &#39;Bigy&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (15, &#39;Tade Row&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (16, &#39;Gimmy&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (17, &#39;Tumyu&#39;, &#39;Six&#39;, 54, &#39;male&#39;), (18, &#39;Honny&#39;, &#39;Five&#39;, 75, &#39;male&#39;), (19, &#39;Tinny&#39;, &#39;Nine&#39;, 18, &#39;male&#39;), (20, &#39;Jackly&#39;, &#39;Nine&#39;, 65, &#39;female&#39;), (21, &#39;Babby John&#39;, &#39;Four&#39;, 69, &#39;female&#39;), (22, &#39;Reggid&#39;, &#39;Seven&#39;, 55, &#39;female&#39;), (23, &#39;Herod&#39;, &#39;Eight&#39;, 79, &#39;male&#39;), (24, &#39;Tiddy Now&#39;, &#39;Seven&#39;, 78, &#39;male&#39;), (25, &#39;Giff Tow&#39;, &#39;Seven&#39;, 88, &#39;male&#39;), (26, &#39;Crelea&#39;, &#39;Seven&#39;, 79, &#39;male&#39;), (27, &#39;Big Nose&#39;, &#39;Three&#39;, 81, &#39;female&#39;), (28, &#39;Rojj Base&#39;, &#39;Seven&#39;, 86, &#39;female&#39;), (29, &#39;Tess Played&#39;, &#39;Seven&#39;, 55, &#39;male&#39;), (30, &#39;Reppy Red&#39;, &#39;Six&#39;, 79, &#39;female&#39;), (31, &#39;Marry Toeey&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (32, &#39;Binn Rott&#39;, &#39;Seven&#39;, 90, &#39;female&#39;), (33, &#39;Kenn Rein&#39;, &#39;Six&#39;, 96, &#39;female&#39;), (34, &#39;Gain Toe&#39;, &#39;Seven&#39;, 69, &#39;male&#39;), (35, &#39;Rows Noump&#39;, &#39;Six&#39;, 88, &#39;female&#39;); . Show Tables . USE test_db; SHOW tables; . Delete a Database . To delete a database test_db run the following command: . DROP DATABASE test_db, FLUSH PRIVILEGES; . Add a Database User . To create a new user (here, we created a new user named redowan with the password password), run the following command in the MySQL shell: . CREATE USER &#39;redowan&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; FlUSH PRIVILEGES; . Ensure that the changes has been saved via running FLUSH PRIVILEGES;. Verify that a user has been successfully created via running the previous command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like below. Notice that a new user named redowan has been created: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | | redowan | localhost | *0756A562377EDF6ED3AC45A00B356AAE6D3C6BB6 | ++--+-+ . Delete a Database User . To delete a database user (here, I’m deleting the user-redowan) run: . DELETE FROM mysql.user WHERE user=&#39;&lt;redowan&gt;&#39; AND host = &#39;localhost&#39; FlUSH PRIVILEGES; . Grant Database User Permissions . Give the user full permissions for your new database by running the following command (Here, I provided full permission of test_db to the user redowan: . GRANT ALL PRIVILEGES ON test_db.table TO &#39;redowan&#39;@&#39;localhost&#39;; . If you want to give permission to all the databases, type: . GRANT ALL PRIVILEGES ON *.* TO &#39;redowan&#39;@&#39;localhost&#39;; FlUSH PRIVILEGES; . Loading Sample Database to Your Own MySQL Server . To load mysqlsampledatabase.sql to your own server (In this case the user is redowan. Provide database password in the prompt), first fireup the server and type the following commands: . mysql -u redowan -p test_db &lt; mysqlsampledatabase.sql; . Now run: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | classicmodels | | mysql | | performance_schema | | sys | | test_db | +--+ 6 rows in set (0.00 sec) . Stop the Server . The following command stops the server. . docker-compose down . Notice that a new database named classicmodels has been added to the list. . Connecting to a Third Party Client . We will be using DBeaver as a third party client. While you can use the mysql shell to work on your data, a third partly client that make the experience much better with auto formatting, earsier import features, syntax highlighting etc. . Installing DBeaver . You can install DBeaver installer from here. Installation is pretty straight forward. . Connecting MySQL Database to DBeaver . Fire up DBeaver and you should be presented with this screen. Select MySQL 8+ and go next. . . The dialogue box will ask for credentials to connect to a database. In this case, I will log into previously created local database test_db with the username redowan, corresponding password password and press test connection tab. A dialogue box might pop up, prompting you download necessary drivers. . . If everything is okay, you should see a success message. You can select the SQL Editor and start writing your MySQL scripts right away. . Connecting to MySQL Server via Python . PyMySQL and DBUtils can be used to connect to MySQL Server. . import pymysql import os from dotenv import load_dotenv from DBUtils.PooledDB import PooledDB load_dotenv(verbose=True) MYSQL_REPLICA_CONFIG = { &quot;host&quot;: os.environ.get(&quot;SQL_HOST&quot;), &quot;port&quot;: int(os.environ.get(&quot;SQL_PORT&quot;)), &quot;db&quot;: os.environ.get(&quot;SQL_DB&quot;), &quot;password&quot;: os.environ.get(&quot;SQL_PASSWORD&quot;), &quot;user&quot;: os.environ.get(&quot;SQL_USER&quot;), &quot;charset&quot;: os.environ.get(&quot;SQL_CHARSET&quot;), &quot;cursorclass&quot;: pymysql.cursors.DictCursor, } # class to create database connection pooling POOL = PooledDB(**configs.MYSQL_POOL_CONFIG, **configs.MYSQL_REPLICA_CONFIG) class SqlPooled: &quot;&quot;&quot;Sql connection with pooling.&quot;&quot;&quot; def __init__(self): self._connection = POOL.connection() self._cursor = self._connection.cursor() def fetch_one(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchone() return result def fetch_all(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchall() return result def __del__(self): self._connection.close() .",
            "url": "https://rednafi.github.io/digressions/database/2020/03/15/mysql-install.html",
            "relUrl": "/database/2020/03/15/mysql-install.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Reduce Boilerplate Code with Python's Dataclasses",
            "content": "Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: . class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; print(CartesianPoint(1, 2, 3)) . &gt;&gt;&gt; CartesianPoint(x = 1, y = 2, z = 3) . This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. . Dataclasses . Let’s see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) . &gt;&gt;&gt; CartesianPoint(x=1, y=2, z=3) . In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotation and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via python’s help. . help(CartesianPoint) . Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -&gt; None | | __repr__(self) | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes defined here: | | __annotations__ = {&#39;x&#39;: &lt;class &#39;float&#39;&gt;, &#39;y&#39;: &lt;class &#39;float&#39;&gt;, &#39;z&#39;: &lt;c... | | __dataclass_fields__ = {&#39;x&#39;: Field(name=&#39;x&#39;,type=&lt;class &#39;float&#39;&gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None . Using Default Values . You can provide default values to the fields in the following way: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 . Using Arbitrary Field Type . If you don’t want to specify your field type during type hinting, you can use Any type from python’s typing module. . from dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any . Instance Ordering . You can check if two instances are equal without making any modification to the class. . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) . &gt;&gt;&gt; False . However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually. . from dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 &gt; point_2) . &gt;&gt;&gt; True . By default, while comparing instances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function. . Suppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way: . from dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create intance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 &lt; point_2) . &gt;&gt;&gt; True &gt;&gt;&gt; False . You can see the above code prints out True despite the instances have different y and z attributes. . Adding Methods . Methods can be added to dataclasses just like normal classes. Let’s add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin. . from dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) # create a new instance and use method `abs_val` point = CartesianPoint(5, 6, 7) norm = point.abs_val() print(norm) . &gt;&gt;&gt; 10.488088481701515 . Making Instances Immutable . By default, instances of dataclasses are immutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass. . from dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float . If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError. . point = CartesianPoint(2, 4, 6) point.x = 23 . FrozenInstanceError Traceback (most recent call last) &lt;ipython-input-34-b712968bd0eb&gt; in &lt;module&gt; 1 point = CartesianPoint(2, 4, 6) -&gt; 2 point.x = 23 &lt;string&gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;x&#39; . Making Instances Hashable . You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perform set operation on them. However, if you are using unsafe_hash make sure that your dataclasses do not contain any mutable data structure in it. . from dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f&quot;{point}&quot;: &quot;origin&quot;}) . &gt;&gt;&gt; {&#39;CartesianPoint(x=0, y=0, z=0)&#39;: &#39;origin&#39;} . Converting Instances to Dicts . The asdict() function converts a dataclass instance to a dict of its fields. . from dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) . &gt;&gt;&gt; {&#39;x&#39;: 1, &#39;y&#39;: 5, &#39;z&#39;: 6} . Post-init Processing . When dataclass generates the __init__ method, internally it will call _post_init__ method. You can add additional processing in the __post_init__ method. Here, I have added another attribute tup that returns the cartesian point as a tuple. . from dataclasses import dataclass @dataclass class CartesianPoint: x : float y : float z : float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) . &gt;&gt;&gt; (4, 5, 6) . Refactoring the Entire Cartesian Point Class . The feature rich original CartesianPoint looks something like this: . import math class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): &quot;&quot;&quot;Print the instance neatly.&quot;&quot;&quot; return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; def __eq__(self, other): &quot;Checks if equal.&quot; return self.x == other.x def __nq__(self, other): &quot;&quot;&quot;Checks non equality.&quot;&quot;&quot; return self.x != other.x def __gt__(self, other): &quot;&quot;&quot;Checks if greater than.&quot;&quot;&quot; return self.x &gt; other.x def __ge__(self, other): &quot;&quot;&quot;Checks if greater than or equal.&quot;&quot;&quot; return self.x &gt;= other.x def __lt__(self, other): &quot;&quot;&quot;Checks if less than.&quot;&quot;&quot; return self.x &lt; other.x def __le__(self, other): &quot;&quot;&quot;Checks if less than or equal.&quot;&quot;&quot; return self.x &lt;= other.x def __hash__(self): &quot;&quot;&quot;Make the instances hashable.&quot;&quot;&quot; return hash(self) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Let’s see the class in action: . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x = 1, y = 2, z = 3) True True False True {&#39;CartesianPoint(x = 1, y = 2, z = 3)&#39;: 3.7416573867739413} . Below is the same class refactored using dataclass. . from dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Use this class like before. . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x=1, y=2, z=3) True True False True {&#39;CartesianPoint(x=1, y=2, z=3)&#39;: 3.7416573867739413} . References . Python Dataclasses: Official Doc | The Ultimate Guide to Data Classes in Python 3.7 | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/12/python-dataclasses.html",
            "relUrl": "/python/2020/03/12/python-dataclasses.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Virtual Environment Workflow for Sanity",
            "content": "There are multiple ways of installing Python, creating and switching between different virtual environments. Also, Python’s package manager hyperspace is a mess. So, things can quickly get out of hands while dealing with projects that require quick environment switching across multiple versions of Python. I use Debian linux in my primary development environment and this is how I keep the option explosion in check: . Installing Python . Run the following commands one by one: . # update the packages list and install the prerequisites sudo apt update sudo apt install software-properties-common # add deadsnakes ppa to your sources&#39; list (When prompted press Enter to continue) sudo add-apt-repository ppa:deadsnakes/ppa # install python3.7 sudo apt install python3.8 # verify python installation python3.8 --version . Creating Virtual Environment . There are multiple ways creating and switching between different environments can be done. I use venv for creating virtual environments. For demonstration, here I’m creating a virtual environment that uses python3.8. . Install python3-venv for creating virtual environment sudo apt install python3.8-venv . | Create virtual environment named venv in the project folder . python3.8 -m venv venv . | Activate venv . source venv/bin/activate . | Deactivate venv deactivate . | . Switching Between Different Environments . To create another environment with a different python version, you have to: . Install the desired version of python following the procedures stated above. | Install python3.7-venv specific for your python version, like if you are using python3.7, you should run: . sudo apt install python3.7-venv . | Create multiple environments with multiple versions and name them distinctively. i.e. venv3.7, venv3.8 etc. Follow the instructions above. | Activate and deactivate the desired virtual environment. | . Package Management . For local development, I use pip. | For production application and libraries poetry is preferred. | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/11/python-venv-workflow.html",
            "relUrl": "/python/2020/03/11/python-venv-workflow.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "A Minimalistic Approach to ZSH",
            "content": ". Although I’m on Debian Linux, Apple’s recent announcement about replacing Bash with Zsh on MacOS made me take a look at Z-shell aka zsh. It’s a POSIX compliant Bash alternative that has been around for quite a long time. While Bash shell’s efficiency and ubiquity make it hard to think about changing the default shell of your primary development machine, I find its features as an interactive shell to be somewhat limited. So I did some digging around and soon found out that zsh’s lackluster default configurations and bloated ecosystem make it difficult for someone who just want to switch without any extra overhead. So, let’s make the process quicker. Here is what we are aiming for: . A working shell that can (almost always) take bash commands without complaining (looking at you fish) | History based autocomplete | Syntax highlighting | Git branch annotations | . Instructions were applied and tested on debian based linux (Ubuntu) . Install Z Shell . GNU/Linux . To install on a debian based linux, type: . $ apt install zsh . MacOS . Use homebrew to install zsh on MacOs. Run: . $ brew install zsh . Make Zsh as Your Default Shell . Run: . $ chsh -s $(which zsh) . Install Oh-My-Zsh Framework . Oh-My-Zsh is the tool that makes zsh so much fun and overly configurable at the same time. So we’ll tread here carefully. To install oh-my-zsh , type: . $ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; . Set Firacode As the Default Terminal Font . Your selected theme may not display all the glyphs if the default terminal font doesn’t support them. Installing a font with glyphs and ligature support can solve this. I recommend installing firacode and setting that as your default terminal font. Install Fira Code From here. . Set Syntax Highlighting . Using zsh-syntax-highlighting to achieve this. . Clone this repository in oh-my-zsh’s plugins directory . $ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . | Activate the plugin in ~/.zshrc . plugins=( [plugins...] zsh-syntax-highlighting) . | Source ~/.zshrc . | . Set Suggestions . Using zsh-autosuggestions to achieve this. . Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) . $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . | Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc ) . plugins=(zsh-autosuggestions) . | Source ~/.zshrc . | . Start a new terminal session to see the effects!!! You might need to log out and log in again for the changes to be effective. . Load .profile from .zprofile . Add the following lines to ~/.zprofile and source via the command: source ~/.zprofile. Make sure you are on zsh before running the source command. . [[ -e ~/.profile ]] &amp;&amp; emulate sh -c &#39;source ~/.profile&#39; . A Barebone ~/.zshrc . Instead of adding the plugins individually, you can just install the plugins and then add this barebone config to your ~/.zshrc . Don’t forget to replace YourUserName with your username. Source your zshrc once you are done. . # ===================== # MINIMALIST ZSHRC # AUTHOR: REDNAFI # ===================== # omz path export ZSH=&quot;$HOME/.oh-my-zsh&quot; # theme settings ZSH_THEME=&quot;juanghurtado&quot; # pluging settings plugins=(git zsh-syntax-highlighting zsh-autosuggestions) # autosuggestion highlight ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=4&quot; # source omz source $ZSH/oh-my-zsh.sh #History setup HISTFILE=$HOME/.zsh_history HISTSIZE=100000 SAVEHIST=$HISTSIZ zstyle &#39;:completion:*&#39; menu select zstyle &#39;:completion:*&#39; group-name &#39;&#39; # group results by category zstyle &#39;:completion:::::&#39; completer _expand _complete _ignored _approximate #enable approximate matches for completion #disable auto correct unsetopt correct_all . Set Terminal Color (Optional) . Optionally you customize your terminal color and in this case I’ve used Gogh to achieve this. . Pre Install | . $ sudo apt-get install dconf-cli uuid-runtime . Install on Linux | . $ bash -c &quot;$(wget -qO- https://git.io/vQgMr)&quot; . Install on MacOS $ bash -c &quot;$(curl -sLo- https://git.io/vQgMr)&quot; . | Put the code associated with your desired color scheme. | . Updating OMZ . $ upgrade_oh_my_zsh . Uninstall Zsh . $ sudo apt-get --purge remove zsh . Uninstall OMZ . $ uninstall_oh_my_zsh . Switch Back to Bash . $ chsh -s $(which bash) . Reference . Oh-My-Zsh | FiraCode | Gogh |",
            "url": "https://rednafi.github.io/digressions/linux/2019/10/29/minimal-zsh.html",
            "relUrl": "/linux/2019/10/29/minimal-zsh.html",
            "date": " • Oct 29, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Essential Bash Scripting",
            "content": ". Shell . Several layers of events take place whenever a Linux command is entered into the terminal. The top layer of that is known as shell. . A shell is any user interface to the UNIX operating system, i.e., any program that takes input from the user, translates it into instructions that the operating system can understand, and conveys the operating system’s output back to the user. . Let’s look at an example: . sort -n src/files/numbers.txt &gt; src/files/sorted_numbers.txt . This command will perform the following tasks: . Go to the src/files directory | Sort the numbers in the numbers.txt files in ascending order | Save the result in a new file called sorted_numbers.txt in the same directory | . History . The first major shell was the Bourne shell (named after its inventor, Steven Bourne); it was included in the first popular version of UNIX, Version 7, starting in 1979. The Bourne shell is known on the system as sh. Although UNIX has gone through many, many changes, the Bourne shell is still popular and essentially unchanged. Several UNIX utilities and administration features depend on it. . Variants of some popular shells: . C Shell or csh (The syntax has resemblance with C programming language) | Korn Shell or ksh (Similar to Bourne Shell with features from both Bourne and C Shell) | The Bourne Again Shell or BASH (Started with the GNU project in 1988.) | . BASH is going to be our primary focus here. . A Few Basic Commands . List of most frequently used commands. All of these commands can be run directly from a bash command prompt: . cd | ls | cat | cp | mv | mkdir | rm | grep | lp | . All of the following command summaries can be found via: . curl cheat.sh/&lt;prompt&gt; . cd . cd is used to change directory . #Go to the given directory cd path/to/directory #Go to home directory of current user cd #Go up to the parent of the current directory cd .. #Go to the previously chosen directory cd - . ls . ls lists all the files and folders in a user-specified directory . # Displays everything in the target directory ls path/to/the/target/directory # Displays everything including hidden files ls -a # Displays all files, along with the size (with unit suffixes) and timestamp ls -lh # Display files, sorted by size ls -S # Display directories only ls -d */ # Display directories only, include hidden ls -d .*/ */ . cat . cat shows the contents of a user-specified file . # Display the contents of a file cat /path/to/foo # Display contents with line numbers cat -n /path/to/foo # Display contents with line numbers (blank lines excluded) cat -b /path/to/foo . cp . cp copies files or folders from one directory to another . # Create a copy of a file cp ~/Desktop/foo.txt ~/Downloads/foo.txt # Create a copy of a directory cp -r ~/Desktop/cruise_pics/ ~/Pictures/ # Create a copy but ask to overwrite if the destination file already exists cp -i ~/Desktop/foo.txt ~/Documents/foo.txt # Create a backup file with date cp foo.txt{,.&quot;$(date +%Y%m%d-%H%M%S)&quot;} . mv . mv moves files or folders from one directory to another and can also be used to rename files or folders . # Move a file from one place to another mv ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another and automatically overwrite if the destination file exists # (This will override any previous -i or -n args) mv -f ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but ask before overwriting an existing file # (This will override any previous -f or -n args) mv -i ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but never overwrite anything # (This will override any previous -f or -i args) mv -n ~/Desktop/foo.txt ~/Documents/foo.txt # Move listed files to a directory mv -t ~/Desktop/ file1 file2 file3 . mkdir . mkdir is used to create a folder in a directory . # Create a directory and all its parents mkdir -p foo/bar/baz # Create foo/bar and foo/baz directories mkdir -p foo/{bar,baz} # Create the foo/bar, foo/baz, foo/baz/zip and foo/baz/zap directories mkdir -p foo/{bar,baz/{zip,zap}} . rm . rm is mainly used to delete files or folders . # Remove files and subdirs rm -rf path/to/the/target/ # Ignore non existent files rm -f path/to/the/target # Remove a file with his inode find /tmp/ -inum 6666 -exec rm -i &#39;{}&#39; ; . grep . grep can be used to search through the output of another command . # Search a file for a pattern grep pattern file # Case insensitive search (with line numbers) grep -in pattern file # Recursively grep for string &lt;pattern&gt; in folder: grep -R pattern folder # Read search patterns from a file (one per line) grep -f pattern_file file # Find lines NOT containing pattern grep -v pattern file # You can grep with regular expressions grep &quot;^00&quot; file #Match lines starting with 00 grep -E &quot;[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3}&quot; file #Find IP add # Find all files which match {pattern} in {directory} # This will show: &quot;file:line my research&quot; grep -rnw &#39;directory&#39; -e &quot;pattern&quot; # Exclude grep from your grepped output of ps. # Add [] to the first letter. Ex: sshd -&gt; [s]shd ps aux | grep &#39;[h]ttpd&#39; # Colour in red {bash} and keep all other lines ps aux | grep -E --color &#39;bash|$&#39; . lp . lp prints the specified output via an available printer . # lp # Print files. # Print the output of a command to the default printer (see `lpstat` command): echo &quot;test&quot; | lp # Print a file to the default printer: lp path/to/filename # Print a file to a named printer (see `lpstat` command): lp -d printer_name path/to/filename # Print N copies of a file to the default printer (replace N with the desired number of copies): lp -n N path/to/filename # Print only certain pages to the default printer (print pages 1, 3-5, and 16): lp -P 1,3-5,16 path/to/filename # Resume printing a job: lp -i job_id -H resume . clear . clear is used to clear the CLI window . # clear # Clears the screen of the terminal. # Clear the screen (equivalent to typing Control-L when using the bash shell): clear . exit . exit closes the CLI window . # exit # Quit the current CMD instance or the current batch file. # More information: #&lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/exit&gt;. # Quit the current CMD instance: exit # Quit the current batch script: exit /b # Quit using a specific exit code: exit exit_code . Basic Scripting Examples . When you need to execute multiple shell commands sequentially or want to do more complex stuffs, it’s better to enclose the commands in a bash script. . Running a Shell Script . Create a file with .sh extension. I have used Ubuntu’s built-in nano editor for that. . $ nano script.sh . | Put your code in the .sh file | Make sure you put the shebang #!/bin/bash at the beginning of each script, otherwise, the system wouldn’t know which interpreter to use. . | Give permission to run: . $ chmod +x script.sh . | Run the script via: . $ ./script . | If the script takes in one or multiple arguments, then place those with spaces in between. . $ ./script arg1 arg2 . | . conditionals (if-else) . Example-1: This program, Takes in two integers as arguments | Compares if one number is greater than the other or if they are equal | Returns the greater of the two numbers or if they are equal, returns equal | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number1=&quot;$1&quot; number2=&quot;$2&quot; if [ $number1 -eq $number2 ] then echo &quot;The numbers are equal&quot; elif [ $number1 -gt $number2 ] then echo &quot;The greater number is $number1&quot; elif [ $number2 -gt $number1 ] then echo &quot;The greater number is $number2&quot; fi . $ ./script.sh 12 13 The greater number is 13 . | Example-2: This program, Takes a single number as an argument | Checks whether the number is Odd or Even | Returns Odd or Even accordingly | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number=&quot;$1&quot; if [ $(( number%2 )) -eq 0 ] then echo &quot;Even&quot; else echo &quot;Odd&quot; fi . $ ./script.sh 20 Even . | Example-3: This program, Takes in two integers and an operation instruction | Returns the value according to the operation | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail echo &quot;Enter two numbers and the intended operation: * for addition, write add * for subtraction, write sub * for multiplication, write mul * for division, write div (write quit to quit the program)&quot; num1=&quot;$1&quot; num2=&quot;$2&quot; operation=&quot;$3&quot; if [ $num1 == &quot;quit&quot; ] then break elif [ $operation == &quot;add&quot; ] then ans=$(( $num1 + $num2 )) echo &quot;addition: $ans&quot; elif [ $operation == &quot;sub&quot; ] then ans=$(( $num1 - $num2 )) echo &quot;subtraction: $ans&quot; elif [ $operation == &quot;mul&quot; ] then ans=$(( $num1 * $num2 )) echo &quot;multiplication: $ans&quot; elif [ $operation == &quot;div&quot; ] then ans=$(( $num1 / $num2 )) echo &quot;division: $ans&quot; fi . $ ./script.sh 12 13 add 25 . | . for loop . Example-1: Looping through 0 to 9 with increment 3 and printing the numbers . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for var in {0..9..3} do echo $var done . $ ./script.sh 0 3 6 9 . | Example-2: Looping through files in a folder and printing them one by one . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for file in $(ls ./files) do echo $file done . $ ./script.sh numbers.txt sorted_numbers.txt . | Example-3: This program, Doesn’t take any argument | Returns the summation of all the integers, starting from 0, up to 100 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail sum=0 for num in $(seq 0 100) do sum=$(($sum + $num)) done echo &quot;Total sum is $sum&quot; . $ ./script.sh Total sum is 5050 . | Example-4: This program, Takes in an integer as an argument | Prints all the numbers up to that number, starting from 0 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail input_number=&quot;$1&quot; for num in $(seq 0 $input_number) do if [ $num -lt $input_number ] then echo $num fi done . $ ./script.sh 100 0 1 . . 99 . | . while loop . Example-1: This program, Takes in a single integer as an argument | Returns the factorial of that number | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail counter=&quot;$1&quot; factorial=1 while [ $counter -gt 0 ] do factorial=$(( $factorial * $counter )) counter=$(( $counter - 1 )) done echo &quot;Factorial of $1 is $factorial&quot; . $ ./script.sh 5 Factorial of 5 is 120 . | Example-2: This program, Takes two integers as arguments | Returns the summation of the numbers | Sending -1 as an input quits the program | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail while : do read -p &quot;Enter two numbers ( - 1 to quit ) : &quot; &quot;a&quot; &quot;b&quot; if [ $a -eq -1 ] then break fi ans=$(( $a + $b )) echo $ans done . $ ./script.sh Enter two numbers (-1 to quit): 20 30 30 . | Example-3: This program, Takes in a text filepath as argument | Reads and prints out each line of the file | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail file=&quot;$1&quot; while read -r line do echo &quot;$line&quot; done &lt; &quot;$file&quot; . $ ./script.sh files/numbers.txt 5 55 . . 11 10 . | . functions . Functions are incredible tools when we need to reuse code. Creating functions are fairly straight forward in bash. . Example-1: This function, . Takes a directory as an input argument | Counts the number of files in that directory and prints that out | Note that this function ignores the dot files (The ls -1 flag ignores dot files) | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function file_count () { ls -1 &quot;$1&quot; | wc -l } # calling the function echo $( file_count $1 ) . $ ./script.sh ./files $ 2 . | Example-2: This function, Takes in a shortcode for any of the following languages (a) en for English (b) fr for French (c) bn for bangla . | Returns a welcome message in the selected language . | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function greetings () { language=&quot;$1&quot; if [ $language == &quot;en&quot; ] then echo &quot;Greetings Mortals!&quot; elif [ $language == &quot;fr&quot; ] then echo &quot;Salutations Mortels!&quot; elif [ $language == &quot;bn&quot; ] then echo &quot;নশ্বরকে শুভেচ্ছা!&quot; fi } # calling the function echo $( greetings $1 ) . $ ./script.sh en Greetings Mortals! . | Example-3: This function, Takes a directory as an argument | Loop through the files | Only returns the text files with full path | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function return_text () { dir=&quot;$1&quot; for file in $dir&quot;/*.txt&quot; do echo &quot;$( realpath $file )&quot; done } echo &quot;$( return_text $1 )&quot; . $ ./script.sh /home/redowan/code/bash/files/numbers.txt /home/redowan/code/bash/files/sorted_numbers.txt . | . Some Good Practices . Use a Bash Strict Mode . Your bash scripts will be more robust, reliable and easy to debug if it starts with: . #!/bin/bash set -euo pipefail . This can be regarded as an unofficial bash strict mode and often prevents many classes of sneaky bugs in your script. The above command can be synthesized into multiple commands. . set -euo pipefail is short for: . set -e set -u set -o pipefail . Let’s have a look at each of them separately. . set-e: This instruction forces the bash script to exit immediately if any command has a non zero exit status. If there is an issue in any of the lines in your code, the subsequent lines simply won’t run. . | set-u: If your code has a reference to any variable that wasn’t defined previously, this will cause the program to exit. . | set -o pipefail: This setting prevents errors in a pipeline being masked. If any command in a pipeline fails, that return code will be used as the return code of the whole pipeline, not the last command’s return code. . | . For a more in depth explanation of the different settings and Bash Strict Mode in general, check out, AAron Maxwell’s blog on this topic. . Double Quote Your Variables . It is generally a good practice to double quote your variables, specially user input variables where spaces are involved. . External Resources . Here are some awesome sources where you can always look into if you get stuck: . Command Line Crash Course | Ryans Bash Tutorial | W3 School CLI Tutorial | .",
            "url": "https://rednafi.github.io/digressions/linux/2019/09/05/essential-bash.html",
            "relUrl": "/linux/2019/09/05/essential-bash.html",
            "date": " • Sep 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Redowan Delowar is currently working as a junior Data Scientist for ShopUp (~2 years). . Redowan primarily focuses on Computational Statistics, Representation Learning and Software development. He is an avid Machine Learning evangelist, researcher, practitioner and Open Source developer. He is also available for on-site teaching, presentations and certain types of short term contract works.  . Contact . Gmail: redowan.nafi@gmail.com | Github: rednafi | Twitter: rednafi | LinkedIn: Redowan Delowar | .",
          "url": "https://rednafi.github.io/digressions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}