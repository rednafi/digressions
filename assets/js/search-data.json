{
  
    
        "post0": {
            "title": "Migrating From Medium & Other Maladies",
            "content": "I picked up Medium for dumping my thoughts and learnings immediately after I discovered it back in 2016 and up until very recently, it has served me quite well. It tries democratize online content writing by lowering the access barrier for the non-technical writers and does a fairly good job in doing so. It’s extremely easy to fire up Medium’s built-in editor and just start writing. There is nothing to install or configure as everything is embedded into their website. The formatting tools are not fancy but adequate and the process of publishing involves merely adding a few tags and hitting the designated button. This reduction of friction usually means less focus on formatting, deployment, hosting and more focus on the actual writing. May be that’s why people outside of the tech sphere have embraced the platform wholeheartedly. Also, the platform has fantastic SEO and no matter what you are writing about, it’s almost guaranteed to garner a few eyeballs around it. There are a number publications dedicated to different topics and this makes the process of building up an audience even easier. However, the good stuffs probably end there. . Although Medium has been adopted by numerous technical writers and publications, it wasn’t necessarily built targeting this group and doesn’t consider them as first class citizens of the platform. I primarily write about Python, data science, software development and infosec in general. There’re multiple pain points that eventually forced me to steer away from Medium and look for better alternatives. First, It doesn’t support markdown syntax and the bare-bone formatting tools can get in the way of writing contents that have code snippets or require custom formatting. Secondly, there’s no built-in support for code syntax highlighting and you have to embed your code snippets as github gists. Managing and maintaining all these random gists can be a lot of work if you have many sizeable works that contain code snippets. Also, there’s no proper support for Latex syntax to render mathematical equations. That’s a huge deal breaker for me since many of my Data Science blogs use mathematical equations to explain disparate concepts. Usually, you get around this by converting the equations to images and embedding them in the blogs. But it completely borks mobile device readability. Another thing is that you don’t control your contents in the platform, Medium hosts and manages them for you. It can be both a blessing and a curse. Blessing because you don’t have to worry about deploying, hosting or managing your writings but at the same time you lose control over them too. Medium can censor you without you even knowing and as of writing this rant, you can only export your content as html, not in markdown format. So, if you ever think of leaving the platform, migrating can be big pain in the rear. . For me, the final nail in the coffin was their introduction of the premium tier. Until then reading and writing on Medium was, although problematic but quite manageable. Then they started blasting these premium subscription banners right at your face. This incessant pestering for subscribing turned into an unavoidable nuisance pretty quickly. Now, I don’t know about you, but most of the contents Medium displays in my feed are premium contents. Often I open an article just to arrive at a dead end that hides the rest of the content behind a paywall. Then I started paying 5 dollar per month to get rid of it, only to be disappointed by a plethora of low quality articles that were previously hidden behind the paywall. I’m all in for a subscription based business model but it seems like Medium tries its best to make you feel almost sorry for yourself if you’re not paying. . So, I finally took the step and wanted to get out of the platform even at the cost of losing a few visitors in the process and started looking for other options. I had a few almost non-negotiable requirements in my mind. . First, I needed absolute control over all of my contents, which means writing them using my favorite text editor (VSCode). Secondly, complete access to the contents in their raw format is mandatory. Markdown had to be the format of choice since it’s fast and easy to write without losing focus while tinkering with bolts and knobs of different custom menus of the ib-built editor that usually these blogging platform offers. Also, Latex support and image rendering was crucial for me as several of my blogs contained plots and mathematical equations in them. It also had to be mobile friendly. Finally, the deployment and publication procedures should be almost as easy as medium, means, I wouldn’t have to deal with the complexities that might arise while hosting and deployment when I just needed to get the job done. . So, I started exploring multiple options. Since I primarily work with Python, I was looking for a Python based blogging framework and discovered [pelican]. However, to my disappointment, almost all of the themes pelican offered were either archaic, didn’t work on phones, unmaintained or several years old. So, I shifted my focus on some of the newer frameworks like [Hugo] and [Gatsby]. Hugo is written primarily in Go and the ecosystem is pretty darn cool. It has a huge collection of official and community based themes that works almost out of the box. Gatsby is even better if you are comfortable with react. I picked up Hugo and started a building brand new blog. Unfortunately, I quickly became obsessed with the process of building a new blog and began tinkering and exploring the framework a little too much. Changing the themes multiple times, customizing them with CSS, changing colors and stuff etc. I focused more on the building process than the writings itself. Then there was this complicated process of deployment. Since I wanted full control over my contents, I decided to deploy the blog using Github pages. Hugo supports deployments using gh-pages but I couldn’t find any CI formula that worked out of the box. So I wrote something myself to automate the deployment but wasn’t very happy with the speed of the entire process. Dealing with deployment issues weren’t the best experience exactly. . While I was again in search of a simple tool to make my own blog, I found this tweet where Hamel Hussain and Jeremy P Howard announced [fastpages]. It has pretty much everything that I want in a tool to architect my blog. The UI is super simple, it supports blogging in multiple formats, I can write and preserve my blogs as markdown files, jupyter notebooks or even as microsoft word documents. The UI looks great on mobile devices and the it hosts the blog in Github. I don’t have to get out of my text editor to write a blog and the CI works out of the box. This makes the entire hosting and deployment completely automatic and hassle free. Now I just write my contents in markdown or jupyter notebook format and push it to the master branch. The CI takes care of the rest of the things. . Fastpages have few loose ends for you to tinker with which is its strength in my opinion. Also, it’s the perfect blogging tool if you work in the data science paradigm since it can directly turn your jupyter notebooks into a beautifully formatted static blog. However, the process of migrating my previous blogs was a chore. I haven’t yet fully migrated all of the writings as Medium doesn’t export your contents in markdown format. So copy paste is my friend for now. However, I’m quite enjoying it here. Also, my Google Analytics is showing a pretty encouraging amount of traffic here. Not regretting the journey at all! .",
            "url": "https://rednafi.github.io/digressions/meta/2020/04/24/leaving-medium.html",
            "relUrl": "/meta/2020/04/24/leaving-medium.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Effortless Concurrency with Python's concurrent.futures",
            "content": "Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all these icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect: . In Python, if the task at hand is I/O bound, you can use use standard library’s threading module or if the task is CPU bound then multiprocessing module can be your friend. These threading and multiprocessing APIs give you a lot of control and flexibility but they come at the cost of having to write relatively low-level verbose code that adds extra layers of complexity on top of your core logic. Sometimes when the target task is complicated, it’s often impossible to avoid complexity while adding concurrency. However, a lot of simpler tasks can be made concurrent without adding too much extra overhead. . Python standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for providing the developers a high-level interface to launch asynchronous tasks. It’s a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It’s the perfect tool when you just want to run a piece of eligible code concurrently and don’t need the added modularity that the threading and multiprocessing APIs expose. . Anatomy of concurrent.futures . From the official docs, . The concurrent.futures module provides a high-level interface for asynchronously executing callables. . What it means is you can run your subroutines asynchronously using either threads or processes through a common high-level interface. Basically, the module provides an abstract class called Executor. You can’t instantiate it directly, rather you need to use one of two subclasses that it provides to run your tasks. . Executor (Abstract Base Class) │ ├── ThreadPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage I/O bound tasks with threading underneath │ ├── ProcessPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage CPU bound tasks with multiprocessing underneath . Internally, these two classes interact with the pools and manage the workers. Futures are used for managing results computed by the workers. To use a pool of workers, an application creates an instance of the appropriate executor class and then submits them for it to run. When each task is started, a Future instance is returned. When the result of the task is needed, an application can use the Future object to block until the result is available. Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects do not need to be managed directly. . Executor Objects . Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I’ll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim. . submit(fn, args, *kwargs) . Schedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable. . with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(pow, 323, 1235) print(future.result()) . map(func, *iterables, timeout=None, chunksize=1) . Similar to map(func, *iterables) except: . the iterables are collected immediately rather than lazily; | func is executed asynchronously and several calls to func may be made concurrently. . The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn’t available after timeout seconds from the original call to Executor.map(). Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time. . If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator. . When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . | . Generic Workflows for Running Tasks Concurrently . A lot of my scripts contains some variants of the following: . for task in get_tasks(): perform(task) . Here, get_tasks returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. Tasks are usually blocking callables and they run one after another, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming. . A general rule of thumb is using ThreadPoolExecutor when the tasks are primarily I/O bound like - sending multiple http requests to many urls, saving a large number of files to disk etc. ProcessPoolExecutor should be used in tasks that are primarily CPU bound like - running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc. . Running Tasks with Executor.submit . When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task) for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): print(f&quot;The outcome is {fut.result()}&quot;) . Here you start by creating an Executor, which manages all the tasks that are running – either in separate processes or threads. Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling the executor.shutdown() method implicitly when you’re done. . In real code, you’d would need to replace the Executor with ThreadPoolExecutor or a ProcessPoolExecutor depending on the nature of the callables. Then a set comprehension has been used here to start all the tasks. The executor.submit() method schedules each task. This creates a Future object, which represents the task to be done. Once all the tasks have been scheduled, the method concurrent.futures_as_completed() is called, which yields the futures as they’re done – that is, as each task completes. The executor.result() method gives you the return value of perform(task), or throws an exception in case of failure. . The executor.submit() method schedules the tasks asynchronously and doesn’t hold any contexts regarding the original tasks. So if you want to map the results with the original tasks, you need to track those yourself. . import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task): task for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): original_task = futures[fut] print(f&quot;The result of {original_task} is {fut.result()}&quot;) . Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary. . Running Tasks with Executor.map . Another way the results can be collected in the same order they’re scheduled is via using execuror.map() method. . import concurrent.futures with concurrent.futures.Executor() as executor: for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())): print(f&quot;The result of {arg} is {res}&quot;) . Notice how the map function takes the entire iterable at once. It spits out the results immediately rather than lazily and in the same order they’re scheduled. If any unhandled exception occurs during the operation, it’ll also be raised immediately and the execution won’t go any further. . In Python 3.5+, executor.map() receives an optional argument: chunksize. While using ProcessPoolExecutor, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. . A Few Real World Examples . Before proceeding with the examples, let’s write a small decorator that’ll be helpful to measure and compare the execution time between concurrent and sequential code. . import time from functools import wraps def timeit(method): @wraps(method) def wrapper(*args, **kwargs): start_time = time.time() result = method(*args, **kwargs) end_time = time.time() print(f&quot;{method.__name__} =&gt; {(end_time-start_time)*1000} ms&quot;) return result return wrapper . The decorator can be used like this: . @timeit def func(n): return list(range(n)) . This will print out the name of the method and how long it took to execute it. . Download &amp; Save Files from URLs with Multi-threading . First, let’s download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we’ll be using the ThreadPoolExecutor class to carry out the operation. But before that, let’s do this sequentially first. . from pathlib import Path import urllib.request def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): return [download_one(url) for url in urls] if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 22850.6863117218 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk. The second function download_all just iterates through a sequence of URLs and applies the download_one function on each of them. The sequential code takes about 22.8 seconds to run. Now let’s see how our threaded version of the same code performs. . from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor, as_completed def download_one(url): &quot;&quot;&quot; Downloads the specified URL and saves it to disk &quot;&quot;&quot; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(&quot;URL does not contain an extension&quot;) with open(fname, &quot;wb&quot;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f&quot;Finished downloading {fname}&quot; return msg @timeit def download_all(urls): &quot;&quot;&quot; Create a thread pool and download specified urls &quot;&quot;&quot; with ThreadPoolExecutor(max_workers=13) as executor: return executor.map(download_one, urls, timeout=60) if __name__ == &quot;__main__&quot;: urls = ( &quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf&quot;, &quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf&quot;, ) results = download_all(urls) for result in results: print(result) . &gt;&gt;&gt; download_all =&gt; 5042.651653289795 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf . The concurrent version of the code takes only about 1/4 th the time of it’s sequential counterpart. Notice in this concurrent version, the download_one function is the same as before but in the download_all function, a ThreadPoolExecutor context manager wraps the execute.map() method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using 2 * multiprocessing.cpu_count() + 1. My machine has 6 physical cores with 12 threads. So 13 is the value I chose. . Note: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task. . Running Multiple CPU Bound Subroutines with Multi-processing . The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. Let’s run the function sequentially first. . import hashlib def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; for i in range(n): hsh = hash_one(n) return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 18317.330598831177 ms . If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. The above code takes roughly 18 seconds to run in sequential mode. Now let’s run it parallelly using ProcessPoolExecutor. . import hashlib from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor def hash_one(n): &quot;&quot;&quot;A somewhat CPU-intensive task.&quot;&quot;&quot; for i in range(1, n): hashlib.pbkdf2_hmac(&quot;sha256&quot;, b&quot;password&quot;, b&quot;salt&quot;, i * 10000) return &quot;done&quot; @timeit def hash_all(n): &quot;&quot;&quot;Function that does hashing in serial.&quot;&quot;&quot; with ProcessPoolExecutor(max_workers=10) as executor: for arg, res in zip(range(n), executor.map(hash_one, range(n), chunksize=2)): pass return &quot;done&quot; if __name__ == &quot;__main__&quot;: hash_all(20) . &gt;&gt;&gt; hash_all =&gt; 1673.842430114746 ms . If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance. As you can see the concurrent version of the above CPU intensive operation is about 11 times faster than its sequential counterpart. . Avoiding Concurrency Pitfalls . Since the concurrent.futures provides such a simple API, you might be tempted to apply concurrency to every simple tasks at hand. However, that’s not a good idea. First, the simplicity has its fair share of constraints. In this way, you can apply concurrency only to the simplest of the tasks, usually mapping a function to an iterable or running a few subroutines simultaneously. If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules. . Another pitfall of using concurrency is deadlock situations that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let’s see a slightly modified example from the official docs. . import time from concurrent.futures import ThreadPoolExecutor def wait_on_b(): time.sleep(5) print(b.result()) # b will never complete because it is waiting on a. return 5 def wait_on_a(): time.sleep(5) print(a.result()) # a will never complete because it is waiting on b. return 6 with ThreadPoolExecutor(max_workers=2) as executor: # here, the future from a depends on the future from b # and vice versa # so this is never going to be completed a = executor.submit(wait_on_b) b = executor.submit(wait_on_a) print(&quot;Result from wait_on_b&quot;, a.result()) print(&quot;Result from wait_on_a&quot;, b.result()) . In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function’s result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. Let’s explain another deadlock situation from the official docs. . from concurrent.futures import ThreadPoolExecutor def wait_on_future(): f = executor.submit(pow, 5, 2) # This will never complete because there is only one worker thread and # it is executing this function. print(f.result()) with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(wait_on_future) print(future.result()) . The above situation usually happens when a subroutine produces nested Future object and runs runs on a single thread. In the function wait_on_future, the executor.submit(pow, 5, 2) creates another Future object. Since I’m running the entire thing using a single thread, the internal future object is blocking the thread and the external executor.submit() method inside the context manager can not use any threads. This situation can be avoided using multiple threads but in general, this is a bad design itself. . Then there’re situations when you might be getting lower performance with concurrent code than its sequential counterpart. This could happen for multiple reasons. . Threads were used to perform CPU bound tasks | Multiprocessing were used to perform I/O bound tasks | The tasks were too trivial to justify using either threads or multiple processes | Spawning and squashing multiple threads or processes bring extra overheads. Usually threads are much faster than processes to spawn and squash. However, using the wrong type of concurrency can actually slow down your code rather than making it any performant. Below is a trivial example where both ThreadPoolExecutor and ProcessPoolExecutor perform worse than their sequential counterpart. . import math PRIMES = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): for number in PRIMES: print(f&quot;{number} is prime: {is_prime(number)}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 67.65174865722656 ms . The above examples verifies whether a number in a list is prime or not. We ran the function on 1000 numbers to determine if they’re prime or not. The sequential version took roughly 67ms to do that. However, look below where the threaded version of the same code takes more than double the time (140ms) to so the same task. . from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ThreadPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 140.17250061035156 ms . The multiprocessing version of the same code is even slower. The tasks doesn’t justify opening so many processes. . from concurrent.futures import ProcessPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n &lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ProcessPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f&quot;{number} is prime: {prime}&quot;) if __name__ == &quot;__main__&quot;: main() . &gt;&gt;&gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =&gt; 311.3126754760742 ms . Although intuitively, it may seem like the task of checking prime numbers should be a CPU bound operation, it’s also important to determine if the task itself is computationally heavy enough to justify spawning multiple threads or processes. Otherwise you might end up with complicated code that performs worse than the simple solutions. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. . References . concurrent.fututures- the official documentation | Easy Concurrency in Python | Adventures in Python with concurrent.futures |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/21/python-concurrent-futures.html",
            "relUrl": "/python/2020/04/21/python-concurrent-futures.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "No Really, Python's Pathlib is Great",
            "content": "When I first encountered Python’s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when this ticket confirmed that Django was replacing os.path with pathlib, I got curious. . The os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I’ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top. . Problem with Python’s Path Handling . Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome . However, paths are not actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src. . from glob import glob import os import shutil for fname in glob(&quot;*.py&quot;): new_path = os.path.join(&quot;src&quot;, fname) shutil.copy(fname, new_path) . The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let’s have a look at a few more examples of performing the same tasks using os.path and pathlib modules. . Joining &amp; Creating New Paths . Say you want to achieve the following goals: . There is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory . | Then you want to save the absolute path of file_another.txt in a new variable. . | . Let’s see how you’d usually do this via the os module. . from os.path import abspath, dirname, join file_path = abspath(&quot;./file.txt&quot;) base_dir = dirname(file_path) file_another_path = join(base_dir, &quot;file_another.txt&quot;) . The variables file_path, base_dir, file_another_path look like this on my machine: . print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . You can use the usual string methods to transform the paths but generally, that’s not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems do not define paths in the same way. Windows uses &quot; &quot; while Mac and *nix based OSes use &quot;/&quot; as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses &quot;/&quot; operator overloading and make this a little less painful. . from pathlib import Path file_path = Path(&quot;file.txt&quot;).resolve() base_dir = file_path.parent file_another_path = base_dir / &quot;another_file.txt&quot; print(&quot;file_path:&quot;, file_path) print(&quot;base_dir:&quot;, base_dir) print(&quot;file_another_path:&quot;, file_another_path) . &gt;&gt;&gt; file_path: /home/rednafi/code/demo/file.txt &gt;&gt;&gt; base_dir: /home/rednafi/code/demo &gt;&gt;&gt; file_another_path: /home/rednafi/code/demo/file_another.txt . The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly. . Making Directories &amp; Renaming Files . Here’s a piece of code that: . Tries to make a src/stuff/ directory when it already exists | Renames a file in the src directory called .config to .stuffconfig: | . import os import os.path os.makedirs(os.path.join(&quot;src&quot;, &quot;stuff&quot;), exist_ok=True) os.rename(&quot;src/.config&quot;, &quot;src/.stuffconfig&quot;) . Here is the same thing done using the pathlib module: . from pathlib import Path Path(&quot;src/stuff&quot;).mkdir(parents=True, exist_ok=True) Path(&quot;src/.config&quot;).rename(&quot;src/.stuffconfig&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/.stuffconfig&#39;) . Notice the output where the renamed file path is printed. It’s not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably. . Listing Specific Types of Files in a Directory . Let’s say you want to recursively visit nested directories and list .py files in a directroy called source. The directory looks like this: . src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py . Usually, glob module is used to resolve this kind of situation: . from glob import glob top_level_py_files = glob(&quot;src/*.py&quot;) all_py_files = glob(&quot;src/**/*.py&quot;, recursive=True) print(top_level_py_files) print(all_py_files) . &gt;&gt;&gt; [&#39;src/module.py&#39;] &gt;&gt;&gt; [&#39;src/module.py&#39;, &#39;src/stuff/__init__.py&#39;, &#39;src/stuff/submodule.py&#39;] . The above approach works perfectly. However, if you don’t want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way: . from pathlib import Path top_level_py_files = Path(&quot;src&quot;).glob(&quot;*.py&quot;) all_py_files = Path(&quot;src&quot;).rglob(&quot;*.py&quot;) print(list(top_level_py_files)) print(list(all_py_files)) . This will also print the same as before: . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;)] &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh? . Opening Multiple Files &amp; Reading their Contents . Now let’s open the .py files and read their contents that you recursively discovered in the previous example. . from glob import glob contents = [] for fname in glob(&quot;src/**/*.py&quot;, recursive=True): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib ...&#39;] . The pathlib implementation is almost identical as above. . from pathlib import Path contents = [] for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;): with open(fname, &quot;r&quot;) as f: contents.append(f.read()) print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . You can also cook up a more robust implementation with generator comprehension and context manager. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] print(contents) . &gt;&gt;&gt; [&#39;from contextlib import ...&#39;] . Anatomy of the Pathlib Module . Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are: . PurePath (Useful when you want to work with windows path on a Linux machine) | PurePosixPath (Subclass of PurePath) | PureWindowsPath (Subclass of PurePath) | Path (Concrete path object, most of the time, you’ll be dealing with this one) | PosixPath (Concrete posix path, subclass of Path) | WindowsPath (Concrete windows path, subclass of Path) | This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes. . . Unless you are doing cross platform path manipulation, most of the time you’ll be working with the concrete Path object. So I’ll focus on the methods and properties of Path class only. . Operators . Instead of using os.path.join you can use / operator to create child paths. . from pathlib import Path base_dir = Path(&quot;src&quot;) child_dir = base_dir / &quot;stuff&quot; file_path = child_dir / &quot;__init__.py&quot; print(file_path) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Attributes &amp; Methods . The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We’ll linearly traverse through the tree and provide necessary examples to grasp their usage. . Path │ ├── Attributes │ ├── parts │ ├── parent &amp; parents │ ├── name │ ├── suffix &amp; suffixes │ └── stem │ │   └── Methods    ├── joinpath(*other) ├── cwd() ├── home() ├── exists() ├── expanduser() ├── glob() ├── rglob(pattern) ├── is_dir() ├── is_file() ├── is_absolute() ├── iterdir() ├── mkdir(mode=0o777, parents=False, exist_ok=False) ├── open(mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None) ├── rename(target) ├── replace(target) ├── resolve(strict=False) └── rmdir() . Let’s dive into their usage one by one. For all the examples, We’ll be using the previously seen directory structure. . src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefile.tar.gz └── module.py . Path.parts . Returns a tuple containing individual components of a path. . from pathlib import Path file_path = Path(&quot;src/stuff/__init__.py&quot;) file_path.parts . &gt;&gt;&gt; (&#39;src&#39;, &#39;stuff&#39;, &#39;__init__.py&#39;) . Path.parents &amp; Path.parent . Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path. . file_path = Path(&quot;src/stuff/__init__.py&quot;) for parent in file_path.parents: print(parent) . &gt;&gt;&gt; src/stuff ... src ... . . file_path.parent . &gt;&gt;&gt; PosixPath(&#39;src/stuff&#39;) . Path.name . Returns the last component of a path as string. Usually used to extract file name from a path. . from pathlib import Path file_path = Path(&quot;src/module.py&quot;) file_path.name . &gt;&gt;&gt; &#39;module.py&#39; . Path.suffixes &amp; Path.suffix . Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.suffixes . &gt;&gt;&gt; [&#39;.tar&#39;, &#39;.gz&#39;] . file_path.suffix . &gt;&gt;&gt;&#39;.gz&#39; . Path.stem . Returns the final path component without the suffix. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.stem . &gt;&gt;&gt; &#39;somefile.tar&#39; . Path.is_absolute . Checks if a path is absolute or not. Return boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.is_absolute() . &gt;&gt;&gt; False . Path.joinpath(*other) . This method is used to combine multiple components into a complete path. This can be used as an alternative to &quot;/&quot; operator for joining path components. . from pathlib import Path file_path = Path(&quot;src&quot;).joinpath(&quot;stuff&quot;).joinpath(&quot;__init__.py&quot;) file_path . &gt;&gt;&gt; PosixPath(&#39;src/stuff/__init__.py&#39;) . Path.cwd() . Returns the current working directory. . from pathlib import Path file_path = Path(&quot;src/stuff/somefile.tar.gz&quot;) file_path.cwd() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo&#39;) . Path.home() . Returns home directory. . from pathlib import Path Path.home() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi&#39;) . Path.exists() . Checks if a path exists or not. Returns boolean value. . from pathlib import Path file_path = Path(&quot;src/stuff/thisisabsent.py&quot;) file_path.exists() . &gt;&gt;&gt; False . Path.expanduser() . Returns a new path with expanded ~ symbol. . from pathlib import Path file_path = Path(&quot;~/code/demo/src/stuff/somefile.tar.gz&quot;) file_path.expanduser() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/somefile.tar.gz&#39;) . Path.glob() . Globs and yields all file paths matching a specific pattern. Let’s discover all the files in src/stuff/ directory that have .py extension. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) file_paths = dir_path.glob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.rglob(pattern) . This is like Path.glob method but matches the file pattern recursively. . from pathlib import Path dir_path = Path(&quot;src&quot;) file_paths = dir_path.rglob(&quot;*.py&quot;) print(list(file_paths)) . &gt;&gt;&gt; [PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/stuff/__init__.py&#39;), PosixPath(&#39;src/stuff/submodule.py&#39;)] . Path.is_dir() . Checks if a path points to a directory or not. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_dir() . &gt;&gt;&gt; True . Path.is_file() . Checks if a path points to a file. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_file() . &gt;&gt;&gt; False . Path.is_absolute() . Checks if a path is absolute or relative. Returns boolean value. . from pathlib import Path dir_path = Path(&quot;src/stuff/&quot;) dir_path.is_absolute() . &gt;&gt;&gt; False . Path.iterdir() . When the path points to a directory, this yields the content path objects. . from pathlib import Path base_path = Path(&quot;src&quot;) contents = [content for content in base_path.iterdir()] print(contents) . &gt;&gt;&gt; [PosixPath(&#39;src/stuff&#39;), PosixPath(&#39;src/module.py&#39;), PosixPath(&#39;src/.stuffconfig&#39;)] . Path.mkdir(mode=0o777, parents=False, exist_ok=False) . Creates a new directory at this given path. . Parameters: . mode:(str) Posix permissions (mimicking the POSIX mkdir -p command) . | parents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised. . | exist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored. . | . from pathlib import Path dir_path = Path(&quot;src/other/side&quot;) dir_path.mkdir(parents=True) . Path.open(mode=’r’, buffering=-1, encoding=None, errors=None, newline=None) . This is same as the built in open function. . from pathlib import Path with Path(&quot;src/module.py&quot;) as f: contents = open(f, &quot;r&quot;) for line in contents: print(line) . &gt;&gt;&gt; from contextlib import contextmanager ... from time import time ... ..... . Path.rename(target) . Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found. . from pathlib import Path file_path = Path(&quot;src/stuff/submodule.py&quot;) file_path.rename(file_path.parent / &quot;anothermodule.py&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/anothermodule.py&#39;) . Path.replace(target) . Replaces a file or directory to the given target. Returns the new path instance. . from pathlib import Path file_path = Path(&quot;src/stuff/anothermodule.py&quot;) file_path.replace(file_path.parent / &quot;Dockerfile&quot;) . &gt;&gt;&gt; PosixPath(&#39;src/stuff/Dockerfile&#39;) . Path.resolve(strict=False) . Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn’t exist, FileNotFoundError will be raised. . from pathlib import Path file_path = Path(&quot;src/./stuff/Dockerfile&quot;) file_path.resolve() . &gt;&gt;&gt; PosixPath(&#39;/home/rednafi/code/demo/src/stuff/Dockerfile&#39;) . Path.rmdir() . Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised. . from pathlib import Path file_path = Path(&quot;src/stuff&quot;) file_path.rmdir() . So, Should You Use It? . Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems. . The ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful. . Remarks . All the pieces of codes in the blog were written and tested with python 3.8 on a machine running Ubuntu 18.04. . References . pathlib — Object-oriented filesystem paths | Python 3’s pathlib Module: Taming the File System | Why you should be using pathlib |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/13/python-pathlib.html",
            "relUrl": "/python/2020/04/13/python-pathlib.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Running Python Linters with Pre-commit Hooks",
            "content": "Pre-commit hooks can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let’s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase. . To keep my sanity, I only use three linters in all of my python projects: . Black: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you’re reading. . | Isort: Isort is a Python utility to sort imports alphabetically, and automatically separated into sections and by type. . Isort parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import: . Future | Python Standard Library | Third Party | Current Python Project | Explicitly Local (. before import, as in: from . import x) | Custom Separate Sections (Defined by forced_separate list in the configuration file) | Custom Sections (Defined by sections list in configuration file) | . Inside each section, the imports are sorted alphabetically. Isort automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79). . | Flake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder’s McCabe script. The combination of these three linters makes sure that your code is compliant with PEP 8 and free of some obvious code smells. . | . Installing Pre-commit . Install using pip: . pip install pre-commit . | Install via curl: . curl https://pre-commit.com/install-local.py | python - . | . Defining the Pre-commit Config File . Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m &quot;Commit message&quot;. The following example shows how black and a few other linters can be added as hooks to the config: . # .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black . Installing the Git Hook scripts . Run . pre-commit install . This will set up the git hook scripts and should show the following output in your terminal: . pre-commit installed at .git/hooks/pre-commit . Now you will be able to implicitly or explicitly run the hooks before each commit. . Running the Hooks Against All the Files . By default, the hooks will run every time you say: . git commit -m &quot;Commit message&quot; . However, if you wish to run the hooks manually on every file, you can do so via: . pre-commit run --all-files . Running the Linters as Pre-commit Hooks . To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there’re a few minor issues that need to be taken care of. . The default line length of black formatter is 88 (you should embrace that) but both flake8 and isort cap the line at 79 characters. This raises conflict and can cause failures. . | Black and isort format the imports differently. . | Flake8 can be overly strict at times. You’ll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors. . | . The following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts. . # .pre-commit-config.yaml # isort - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort args: # arguments to configure isort # making isort line length compatible with black - --line_length 88 - --use_parentheses True - --include_trailing_comma True - --multi_line_output 3 # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include=&#39; .pyi?$&#39; # these folders wont be formatted by black - --exclude=&quot;&quot;&quot; .git | .__pycache__| .hg| .mypy_cache| .tox| .venv| _build| buck-out| build| dist&quot;&quot;&quot; language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - &quot;--max-line-length=88&quot; - &quot;--max-complexity=18&quot; - &quot;--select=B,C,E,F,W,T4,B9&quot; # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - &quot;--ignore=E203,E266,E501,W503,F403,F401,E402&quot; . You can add the above lines to your configuration and run . pre-commit run --all-files . This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters. .",
            "url": "https://rednafi.github.io/digressions/python/2020/04/06/python-precommit.html",
            "relUrl": "/python/2020/04/06/python-precommit.html",
            "date": " • Apr 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Generic Functions with Python's Singledispatch",
            "content": "Recently, I was refactoring a portion of a Python function that somewhat looked like this: . def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 &amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 &amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 &amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... . This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism comes into play. The idea is, you have to define a single function that will be dynamically overloaded with alternative implementations based on the type of the function arguments. . Function Overloading &amp; Generic Functions . Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. When function overloading happens based on its argument types, the resulting function is known as generic function. Let’s see how Python’s singledispatch decorator can help to design generic functions and refactor the icky code above. . Singledispatch . Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. . A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. . As PEP-443 said, singledispatch only happens based on the first argument’s type. Let’s take a look at an example to see how this works! . Example-1: Singledispatch with built-in argument type . Let’s consider the following code: . # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; def process_float(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this code will return . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let’s see how the same thing can be achieved with singledispatch. . # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(&quot;Implement process function.&quot;) @process.register(int) def sub_process(num): # processing interger return f&quot;Integer {num} has been processed successfully!&quot; @process.register(float) def sub_process(num): # processing float return f&quot;Float {num} has been processed successfully!&quot; # use the function print(process(12.0)) print(process(1)) . Running this will return the same result as before. . &gt;&gt;&gt; Float 12.0 has been processed successfully! &gt;&gt;&gt; Integer 1 has been processed successfully! . Example-2: Singledispatch with custom argument type . Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: . def process(data: dict): if data[&quot;genus&quot;] == &quot;Felis&quot; and data[&quot;bucket&quot;] == &quot;cat&quot;: return process_cat(data) elif data[&quot;genus&quot;] == &quot;Canis&quot; and data[&quot;bucket&quot;] == &quot;dog&quot;: return process_dog(data) def process_cat(data: dict): # processing cat return &quot;Cat data has been processed successfully!&quot; def process_dog(data: dict): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_data = {&quot;genus&quot;: &quot;Felis&quot;, &quot;species&quot;: &quot;catus&quot;, &quot;bucket&quot;: &quot;cat&quot;} dog_data = {&quot;genus&quot;: &quot;Canis&quot;, &quot;species&quot;: &quot;familiaris&quot;, &quot;bucket&quot;: &quot;dog&quot;} # using process print(process(cat_data)) print(process(dog_data)) . Running this snippet will print out: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . To refactor this with singledispatch, you can create two data types Cat and Dog.When you make Cat and Dog objects from the classes and pass them through the process function, singledispatch will take care of dispatching the appropriate implementation of that function. . from functools import singledispatch from dataclasses import dataclass @dataclass class Cat: genus: str species: str @dataclass class Dog: genus: str species: str @singledispatch def process(obj=None): raise NotImplementedError(&quot;Implement process for bucket&quot;) @process.register(Cat) def sub_process(obj): # processing cat return &quot;Cat data has been processed successfully!&quot; @process.register(Dog) def sub_process(obj): # processing dog return &quot;Dog data has been processed successfully!&quot; if __name__ == &quot;__main__&quot;: cat_obj = Cat(genus=&quot;Felis&quot;, species=&quot;catus&quot;) dog_obj = Dog(genus=&quot;Canis&quot;, species=&quot;familiaris&quot;) print(process(cat_obj)) print(process(dog_obj)) . Running this will print out the same output as before: . &gt;&gt;&gt; Cat data has been processed successfully! &gt;&gt;&gt; Dog data has been processed successfully! . References . Transform a function into a single dispatch generic function | Function overloading | Parametric polymorphism |",
            "url": "https://rednafi.github.io/digressions/python/2020/04/05/python-singledispatch.html",
            "relUrl": "/python/2020/04/05/python-singledispatch.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The Curious Case of Python's Context Manager",
            "content": "Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: . with open(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: . try: f = open(&quot;file.txt&quot;, &quot;wt&quot;) text = f.write(&quot;contents go here&quot;) finally: f.close() . Writing Custom Context Managers . To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above. . class CustomFileOpen: &quot;&quot;&quot;Custom context manager for opening files.&quot;&quot;&quot; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() . You can use the above class just like a regular context manager. . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . From Generators to Context Managers . Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: . @contextmanager def some_generator(&lt;arguments&gt;): &lt;setup&gt; try: yield &lt;value&gt; finally: &lt;cleanup&gt; . When you use the context manager with the with statement: . with some_generator(&lt;arguments&gt;) as &lt;variable&gt;: &lt;body&gt; . It roughly translates to: . &lt;setup&gt; try: &lt;variable&gt; = &lt;value&gt; &lt;body&gt; finally: &lt;cleanup&gt; . The setup code goes before the try..finally block. Notice the point where the generator yields. This is where the code block nested in the with statement gets executed. After the completion of the code block, the generator is then resumed. If an unhandled exception occurs in the block, it’s re-raised inside the generator at the point where the yield occurred and then the finally block is executed. If no unhandled exception occurs, the code gracefully proceeds to the finally block where you run your cleanup code. . Let’s implement the same CustomFileOpen context manager with contextmanager decorator. . from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): &quot;&quot;&quot;Custom context manager for opening a file.&quot;&quot;&quot; f = open(filename, method) try: yield f finally: f.close() . Now use it just like before: . with CustomFileOpen(&quot;file.txt&quot;, &quot;wt&quot;) as f: f.write(&quot;contents go here&quot;) . Writing Context Managers as Decorators . You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let’s make a RunTime decorator that will be applied on a file-opening function. The decorator will: . Print a user provided description of the function | Print the time it takes to run the function | . from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): &quot;&quot;&quot;Timing decorator.&quot;&quot;&quot; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . You can use the decorator like this: . @RunTime(&quot;This function opens a file&quot;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) . Using the function like this should return: . print(custom_file_write(&quot;file.txt&quot;, &quot;wt&quot;, &quot;jello&quot;)) . This function opens a file The function took 0.0005390644073486328 seconds to run. None . You can also create the same decorator via contextlib.contextmanager decorator. . from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f&quot;The function took {run_time} seconds to run.&quot;) . Nesting Contexts . You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: . from contextlib import contextmanager @contextmanager def get_state(name): print(&quot;entering:&quot;, name) yield name print(&quot;exiting :&quot;, name) # multiple get_state can be nested like this with get_state(&quot;A&quot;) as A, get_state(&quot;B&quot;) as B, get_state(&quot;C&quot;) as C: print(&quot;inside with statement:&quot;, A, B, C) . entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A . Notice the order they’re closed. Context managers are treated as a stack, and should be exited in reverse order in which they’re entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. . Combining Multiple Context Managers . You can combine multiple context managers too. Let’s consider these two managers. . from contextlib import contextmanager @contextmanager def a(name): print(&quot;entering a:&quot;, name) yield name print(&quot;exiting a:&quot;, name) @contextmanager def b(name): print(&quot;entering b:&quot;, name) yield name print(&quot;exiting b:&quot;, name) . Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. . @contextmanager def ab(a, b): with a(&quot;A&quot;) as A, b(&quot;B&quot;) as B: yield (A, B) . This can be used as: . with ab(a, b) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: (&#39;A&#39;, &#39;B&#39;) exiting b: B exiting a: A . If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let’s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. . from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] . with ab((a, b), (&quot;A&quot;, &quot;B&quot;)) as AB: print(&quot;Inside the composite context manager:&quot;, AB) . entering a: A entering b: B Inside the composite context manager: [&#39;A&#39;, &#39;B&#39;] exiting b: B exiting a: A . ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let’s see, how you can do so while avoiding accidental memory leakage with robust resource management. . from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, &quot;r&quot;)) for fname in Path(&quot;src&quot;).rglob(&quot;*.py&quot;) ) contents = [f.read() for f in streams] . Using Context Managers to Create SQLAlchemy Session . If you are familiar with SQLALchemy, Python’s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: . from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(&quot;sqlite://&quot;) # create a configured &quot;Session&quot; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): &quot;&quot;&quot;Provide a transactional scope around a series of operations.&quot;&quot;&quot; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() . The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of committing and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: . with session_scope() as session: myobject = MyObject(&quot;foo&quot;, &quot;bar&quot;) session.add(myobject) . Abstract Away Exception Handling Monstrosity with Context Managers . This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let’s write a decorator that will handle ZeroDivisionError and TypeError simultaneously. . from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(&quot;This is a custom ZeroDivisionError message.&quot;) raise except TypeError: print(&quot;This is a custom TypeError message.&quot;) raise . Now use this in a function where these exceptions occur. . @errhandler() def div(a, b): return a // b . div(&quot;b&quot;, 0) . This is a custom TypeError message. TypeError Traceback (most recent call last) &lt;ipython-input-43-65497ed57253&gt; in &lt;module&gt; -&gt; 1 div(&#39;b&#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): &gt; 75 return func(*args, **kwds) 76 return inner 77 &lt;ipython-input-42-b7041bcaa9e6&gt; in div(a, b) 1 @errhandler() 2 def div(a, b): -&gt; 3 return a // b TypeError: unsupported operand type(s) for //: &#39;str&#39; and &#39;int&#39; . You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? . The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logics from the main method. . import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=&quot; n(asctime)s [%(levelname)s] %(message)s&quot;, handlers=[logging.FileHandler(&quot;./debug.log&quot;), logging.StreamHandler()], ) class Calculation: &quot;&quot;&quot;Dummy class for demonstrating exception decoupling with contextmanager.&quot;&quot;&quot; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f&quot;Custom handling of Zero Division Error! Printing &quot; &quot;only 2 levels of traceback..&quot; ) logging.exception(&quot;ZeroDivisionError&quot;) def main_func(self): &quot;&quot;&quot;Function that we want to save from nasty error handling logic.&quot;&quot;&quot; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) . This will return . (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 25, in errorhandler yield File &quot;&lt;ipython-input-44-ff609edb5d6e&gt;&quot;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None . Persistent Parameters Across Http Requests with Context Managers . Another great use case for context managers is making parameters persistent across multiple http requests. Python’s requests library has a Session object that will let you easily achieve this. So, if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. The following example is taken directly from requests’ official docs. Let’s persist some cookies across requests. . with requests.Session() as session: session.get(&quot;http://httpbin.org/cookies/set/sessioncookie/123456789&quot;) response = session.get(&quot;http://httpbin.org/cookies&quot;) print(response.text) . This should show: . { &quot;cookies&quot;: { &quot;sessioncookie&quot;: &quot;123456789&quot; } } . Remarks . All the code snippets are updated for python 3.8. To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. . Resources . Python Contextlib Documentation | Python with Context Manager - Jeff Knupp | SQLALchemy Session Creation | Scipy Lectures: Context Managers | Merging Context Managers |",
            "url": "https://rednafi.github.io/digressions/python/2020/03/26/python-contextmanager.html",
            "relUrl": "/python/2020/03/26/python-contextmanager.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Up and Running with MySQL in Docker",
            "content": ". Setting Up . Installation . This part describes the basic installation steps of setting up MySQL 5.7 server on Ubuntu Linux using docker. . Install docker on your Linux machine. See the instruction here. . | Install docker compose via following the instructions here. . | Create another folder on your project folder and make a docker-compose.yml file. Run the following instructions one by one: . mkdir mysql_dump cd mysql_dump touch docker-compose.yml . | Open the docker-compose.yml file and copy the following lines into it. . # docker-compose version version: &quot;3.3&quot; services: # images mysql-dev: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test_db ports: - &quot;3306:3306&quot; # making data persistent volumes: - db-data:/var/lib/mysql volumes: db-data: . | . Run MySQL Server . Run the docker-compose command. This will build and run the server in detached mode. . docker compose up -d . Connect Shell to Server . Check the name of the running container with docker ps command. In this case, the running container is called mysql_dumps_mysql-dev_1. Then run the following command to connect your shell to the running server. . # connect shell to server docker exec -it mysql_dumps_mysql-dev_1 mysql -uroot -p . Alter Root Password . If you want to change the root password, enter the following command in the MySQL shell. Replace MyNewPass with your new root password: . ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;; . You should see something like this in the command prompt: . Query OK, 0 rows affected (0.02 sec) . To make the change take effect, type the following command: . FLUSH PRIVILEGES; . View Users . MySQL stores the user information in its own database. The name of the database is mysql. If you want to see what users are set up in the MySQL user table, run the following command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like this: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | ++--+-+ 4 rows in set (0.00 sec) . Create a Database . According to the docker-compose.yml file, you already have created a database named test_db. You can create anotehr database named test_db_2 via the following command: . CREATE DATABASE test_db_2; . List your databases via the following command: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | mysql | | performance_schema | | sys | | test_db | | test_db_2 | +--+ 6 rows in set (0.01 sec) . To ensure the changes: . FLUSH PRIVILEGES; . Creating Dummy Table in the Database . -- create dummy table CREATE TABLE IF NOT EXISTS `student` ( `id` int(2) NOT NULL DEFAULT &#39;0&#39;, `name` varchar(50) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `class` varchar(10) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;, `mark` int(3) NOT NULL DEFAULT &#39;0&#39;, `sex` varchar(6) CHARACTER SET utf8 NOT NULL DEFAULT &#39;male&#39; ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- insert data into the dummy table INSERT INTO `student` (`id`, `name`, `class`, `mark`, `sex`) VALUES (1, &#39;John Deo&#39;, &#39;Four&#39;, 75, &#39;female&#39;), (2, &#39;Max Ruin&#39;, &#39;Three&#39;, 85, &#39;male&#39;), (3, &#39;Arnold&#39;, &#39;Three&#39;, 55, &#39;male&#39;), (4, &#39;Krish Star&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (5, &#39;John Mike&#39;, &#39;Four&#39;, 60, &#39;female&#39;), (6, &#39;Alex John&#39;, &#39;Four&#39;, 55, &#39;male&#39;), (7, &#39;My John Rob&#39;, &#39;Fifth&#39;, 78, &#39;male&#39;), (8, &#39;Asruid&#39;, &#39;Five&#39;, 85, &#39;male&#39;), (9, &#39;Tes Qry&#39;, &#39;Six&#39;, 78, &#39;male&#39;), (10, &#39;Big John&#39;, &#39;Four&#39;, 55, &#39;female&#39;), (11, &#39;Ronald&#39;, &#39;Six&#39;, 89, &#39;female&#39;), (12, &#39;Recky&#39;, &#39;Six&#39;, 94, &#39;female&#39;), (13, &#39;Kty&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (14, &#39;Bigy&#39;, &#39;Seven&#39;, 88, &#39;female&#39;), (15, &#39;Tade Row&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (16, &#39;Gimmy&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (17, &#39;Tumyu&#39;, &#39;Six&#39;, 54, &#39;male&#39;), (18, &#39;Honny&#39;, &#39;Five&#39;, 75, &#39;male&#39;), (19, &#39;Tinny&#39;, &#39;Nine&#39;, 18, &#39;male&#39;), (20, &#39;Jackly&#39;, &#39;Nine&#39;, 65, &#39;female&#39;), (21, &#39;Babby John&#39;, &#39;Four&#39;, 69, &#39;female&#39;), (22, &#39;Reggid&#39;, &#39;Seven&#39;, 55, &#39;female&#39;), (23, &#39;Herod&#39;, &#39;Eight&#39;, 79, &#39;male&#39;), (24, &#39;Tiddy Now&#39;, &#39;Seven&#39;, 78, &#39;male&#39;), (25, &#39;Giff Tow&#39;, &#39;Seven&#39;, 88, &#39;male&#39;), (26, &#39;Crelea&#39;, &#39;Seven&#39;, 79, &#39;male&#39;), (27, &#39;Big Nose&#39;, &#39;Three&#39;, 81, &#39;female&#39;), (28, &#39;Rojj Base&#39;, &#39;Seven&#39;, 86, &#39;female&#39;), (29, &#39;Tess Played&#39;, &#39;Seven&#39;, 55, &#39;male&#39;), (30, &#39;Reppy Red&#39;, &#39;Six&#39;, 79, &#39;female&#39;), (31, &#39;Marry Toeey&#39;, &#39;Four&#39;, 88, &#39;male&#39;), (32, &#39;Binn Rott&#39;, &#39;Seven&#39;, 90, &#39;female&#39;), (33, &#39;Kenn Rein&#39;, &#39;Six&#39;, 96, &#39;female&#39;), (34, &#39;Gain Toe&#39;, &#39;Seven&#39;, 69, &#39;male&#39;), (35, &#39;Rows Noump&#39;, &#39;Six&#39;, 88, &#39;female&#39;); . Show Tables . USE test_db; SHOW tables; . Delete a Database . To delete a database test_db run the following command: . DROP DATABASE test_db, FLUSH PRIVILEGES; . Add a Database User . To create a new user (here, we created a new user named redowan with the password password), run the following command in the MySQL shell: . CREATE USER &#39;redowan&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; FlUSH PRIVILEGES; . Ensure that the changes has been saved via running FLUSH PRIVILEGES;. Verify that a user has been successfully created via running the previous command: . SELECT User, Host, authentication_string FROM mysql.user; . You should see something like below. Notice that a new user named redowan has been created: . ++--+-+ | User | Host | authentication_string | ++--+-+ | root | localhost | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | debian-sys-maint | localhost | *8282611144B9D51437F4A2285E00A86701BF9737 | | redowan | localhost | *0756A562377EDF6ED3AC45A00B356AAE6D3C6BB6 | ++--+-+ . Delete a Database User . To delete a database user (here, I’m deleting the user-redowan) run: . DELETE FROM mysql.user WHERE user=&#39;&lt;redowan&gt;&#39; AND host = &#39;localhost&#39; FlUSH PRIVILEGES; . Grant Database User Permissions . Give the user full permissions for your new database by running the following command (Here, I provided full permission of test_db to the user redowan: . GRANT ALL PRIVILEGES ON test_db.table TO &#39;redowan&#39;@&#39;localhost&#39;; . If you want to give permission to all the databases, type: . GRANT ALL PRIVILEGES ON *.* TO &#39;redowan&#39;@&#39;localhost&#39;; FlUSH PRIVILEGES; . Loading Sample Database to Your Own MySQL Server . To load mysqlsampledatabase.sql to your own server (In this case the user is redowan. Provide database password in the prompt), first fireup the server and type the following commands: . mysql -u redowan -p test_db &lt; mysqlsampledatabase.sql; . Now run: . SHOW DATABASES; . You should see something like this: . +--+ | Database | +--+ | information_schema | | classicmodels | | mysql | | performance_schema | | sys | | test_db | +--+ 6 rows in set (0.00 sec) . Stop the Server . The following command stops the server. . docker-compose down . Notice that a new database named classicmodels has been added to the list. . Connecting to a Third Party Client . We will be using DBeaver as a third party client. While you can use the mysql shell to work on your data, a third partly client that make the experience much better with auto formatting, earsier import features, syntax highlighting etc. . Installing DBeaver . You can install DBeaver installer from here. Installation is pretty straight forward. . Connecting MySQL Database to DBeaver . Fire up DBeaver and you should be presented with this screen. Select MySQL 8+ and go next. . . The dialogue box will ask for credentials to connect to a database. In this case, I will log into previously created local database test_db with the username redowan, corresponding password password and press test connection tab. A dialogue box might pop up, prompting you download necessary drivers. . . If everything is okay, you should see a success message. You can select the SQL Editor and start writing your MySQL scripts right away. . Connecting to MySQL Server via Python . PyMySQL and DBUtils can be used to connect to MySQL Server. . import pymysql import os from dotenv import load_dotenv from DBUtils.PooledDB import PooledDB load_dotenv(verbose=True) MYSQL_REPLICA_CONFIG = { &quot;host&quot;: os.environ.get(&quot;SQL_HOST&quot;), &quot;port&quot;: int(os.environ.get(&quot;SQL_PORT&quot;)), &quot;db&quot;: os.environ.get(&quot;SQL_DB&quot;), &quot;password&quot;: os.environ.get(&quot;SQL_PASSWORD&quot;), &quot;user&quot;: os.environ.get(&quot;SQL_USER&quot;), &quot;charset&quot;: os.environ.get(&quot;SQL_CHARSET&quot;), &quot;cursorclass&quot;: pymysql.cursors.DictCursor, } # class to create database connection pooling POOL = PooledDB(**configs.MYSQL_POOL_CONFIG, **configs.MYSQL_REPLICA_CONFIG) class SqlPooled: &quot;&quot;&quot;Sql connection with pooling.&quot;&quot;&quot; def __init__(self): self._connection = POOL.connection() self._cursor = self._connection.cursor() def fetch_one(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchone() return result def fetch_all(self, sql, args): self._cursor.execute(sql, args) result = self._cursor.fetchall() return result def __del__(self): self._connection.close() .",
            "url": "https://rednafi.github.io/digressions/database/2020/03/15/mysql-install.html",
            "relUrl": "/database/2020/03/15/mysql-install.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Reduce Boilerplate Code with Python's Dataclasses",
            "content": "Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: . class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; print(CartesianPoint(1, 2, 3)) . &gt;&gt;&gt; CartesianPoint(x = 1, y = 2, z = 3) . This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. . Dataclasses . Let’s see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) . &gt;&gt;&gt; CartesianPoint(x=1, y=2, z=3) . In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotation and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via python’s help. . help(CartesianPoint) . Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -&gt; None | | __repr__(self) | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes defined here: | | __annotations__ = {&#39;x&#39;: &lt;class &#39;float&#39;&gt;, &#39;y&#39;: &lt;class &#39;float&#39;&gt;, &#39;z&#39;: &lt;c... | | __dataclass_fields__ = {&#39;x&#39;: Field(name=&#39;x&#39;,type=&lt;class &#39;float&#39;&gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None . Using Default Values . You can provide default values to the fields in the following way: . from dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 . Using Arbitrary Field Type . If you don’t want to specify your field type during type hinting, you can use Any type from python’s typing module. . from dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any . Instance Ordering . You can check if two instances are equal without making any modification to the class. . from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) . &gt;&gt;&gt; False . However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually. . from dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 &gt; point_2) . &gt;&gt;&gt; True . By default, while comparing instances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function. . Suppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way: . from dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create intance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 &lt; point_2) . &gt;&gt;&gt; True &gt;&gt;&gt; False . You can see the above code prints out True despite the instances have different y and z attributes. . Adding Methods . Methods can be added to dataclasses just like normal classes. Let’s add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin. . from dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) # create a new instance and use method `abs_val` point = CartesianPoint(5, 6, 7) norm = point.abs_val() print(norm) . &gt;&gt;&gt; 10.488088481701515 . Making Instances Immutable . By default, instances of dataclasses are immutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass. . from dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float . If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError. . point = CartesianPoint(2, 4, 6) point.x = 23 . FrozenInstanceError Traceback (most recent call last) &lt;ipython-input-34-b712968bd0eb&gt; in &lt;module&gt; 1 point = CartesianPoint(2, 4, 6) -&gt; 2 point.x = 23 &lt;string&gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;x&#39; . Making Instances Hashable . You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perform set operation on them. However, if you are using unsafe_hash make sure that your dataclasses do not contain any mutable data structure in it. . from dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f&quot;{point}&quot;: &quot;origin&quot;}) . &gt;&gt;&gt; {&#39;CartesianPoint(x=0, y=0, z=0)&#39;: &#39;origin&#39;} . Converting Instances to Dicts . The asdict() function converts a dataclass instance to a dict of its fields. . from dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) . &gt;&gt;&gt; {&#39;x&#39;: 1, &#39;y&#39;: 5, &#39;z&#39;: 6} . Post-init Processing . When dataclass generates the __init__ method, internally it’ll call _post_init__ method. You can add additional processing in the __post_init__ method. Here, I have added another attribute tup that returns the cartesian point as a tuple. . from dataclasses import dataclass @dataclass class CartesianPoint: x : float y : float z : float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) . &gt;&gt;&gt; (4, 5, 6) . Refactoring the Entire Cartesian Point Class . The feature rich original CartesianPoint looks something like this: . import math class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): &quot;&quot;&quot;Print the instance neatly.&quot;&quot;&quot; return f&quot;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})&quot; def __eq__(self, other): &quot;Checks if equal.&quot; return self.x == other.x def __nq__(self, other): &quot;&quot;&quot;Checks non equality.&quot;&quot;&quot; return self.x != other.x def __gt__(self, other): &quot;&quot;&quot;Checks if greater than.&quot;&quot;&quot; return self.x &gt; other.x def __ge__(self, other): &quot;&quot;&quot;Checks if greater than or equal.&quot;&quot;&quot; return self.x &gt;= other.x def __lt__(self, other): &quot;&quot;&quot;Checks if less than.&quot;&quot;&quot; return self.x &lt; other.x def __le__(self, other): &quot;&quot;&quot;Checks if less than or equal.&quot;&quot;&quot; return self.x &lt;= other.x def __hash__(self): &quot;&quot;&quot;Make the instances hashable.&quot;&quot;&quot; return hash(self) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Let’s see the class in action: . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x = 1, y = 2, z = 3) True True False True {&#39;CartesianPoint(x = 1, y = 2, z = 3)&#39;: 3.7416573867739413} . Below is the same class refactored using dataclass. . from dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: &quot;&quot;&quot;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.&quot;&quot;&quot; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): &quot;&quot;&quot;Finds distance of point from origin.&quot;&quot;&quot; return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) . Use this class like before. . # create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b &gt;= d) # checking the __lt__ method print(c &lt; a) # checking __hash__ and __dist__ method print({f&quot;{a}&quot;: a.dist()}) . CartesianPoint(x=1, y=2, z=3) True True False True {&#39;CartesianPoint(x=1, y=2, z=3)&#39;: 3.7416573867739413} . References . Python Dataclasses: Official Doc | The Ultimate Guide to Data Classes in Python 3.7 | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/12/python-dataclasses.html",
            "relUrl": "/python/2020/03/12/python-dataclasses.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Python Virtual Environment Workflow for Sanity",
            "content": "There are multiple ways of installing Python, creating and switching between different virtual environments. Also, Python’s package manager hyperspace is a mess. So, things can quickly get out of hands while dealing with projects that require quick environment switching across multiple versions of Python. I use Debian linux in my primary development environment and this is how I keep the option explosion in check: . Installing Python . Run the following commands one by one: . # update the packages list and install the prerequisites sudo apt update sudo apt install software-properties-common # add deadsnakes ppa to your sources&#39; list (When prompted press Enter to continue) sudo add-apt-repository ppa:deadsnakes/ppa # install python3.7 sudo apt install python3.8 # verify python installation python3.8 --version . Creating Virtual Environment . There are multiple ways creating and switching between different environments can be done. I use venv for creating virtual environments. For demonstration, here I’m creating a virtual environment that uses python3.8. . Install python3-venv for creating virtual environment sudo apt install python3.8-venv . | Create virtual environment named venv in the project folder . python3.8 -m venv venv . | Activate venv . source venv/bin/activate . | Deactivate venv deactivate . | . Switching Between Different Environments . To create another environment with a different python version, you have to: . Install the desired version of python following the procedures stated above. | Install python3.7-venv specific for your python version, like if you are using python3.7, you should run: . sudo apt install python3.7-venv . | Create multiple environments with multiple versions and name them distinctively. i.e. venv3.7, venv3.8 etc. Follow the instructions above. | Activate and deactivate the desired virtual environment. | . Package Management . For local development, I use pip. | For production application and libraries poetry is preferred. | .",
            "url": "https://rednafi.github.io/digressions/python/2020/03/11/python-venv-workflow.html",
            "relUrl": "/python/2020/03/11/python-venv-workflow.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "A Minimalistic Approach to ZSH",
            "content": ". Although I’m on Debian Linux, Apple’s recent announcement about replacing Bash with Zsh on MacOS made me take a look at Z-shell aka zsh. It’s a POSIX compliant Bash alternative that has been around for quite a long time. While Bash shell’s efficiency and ubiquity make it hard to think about changing the default shell of your primary development machine, I find its features as an interactive shell to be somewhat limited. So I did some digging around and soon found out that zsh’s lackluster default configurations and bloated ecosystem make it difficult for someone who just want to switch without any extra overhead. So, let’s make the process quicker. Here is what we are aiming for: . A working shell that can (almost always) take bash commands without complaining (looking at you fish) | History based autocomplete | Syntax highlighting | Git branch annotations | . Instructions were applied and tested on debian based linux (Ubuntu) . Install Z Shell . GNU/Linux . To install on a debian based linux, type: . $ apt install zsh . MacOS . Use homebrew to install zsh on MacOs. Run: . $ brew install zsh . Make Zsh as Your Default Shell . Run: . $ chsh -s $(which zsh) . Install Oh-My-Zsh Framework . Oh-My-Zsh is the tool that makes zsh so much fun and overly configurable at the same time. So we’ll tread here carefully. To install oh-my-zsh , type: . $ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; . Set Firacode As the Default Terminal Font . Your selected theme may not display all the glyphs if the default terminal font doesn’t support them. Installing a font with glyphs and ligature support can solve this. I recommend installing firacode and setting that as your default terminal font. Install Fira Code From here. . Set Syntax Highlighting . Using zsh-syntax-highlighting to achieve this. . Clone this repository in oh-my-zsh’s plugins directory . $ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . | Activate the plugin in ~/.zshrc . plugins=( [plugins...] zsh-syntax-highlighting) . | Source ~/.zshrc . | . Set Suggestions . Using zsh-autosuggestions to achieve this. . Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins) . $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . | Add the plugin to the list of plugins for Oh My Zsh to load (inside ~/.zshrc ) . plugins=(zsh-autosuggestions) . | Source ~/.zshrc . | . Start a new terminal session to see the effects!!! You might need to log out and log in again for the changes to be effective. . Load .profile from .zprofile . Add the following lines to ~/.zprofile and source via the command: source ~/.zprofile. Make sure you are on zsh before running the source command. . [[ -e ~/.profile ]] &amp;&amp; emulate sh -c &#39;source ~/.profile&#39; . A Barebone ~/.zshrc . Instead of adding the plugins individually, you can just install the plugins and then add this barebone config to your ~/.zshrc . Don’t forget to replace YourUserName with your username. Source your zshrc once you are done. . # ===================== # MINIMALIST ZSHRC # AUTHOR: REDNAFI # ===================== # omz path export ZSH=&quot;$HOME/.oh-my-zsh&quot; # theme settings ZSH_THEME=&quot;juanghurtado&quot; # pluging settings plugins=(git zsh-syntax-highlighting zsh-autosuggestions) # autosuggestion highlight ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=4&quot; # source omz source $ZSH/oh-my-zsh.sh #History setup HISTFILE=$HOME/.zsh_history HISTSIZE=100000 SAVEHIST=$HISTSIZ zstyle &#39;:completion:*&#39; menu select zstyle &#39;:completion:*&#39; group-name &#39;&#39; # group results by category zstyle &#39;:completion:::::&#39; completer _expand _complete _ignored _approximate #enable approximate matches for completion #disable auto correct unsetopt correct_all . Set Terminal Color (Optional) . Optionally you customize your terminal color and in this case I’ve used Gogh to achieve this. . Pre Install | . $ sudo apt-get install dconf-cli uuid-runtime . Install on Linux | . $ bash -c &quot;$(wget -qO- https://git.io/vQgMr)&quot; . Install on MacOS $ bash -c &quot;$(curl -sLo- https://git.io/vQgMr)&quot; . | Put the code associated with your desired color scheme. | . Updating OMZ . $ upgrade_oh_my_zsh . Uninstall Zsh . $ sudo apt-get --purge remove zsh . Uninstall OMZ . $ uninstall_oh_my_zsh . Switch Back to Bash . $ chsh -s $(which bash) . Reference . Oh-My-Zsh | FiraCode | Gogh |",
            "url": "https://rednafi.github.io/digressions/linux/2019/10/29/minimal-zsh.html",
            "relUrl": "/linux/2019/10/29/minimal-zsh.html",
            "date": " • Oct 29, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Essential Bash Scripting",
            "content": ". Shell . Several layers of events take place whenever a Linux command is entered into the terminal. The top layer of that is known as shell. . A shell is any user interface to the UNIX operating system, i.e., any program that takes input from the user, translates it into instructions that the operating system can understand, and conveys the operating system’s output back to the user. . Let’s look at an example: . sort -n src/files/numbers.txt &gt; src/files/sorted_numbers.txt . This command will perform the following tasks: . Go to the src/files directory | Sort the numbers in the numbers.txt files in ascending order | Save the result in a new file called sorted_numbers.txt in the same directory | . History . The first major shell was the Bourne shell (named after its inventor, Steven Bourne); it was included in the first popular version of UNIX, Version 7, starting in 1979. The Bourne shell is known on the system as sh. Although UNIX has gone through many, many changes, the Bourne shell is still popular and essentially unchanged. Several UNIX utilities and administration features depend on it. . Variants of some popular shells: . C Shell or csh (The syntax has resemblance with C programming language) | Korn Shell or ksh (Similar to Bourne Shell with features from both Bourne and C Shell) | The Bourne Again Shell or BASH (Started with the GNU project in 1988.) | . BASH is going to be our primary focus here. . A Few Basic Commands . List of most frequently used commands. All of these commands can be run directly from a bash command prompt: . cd | ls | cat | cp | mv | mkdir | rm | grep | lp | . All of the following command summaries can be found via: . curl cheat.sh/&lt;prompt&gt; . cd . cd is used to change directory . #Go to the given directory cd path/to/directory #Go to home directory of current user cd #Go up to the parent of the current directory cd .. #Go to the previously chosen directory cd - . ls . ls lists all the files and folders in a user-specified directory . # Displays everything in the target directory ls path/to/the/target/directory # Displays everything including hidden files ls -a # Displays all files, along with the size (with unit suffixes) and timestamp ls -lh # Display files, sorted by size ls -S # Display directories only ls -d */ # Display directories only, include hidden ls -d .*/ */ . cat . cat shows the contents of a user-specified file . # Display the contents of a file cat /path/to/foo # Display contents with line numbers cat -n /path/to/foo # Display contents with line numbers (blank lines excluded) cat -b /path/to/foo . cp . cp copies files or folders from one directory to another . # Create a copy of a file cp ~/Desktop/foo.txt ~/Downloads/foo.txt # Create a copy of a directory cp -r ~/Desktop/cruise_pics/ ~/Pictures/ # Create a copy but ask to overwrite if the destination file already exists cp -i ~/Desktop/foo.txt ~/Documents/foo.txt # Create a backup file with date cp foo.txt{,.&quot;$(date +%Y%m%d-%H%M%S)&quot;} . mv . mv moves files or folders from one directory to another and can also be used to rename files or folders . # Move a file from one place to another mv ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another and automatically overwrite if the destination file exists # (This will override any previous -i or -n args) mv -f ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but ask before overwriting an existing file # (This will override any previous -f or -n args) mv -i ~/Desktop/foo.txt ~/Documents/foo.txt # Move a file from one place to another but never overwrite anything # (This will override any previous -f or -i args) mv -n ~/Desktop/foo.txt ~/Documents/foo.txt # Move listed files to a directory mv -t ~/Desktop/ file1 file2 file3 . mkdir . mkdir is used to create a folder in a directory . # Create a directory and all its parents mkdir -p foo/bar/baz # Create foo/bar and foo/baz directories mkdir -p foo/{bar,baz} # Create the foo/bar, foo/baz, foo/baz/zip and foo/baz/zap directories mkdir -p foo/{bar,baz/{zip,zap}} . rm . rm is mainly used to delete files or folders . # Remove files and subdirs rm -rf path/to/the/target/ # Ignore non existent files rm -f path/to/the/target # Remove a file with his inode find /tmp/ -inum 6666 -exec rm -i &#39;{}&#39; ; . grep . grep can be used to search through the output of another command . # Search a file for a pattern grep pattern file # Case insensitive search (with line numbers) grep -in pattern file # Recursively grep for string &lt;pattern&gt; in folder: grep -R pattern folder # Read search patterns from a file (one per line) grep -f pattern_file file # Find lines NOT containing pattern grep -v pattern file # You can grep with regular expressions grep &quot;^00&quot; file #Match lines starting with 00 grep -E &quot;[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3} .[0-9]{1,3}&quot; file #Find IP add # Find all files which match {pattern} in {directory} # This will show: &quot;file:line my research&quot; grep -rnw &#39;directory&#39; -e &quot;pattern&quot; # Exclude grep from your grepped output of ps. # Add [] to the first letter. Ex: sshd -&gt; [s]shd ps aux | grep &#39;[h]ttpd&#39; # Colour in red {bash} and keep all other lines ps aux | grep -E --color &#39;bash|$&#39; . lp . lp prints the specified output via an available printer . # lp # Print files. # Print the output of a command to the default printer (see `lpstat` command): echo &quot;test&quot; | lp # Print a file to the default printer: lp path/to/filename # Print a file to a named printer (see `lpstat` command): lp -d printer_name path/to/filename # Print N copies of a file to the default printer (replace N with the desired number of copies): lp -n N path/to/filename # Print only certain pages to the default printer (print pages 1, 3-5, and 16): lp -P 1,3-5,16 path/to/filename # Resume printing a job: lp -i job_id -H resume . clear . clear is used to clear the CLI window . # clear # Clears the screen of the terminal. # Clear the screen (equivalent to typing Control-L when using the bash shell): clear . exit . exit closes the CLI window . # exit # Quit the current CMD instance or the current batch file. # More information: #&lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/exit&gt;. # Quit the current CMD instance: exit # Quit the current batch script: exit /b # Quit using a specific exit code: exit exit_code . Basic Scripting Examples . When you need to execute multiple shell commands sequentially or want to do more complex stuffs, it’s better to enclose the commands in a bash script. . Running a Shell Script . Create a file with .sh extension. I have used Ubuntu’s built-in nano editor for that. . $ nano script.sh . | Put your code in the .sh file | Make sure you put the shebang #!/bin/bash at the beginning of each script, otherwise, the system wouldn’t know which interpreter to use. . | Give permission to run: . $ chmod +x script.sh . | Run the script via: . $ ./script . | If the script takes in one or multiple arguments, then place those with spaces in between. . $ ./script arg1 arg2 . | . conditionals (if-else) . Example-1: This program, Takes in two integers as arguments | Compares if one number is greater than the other or if they are equal | Returns the greater of the two numbers or if they are equal, returns equal | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number1=&quot;$1&quot; number2=&quot;$2&quot; if [ $number1 -eq $number2 ] then echo &quot;The numbers are equal&quot; elif [ $number1 -gt $number2 ] then echo &quot;The greater number is $number1&quot; elif [ $number2 -gt $number1 ] then echo &quot;The greater number is $number2&quot; fi . $ ./script.sh 12 13 The greater number is 13 . | Example-2: This program, Takes a single number as an argument | Checks whether the number is Odd or Even | Returns Odd or Even accordingly | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail number=&quot;$1&quot; if [ $(( number%2 )) -eq 0 ] then echo &quot;Even&quot; else echo &quot;Odd&quot; fi . $ ./script.sh 20 Even . | Example-3: This program, Takes in two integers and an operation instruction | Returns the value according to the operation | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail echo &quot;Enter two numbers and the intended operation: * for addition, write add * for subtraction, write sub * for multiplication, write mul * for division, write div (write quit to quit the program)&quot; num1=&quot;$1&quot; num2=&quot;$2&quot; operation=&quot;$3&quot; if [ $num1 == &quot;quit&quot; ] then break elif [ $operation == &quot;add&quot; ] then ans=$(( $num1 + $num2 )) echo &quot;addition: $ans&quot; elif [ $operation == &quot;sub&quot; ] then ans=$(( $num1 - $num2 )) echo &quot;subtraction: $ans&quot; elif [ $operation == &quot;mul&quot; ] then ans=$(( $num1 * $num2 )) echo &quot;multiplication: $ans&quot; elif [ $operation == &quot;div&quot; ] then ans=$(( $num1 / $num2 )) echo &quot;division: $ans&quot; fi . $ ./script.sh 12 13 add 25 . | . for loop . Example-1: Looping through 0 to 9 with increment 3 and printing the numbers . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for var in {0..9..3} do echo $var done . $ ./script.sh 0 3 6 9 . | Example-2: Looping through files in a folder and printing them one by one . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail for file in $(ls ./files) do echo $file done . $ ./script.sh numbers.txt sorted_numbers.txt . | Example-3: This program, Doesn’t take any argument | Returns the summation of all the integers, starting from 0, up to 100 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail sum=0 for num in $(seq 0 100) do sum=$(($sum + $num)) done echo &quot;Total sum is $sum&quot; . $ ./script.sh Total sum is 5050 . | Example-4: This program, Takes in an integer as an argument | Prints all the numbers up to that number, starting from 0 | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail input_number=&quot;$1&quot; for num in $(seq 0 $input_number) do if [ $num -lt $input_number ] then echo $num fi done . $ ./script.sh 100 0 1 . . 99 . | . while loop . Example-1: This program, Takes in a single integer as an argument | Returns the factorial of that number | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail counter=&quot;$1&quot; factorial=1 while [ $counter -gt 0 ] do factorial=$(( $factorial * $counter )) counter=$(( $counter - 1 )) done echo &quot;Factorial of $1 is $factorial&quot; . $ ./script.sh 5 Factorial of 5 is 120 . | Example-2: This program, Takes two integers as arguments | Returns the summation of the numbers | Sending -1 as an input quits the program | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail while : do read -p &quot;Enter two numbers ( - 1 to quit ) : &quot; &quot;a&quot; &quot;b&quot; if [ $a -eq -1 ] then break fi ans=$(( $a + $b )) echo $ans done . $ ./script.sh Enter two numbers (-1 to quit): 20 30 30 . | Example-3: This program, Takes in a text filepath as argument | Reads and prints out each line of the file | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail file=&quot;$1&quot; while read -r line do echo &quot;$line&quot; done &lt; &quot;$file&quot; . $ ./script.sh files/numbers.txt 5 55 . . 11 10 . | . functions . Functions are incredible tools when we need to reuse code. Creating functions are fairly straight forward in bash. . Example-1: This function, . Takes a directory as an input argument | Counts the number of files in that directory and prints that out | Note that this function ignores the dot files (The ls -1 flag ignores dot files) | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function file_count () { ls -1 &quot;$1&quot; | wc -l } # calling the function echo $( file_count $1 ) . $ ./script.sh ./files $ 2 . | Example-2: This function, Takes in a shortcode for any of the following languages (a) en for English (b) fr for French (c) bn for bangla . | Returns a welcome message in the selected language . | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function greetings () { language=&quot;$1&quot; if [ $language == &quot;en&quot; ] then echo &quot;Greetings Mortals!&quot; elif [ $language == &quot;fr&quot; ] then echo &quot;Salutations Mortels!&quot; elif [ $language == &quot;bn&quot; ] then echo &quot;নশ্বরকে শুভেচ্ছা!&quot; fi } # calling the function echo $( greetings $1 ) . $ ./script.sh en Greetings Mortals! . | Example-3: This function, Takes a directory as an argument | Loop through the files | Only returns the text files with full path | . #!/bin/bash # bash strict mode for easier debugging set -euo pipefail # declaring the function return_text () { dir=&quot;$1&quot; for file in $dir&quot;/*.txt&quot; do echo &quot;$( realpath $file )&quot; done } echo &quot;$( return_text $1 )&quot; . $ ./script.sh /home/redowan/code/bash/files/numbers.txt /home/redowan/code/bash/files/sorted_numbers.txt . | . Some Good Practices . Use a Bash Strict Mode . Your bash scripts will be more robust, reliable and easy to debug if it starts with: . #!/bin/bash set -euo pipefail . This can be regarded as an unofficial bash strict mode and often prevents many classes of sneaky bugs in your script. The above command can be synthesized into multiple commands. . set -euo pipefail is short for: . set -e set -u set -o pipefail . Let’s have a look at each of them separately. . set-e: This instruction forces the bash script to exit immediately if any command has a non zero exit status. If there’s an issue in any of the lines in your code, the subsequent lines simply won’t run. . | set-u: If your code has a reference to any variable that wasn’t defined previously, this will cause the program to exit. . | set -o pipefail: This setting prevents errors in a pipeline being masked. If any command in a pipeline fails, that return code will be used as the return code of the whole pipeline, not the last command’s return code. . | . For a more in depth explanation of the different settings and Bash Strict Mode in general, check out, AAron Maxwell’s blog on this topic. . Double Quote Your Variables . It is generally a good practice to double quote your variables, specially user input variables where spaces are involved. . External Resources . Here are some awesome sources where you can always look into if you get stuck: . Command Line Crash Course | Ryans Bash Tutorial | W3 School CLI Tutorial | .",
            "url": "https://rednafi.github.io/digressions/linux/2019/09/05/essential-bash.html",
            "relUrl": "/linux/2019/09/05/essential-bash.html",
            "date": " • Sep 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Redowan Delowar is currently working as a junior Data Scientist for ShopUp (~2 years). . Redowan primarily focuses on Computational Statistics, Representation Learning and Software development. He is an avid Machine Learning evangelist, researcher, practitioner and Open Source developer. He is also available for on-site teaching, presentations and certain types of short term contract works.  . Contact . Gmail: redowan.nafi@gmail.com | Github: rednafi | Twitter: rednafi | LinkedIn: Redowan Delowar | .",
          "url": "https://rednafi.github.io/digressions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}